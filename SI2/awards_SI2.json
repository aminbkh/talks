{"AwardsList": {"Award": [{"OrganizationCity": "TEMPE", "OrganizationState": "AZ", "Title": "2016 Software Infrastructure for Sustained Innovation (SI2) Principal Investigators Workshop", "ProgramManager": "Rajiv Ramnath", "State": "AZ", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "01/15/2016", "OrganizationStreet": "ORSPA", "EndDate": "12/31/2016", "PrincipalInvestigator": "Francis Timmes", "ARRAAmount": null, "OrganizationPhone": "4809655479", "AwardNumber": "1606994", "PIEmailAddress": "fxt44@mac.com", "NSFDirectorate": "CSE", "Co-PIName": ["Stanley Ahalt", "Shaowen Wang", "Matthew Turk"], "LastAmendmentDate": "02/08/2016", "Organization": "Arizona State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "104947", "Abstract": "This project will host a 1.5-day workshop in Arlington, VA, which will bring together the community of SI2 awardees (with the goal of involving one principal investigator from each SSE and SSI project, many of which are collaborative awards) from approximately 250 awards. The workshop will have participation from SI2 EAGER and RAPID awardees and selected awardees of the ACI VOSS program that examines cyberinfrastructures from the social and organizational perspective. In addition, the proximity to NSF will encourage participation by Program Officers from across the Foundation. Goals of this workshop include: (a) providing a focused forum for PIs to share technical information with each other and with NSF Program Officers, (b) encouraging exploration of emerging topics, (c) identifying emerging best practices across the supported software projects, (d) stimulating thinking on new ways of achieving software sustainability, and (d) disseminating the shared experiences of the researchers via an online web portal.&lt;br/&gt;&lt;br/&gt;The workshop is expected to host close to 150 SI2 and other awardees, other speakers and panelists. The workshop will use a hybrid style, blending a traditional, top-down driven agenda with a interactive, adaptive, participant-driven agenda. &lt;br/&gt;&lt;br/&gt;The proposed workshop will support the exchange of ideas among the current software cyberinfrastructure development projects. It will provide guidance on issues related to the development of robust software and to the problem of software sustainability. Involvement of program officers across NSF is expected to help the interdisciplinary SI2 awardees understand the relevance and impact of cyberinfrastructure throughout the NSF. The participation of these researchers and program officers in a common forum will help ensure that the cyberinfrastructure software developed as part of SI2 projects will be relevant and broadly applicable to the most science and engineering domains possible. The hybrid approach is innovative and holds the promise of creating rich interactions among PIs, resulting in richer collaboration and learning. The results of this workshop thus have the potential to guide cyberinfrastructure development and cyberinfrastructure driven research for both the participating projects and for the wider software development community.", "ProgramReferenceCode": ["7433", "7556", "8004"], "OrganizationZip": "852816011"}, {"OrganizationCity": "TEMPE", "OrganizationState": "AZ", "Title": "Software Infrastructure for Sustained Innovation - A 2015 SI^2 PI Workshop", "ProgramManager": "Rajiv Ramnath", "State": "AZ", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "01/15/2015", "OrganizationStreet": "ORSPA", "EndDate": "12/31/2016", "PrincipalInvestigator": "Francis Timmes", "ARRAAmount": null, "OrganizationPhone": "4809655479", "AwardNumber": "1521388", "PIEmailAddress": "fxt44@mac.com", "NSFDirectorate": "CSE", "Co-PIName": ["Matthew Turk", "Stanley Ahalt"], "LastAmendmentDate": "01/29/2015", "Organization": "Arizona State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "75527", "Abstract": "This project will host a 1.5-day workshop in Arlington, VA, which will bring together the community of SI2 awardees (with the goal of involving one principal investigator from each SSE and SSI project, many of which are collaborative awards) from 214 awards. The workshop will also solicit participation from NSF-supported science and engineering projects that rely on the national research cyberinfrastructure for their computation, communication, and data management needs. In addition, the proximity to NSF will encourage participation by Program Officers from across the Foundation. Goals of this workshop include: (1) Providing a focused forum for PIs to share technical information with each other and with NSF Program Officers; (2) Encouraging exploration of emerging topics; (3) Identifying emerging best practices across the supported software projects; (4) Stimulating thinking on new ways of achieving software sustainability; and (5) Disseminating the shared experiences of the researchers via an online web portal. The workshop is expected to host 85 SI2 awardees and several other speakers and panelists. The workshop will use a hybrid style, blending a traditional, top-down driven agenda with a interactive, adaptive, participant-driven agenda. &lt;br/&gt;&lt;br/&gt;The proposed workshop will support the exchange of ideas among the current software cyberinfrastructure development projects. It will provide guidance on issues related to the development of robust software and to the problem of software sustainability. Involvement of program officers across NSF is expected to help the interdisciplinary SI2 awardees understand the relevance and impact of cyberinfrastructure throughout the NSF. The participation of these researchers and program officers in a common forum will help ensure that the cyberinfrastructure software developed as part of SI2 projects will be relevant and broadly applicable to the most science and engineering domains possible. The hybrid approach is innovative and holds the promise of creating rich interactions among PIs, resulting in richer collaboration and learning. The results of this workshop thus have the potential to guide cyberinfrastructure development and cyberinfrastructure driven research for both the participating projects and for the wider software development community.", "ProgramReferenceCode": ["7433", "7556", "8004"], "OrganizationZip": "852816011"}, {"OrganizationCity": "Piscataway", "OrganizationState": "NJ", "Title": "Collaborative Research: SI2 CHE: Development and Deployment of Chemical Software for Advanced Potential Energy Surfaces", "ProgramManager": "Evelyn M. Goldfield", "State": "NJ", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes"], "ProgramElementCode": ["1253", "8004"], "StartDate": "05/15/2013", "OrganizationStreet": "33 Knightsbridge Road", "EndDate": "04/30/2016", "PrincipalInvestigator": "David Case", "ARRAAmount": null, "OrganizationPhone": "8489320150", "AwardNumber": "1265704", "PIEmailAddress": "case@biomaps.rutgers.edu", "NSFDirectorate": "MPS", "LastAmendmentDate": "05/15/2013", "Organization": "Rutgers University New Brunswick", "AwardInstrument": "Standard Grant", "NsfOrganization": "CHE", "AwardedAmountToDate": "328229", "Abstract": "An international team consisting of Teresa Head-Gordon and Martin Head-Gordon (University of California, Berkeley), Paul Nerenberg (Claremont McKenna College), David Case (Rutgers University), Jay Ponder (Washington University), Mark Tuckerman (New York University) with their UK collaborators: Lorna Smith and Neil Chue Hong (University of Edinburgh), Chris-Kriton Skylaris and Jonathan W. Essex (University of Southampton), Ilian Todorov (Daresbury Laboratory), Mario Antonioletti (EPCC) are are supported through the SI2-CHE program to develop and deploy robust and sustainable software for advanced potential energy surfaces. The greater accuracy introduced by improvements in the new generation of potential energy surfaces opens up several challenges in their manifestation as algorithms and software on current or emergent hardware platforms that in turn limits their wide adoption by the computational chemistry community. The research team is overcoming these obstacles via multiple but integrated directions: (1) to optimally implement advanced potential energy surfaces across multi-core and GPU enabled systems, (2) to develop a hierarchy of advanced polarizable models that alter the tradeoff between accuracy and computational speed,(3) to create new multiple time stepping methods; (4) to write a Quantum Mechanics/Molecular Mechanics (QM/MM ) application programing interface (API) that fully supports mutual polarization, (5) to adopt software best practices to ensure growth of a self-sustaining community and (6) to provide exemplar calculations with the new software in the several emerging application areas.&lt;br/&gt;&lt;br/&gt;Molecular simulation and quantum chemistry software is an integral part of chemistry and chemical biology, and has been broadly adopted by academic researchers and industry scientists. Next generation scientific breakthroughs that utilize chemical software will be enabled by the deployment of state of the art theoretical models and algorithms that are translated into a sustainable software framework rapidly implemented on emergent high performance computing platforms. Potential energy surfaces describe the interactions between atoms. Advanced and highly accurate potential energy surfaces encounter software-related obstacles that inhibit their application to grand challenge chemistry problems. This UK and US consortium, representing a broad cross section of the computational chemistry software community, is working to directly tackle these obstacles. This US and UK collaboration between universities and High Performance Computing centers works to endure that chemical software investments made in advanced potential energy surface models has a long term payoff in community sustainability and the training of the next generation of scientists. Outreach and training workshops are organized around the emergence of the advanced potential energy software including an introductory molecular simulation software boot camp for undergraduate students.&lt;br/&gt;&lt;br/&gt;The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC.", "ProgramReferenceCode": ["5918", "5946", "5950", "7433", "8009"], "OrganizationZip": "088543925"}, {"OrganizationCity": "BOSTON", "OrganizationState": "MA", "Title": "SI2:SSE-Collaborative Research: Advanced Software Infrastructure for Biomechanical Inverse Problems", "ProgramManager": "Daniel Katz", "State": "MA", "Program": ["Mechanics of Materials and Str", "Biomechanics & Mechanobiology", "Software Institutes"], "ProgramElementCode": ["1630", "7479", "8004"], "StartDate": "06/01/2012", "OrganizationStreet": "881 COMMONWEALTH AVE", "EndDate": "05/31/2016", "PrincipalInvestigator": "Paul Barbone", "ARRAAmount": null, "OrganizationPhone": "6173534365", "AwardNumber": "1148124", "PIEmailAddress": "barbone@bu.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "06/20/2012", "Organization": "Trustees of Boston University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "240450", "Abstract": "Biomechanical imaging refers to the remote measurement of the mechanical properties of tissues, in-situ and in-vivo. Images of the tissue can be thus created by visualizing the mechanical property distributions. This technique relies on imaging tissue while it is deformed by a set of externally applied forces. Through image processing, the displacement field everywhere in the region of interest is inferred. An inverse problem for the relevant mechanical properties is then solved, given the measured displacement fields, an assumed form of the tissue&apos;s constitutive equation, and the law of conservation of momentum. Images of reconstructed parameters find applications in the detection, diagnosis and treatment monitoring of disease, and in designing patient specific models for surgical training and planning. Over the last decade we have developed a software package (NLACE) to solve this inverse problem eciently in different application domains. Through this award we will make enhancements to NLACE that will make it easier to utilize and modify, and extend its user base to a wider community.&lt;br/&gt;&lt;br/&gt;The specific tasks for enhancing NLACE can be divided into two categories: (a) Steps to transition from a working prototype of NLACE to a software resource for the community. These include establishing an Input/Output standard for NLACE, creation of a GUI for input data and for monitoring the progress of the solution, hosting NLACE distributions, and creating sets of test data and documentation for its release. (b) Tasks that would enhance the functional capability of NLACE. These include the creation of a user-defined hyperelastic material model module to address a large class of tissue and material types, parallelization of NLACE on distributed memory, shared memory and GPU platforms, and quantifying uncertainty in the spatial distribution of the reconstructed parameters. We will measure our progress through user-feedback obtained during annual validation tests performed by a committed focus user-group that will test all aspects of the proposed research.&lt;br/&gt;&lt;br/&gt;The proposed improvements to NLACE will further its application in the detection, diagnosis and treatment monitoring of diseases, generation of patient-specic models for surgical planning and image-guidance applications, and studies in biomechanics and mechanobiology. The parallel and uncertainty quantification strategies developed for NLACE can be applied to a broad class of inverse problem with PDE constraints, including acoustic and electromagnetic scattering, seismic inversion, diffuse optical tomography, aquifer permeability and thermometry. Our outreach plans ensure the dissemination of NLACE to our focus group and to a broader community through hosting on the Simtk NIH center website. Finally, two graduate students will be trained in the elds of computational science and mathematics and biomechanics, and results from the proposed research will be presented at conferences on computational and biomedical science and engineering.", "ProgramReferenceCode": ["8005", "1630", "7479", "8004"], "OrganizationZip": "022151300"}, {"OrganizationCity": "BERKELEY", "OrganizationState": "CA", "Title": "SI2 SSI: Collaborative Research: Sustained Innovations for Linear Algebra Software (SILAS)", "ProgramManager": "Daniel Katz", "State": "CA", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes", "CDS&E-MSS"], "ProgramElementCode": ["1253", "8004", "8069"], "StartDate": "10/01/2013", "OrganizationStreet": "Sponsored Projects Office", "EndDate": "09/30/2016", "PrincipalInvestigator": "James Demmel", "ARRAAmount": null, "OrganizationPhone": "5106428109", "AwardNumber": "1339676", "PIEmailAddress": "demmel@cs.berkeley.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "09/02/2015", "Organization": "University of California-Berkeley", "AwardInstrument": "Continuing grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "611954", "Abstract": "As the era of computer architectures dominated by serial processors comes to a close, the convergence of several unprecedented changes in processor design has produced a broad consensus that much of the essential software infrastructure of computational science and engineering is utterly obsolete. Math libraries have historically been in the vanguard of software that must be quickly adapted to such design revolutions because they are the common, low-level software workhorses that do all the most basic mathematical calculations for many different types of applications. The Sustained Innovation for Linear Algebra Software (SILAS) project updates two of the most widely used numerical libraries in the history of Computational Science and Engineering---LAPACK and ScaLAPACK, (abbreviated Sca/LAPACK)---enhancing and hardening them for this ongoing revolution in processor architecture and system design. SILAS creates a layered package of software components, capable of running at every level of the platform deployment pyramid, from the desktop to the largest supercomputers in the world. It achieves three complementary objectives: 1) Wherever possible, SILAS delivers seamless access to the most up-to-date algorithms, numerical implementations, and performance, by way of Sca/LAPACK programming interfaces that are familiar to many computational scientists; 2) Wherever necessary, SILAS makes advanced algorithms, numerical implementations and performance capabilities available through new interface extensions; and 3) SILAS provides a well engineered conduit through which new discoveries at the frontiers of research in these areas can be channeled as quickly as possible to all the application communities that depend on high performance linear algebra. The improvements and innovations included in SILAS derive from a variety of sources. They represent the results (including designs and well tested prototypes) of the PIs&apos; own algorithmic and software research agenda, which has targeted multicore, hybrid and extreme scale system architectures. They are an outcome of extensive and on-going interactions with users, vendors, and the management of large NSF and DOE supercomputing facilities. They flow from cross-disciplinary engagement with other areas of computer science and engineering, anticipating the demands and opportunities of new architectures and programming models. And finally, they come from the enthusiastic participation of the research community in developing and offering enhanced versions of existing Sca/LAPACK codes.&lt;br/&gt;&lt;br/&gt;The primary impact of SILAS is a direct function of the importance of the Sca/LAPACK libraries to many branches of computational science. The Sca/LAPACK libraries are the community standard for dense linear algebra and have been adopted and/or supported by a large community of users, computing centers, and HPC vendors. Learning to use them is a basic part of the education of a computational scientist or engineer in many fields and at many academic institutions. Application domains where Sca/LAPACK have historically been heavily used include (among a host of other examples) airplane wing design, radar cross-section studies, flow around ships and other off-shore constructions, diffusion of solid bodies in a liquid, noise reduction, and diffusion of light through small particles. Moreover, the list of application partners working with SILAS to enhance and transform these libraries for next generation platforms expands this traditional list to include quantum chemistry, adaptive mesh refinement schemes, computational materials science, geophysical flows, stochastic simulation and database research for &quot;big data&quot;. No other numerical library can claim this breadth of integration with the community. Thus, there is every reason to believe that enhancing these libraries with state of the art methods and algorithms and adapting them for new and emerging platforms (reaching up to extreme scale), will have a correspondingly large impact on the research and education community, government laboratories, and private industry.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "947045940"}, {"OrganizationCity": "Troy", "OrganizationState": "NY", "Title": "SI2:SSE-Collaborative Research: Advanced Software Infrastructure for Biomechanical Inverse Problems", "ProgramManager": "Daniel Katz", "State": "NY", "Program": ["Mechanics of Materials and Str", "Biomechanics & Mechanobiology", "Software Institutes"], "ProgramElementCode": ["1630", "7479", "8004"], "StartDate": "06/01/2012", "OrganizationStreet": "110 8TH ST", "EndDate": "05/31/2016", "PrincipalInvestigator": "Assad Oberai", "ARRAAmount": null, "OrganizationPhone": "5182766000", "AwardNumber": "1148111", "PIEmailAddress": "oberaa@rpi.edu", "NSFDirectorate": "CSE", "Co-PIName": "Christopher Carothers", "LastAmendmentDate": "06/20/2012", "Organization": "Rensselaer Polytechnic Institute", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "290301", "Abstract": "Biomechanical imaging refers to the remote measurement of the mechanical properties of tissues, in-situ and in-vivo. Images of the tissue can be thus created by visualizing the mechanical property distributions. This technique relies on imaging tissue while it is deformed by a set of externally applied forces. Through image processing, the displacement field everywhere in the region of interest is inferred. An inverse problem for the relevant mechanical properties is then solved, given the measured displacement fields, an assumed form of the tissue&apos;s constitutive equation, and the law of conservation of momentum. Images of reconstructed parameters find applications in the detection, diagnosis and treatment monitoring of disease, and in designing patient specific models for surgical training and planning. Over the last decade we have developed a software package (NLACE) to solve this inverse problem eciently in different application domains. Through this award we will make enhancements to NLACE that will make it easier to utilize and modify, and extend its user base to a wider community.&lt;br/&gt;&lt;br/&gt;The specific tasks for enhancing NLACE can be divided into two categories: (a) Steps to transition from a working prototype of NLACE to a software resource for the community. These include establishing an Input/Output standard for NLACE, creation of a GUI for input data and for monitoring the progress of the solution, hosting NLACE distributions, and creating sets of test data and documentation for its release. (b) Tasks that would enhance the functional capability of NLACE. These include the creation of a user-defined hyperelastic material model module to address a large class of tissue and material types, parallelization of NLACE on distributed memory, shared memory and GPU platforms, and quantifying uncertainty in the spatial distribution of the reconstructed parameters. We will measure our progress through user-feedback obtained during annual validation tests performed by a committed focus user-group that will test all aspects of the proposed research.&lt;br/&gt;&lt;br/&gt;The proposed improvements to NLACE will further its application in the detection, diagnosis and treatment monitoring of diseases, generation of patient-specic models for surgical planning and image-guidance applications, and studies in biomechanics and mechanobiology. The parallel and uncertainty quantification strategies developed for NLACE can be applied to a broad class of inverse problem with PDE constraints, including acoustic and electromagnetic scattering, seismic inversion, diffuse optical tomography, aquifer permeability and thermometry. Our outreach plans ensure the dissemination of NLACE to our focus group and to a broader community through hosting on the Simtk NIH center website. Finally, two graduate students will be trained in the elds of computational science and mathematics and biomechanics, and results from the proposed research will be presented at conferences on computational and biomedical science and engineering.", "ProgramReferenceCode": ["8005", "1630", "7479", "8004"], "OrganizationZip": "121803522"}, {"OrganizationCity": "Atlanta", "OrganizationState": "GA", "Title": "Collaborative Research: SI2-SSI: Sustainable Development of Next-Generation Software in Quantum Chemistry", "ProgramManager": "Daniel Katz", "State": "GA", "Program": ["DMR SHORT TERM SUPPORT", "OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes", "CHEMISTRY PROJECTS"], "ProgramElementCode": ["1712", "1253", "8004", "1991"], "StartDate": "06/01/2012", "OrganizationStreet": "Office of Sponsored Programs", "EndDate": "05/31/2016", "PrincipalInvestigator": "Charles Sherrill", "ARRAAmount": null, "OrganizationPhone": "4048944819", "AwardNumber": "1147843", "PIEmailAddress": "sherrill@gatech.edu", "NSFDirectorate": "CSE", "Co-PIName": "Edmond Chow", "LastAmendmentDate": "07/06/2012", "Organization": "Georgia Tech Research Corporation", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "614654", "Abstract": "Quantum chemistry can provide highly accurate results for arbitrary molecular systems, making it a vital component in many different disciplines such as materials science, biology, physics, chemical engineering, mechanical engineering, environmental science, geology, and others. It is particularly critical in the rational design of drugs, catalysts, organic electronics, nanostructured materials, and other designed materials. Because of their steep computational costs, quantum chemistry codes must exploit parallel computing and must constantly adapt to rapidly changing high performance computing technologies. This creates a significant barrier for the adoption of new technologies into quantum chemistry codes. Our project involves the development of a parallel, highly reusable library for advanced numerical approximations in quantum chemistry. This will be the first unified library of such techniques, designed for high performance and also reusability by independent research groups. The PANACHE (PArallel Numerical Approximations in CHemistry Engine) library will fill this need. To maximize its impact, PANACHE is being designed to be used by multiple quantum chemistry software packages. PANACHE dramatically speeds up quantum computations, making it much easier to gain insight into a wide array of problems, from studies of reaction mechanisms in catalysis to the design of improved organic photoelectronic devices. &lt;br/&gt;&lt;br/&gt;Our highly interdisciplinary project (involving two theoretical chemists and one computational scientist as co-PI?s) provides excellent opportunities for training graduate students and postdocs in the areas of numerical methods, high-performance computing, quantum mechanics, and computational chemistry. Computer code resulting from this project will be released as freely-available open-source software, enabling its use with any other software package. Workshops on the new software will be held to introduce these new tools to other software developers, and online training material and graduate course material will be developed to improve education in the use of numerical methods in computational science and quantum chemistry.&lt;br/&gt;&lt;br/&gt;This award pertains to the Software Infrastructure for Sustained Innovation (SI2) solicitation.", "ProgramReferenceCode": ["7433", "7683", "8009", "9216", "9263", "1253", "1712", "1991", "8004"], "OrganizationZip": "303320420"}, {"OrganizationCity": "HONOLULU", "OrganizationState": "HI", "Title": "SI2-SSI: SAGEnext: Next Generation Integrated Persistent Visualization and Collaboration Services for Global Cyberinfrastructure", "ProgramManager": "Daniel Katz", "State": "HI", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "05/16/2014", "OrganizationStreet": "2440 Campus Road, Box 368", "EndDate": "09/30/2018", "PrincipalInvestigator": "Jason Leigh", "ARRAAmount": null, "OrganizationPhone": "8089567800", "AwardNumber": "1441963", "PIEmailAddress": "leighj@hawaii.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "04/14/2015", "Organization": "University of Hawaii", "AwardInstrument": "Continuing grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "4645450", "Abstract": "Cyberinfrastructure runs the gamut - from computing, networks, data stores, instruments, observatories, and sensors, to software and application codes - and promises to enable research at unprecedented scales, complexity, resolution, and accuracy. However, it is the research community that must make sense of all the data being amassed, so the SAGEnext (Scalable Adaptive Graphics Environment) framework is an innovative user-centered platform that connects people to their data and colleagues, locally and remotely, via tiled display walls, creating a portal, or wide-aperture &quot;digital lens,&quot; with which to view their data and one another. It enables Cyber-Mashups, or the ability to juxtapose and integrate information from multiple sources in a variety of resolutions, as easily as the Web makes access to lower-resolution images today. &lt;br/&gt;&lt;br/&gt;SAGEnext expands on a vibrant partnership among national and international universities, supercomputer centers, government laboratories and industry; 100 institutions worldwide use the current version of SAGE. For computational scientists, from such diverse fields as biology, earth science, genomics, or physics, SAGEnext will transform the way they manage the scale and complexity of their data. For computer scientists, SAGEnext is a platform for conducting research in human-computer interaction, cloud computing, and advanced networking. SAGEnext capabilities, integrating visualization application codes, cloud documents, stereo 3D, and new user-interaction paradigms, is unprecedented and heretofore not available, and will have a transformative effect on data exploration and collaboration, making cyberinfrastructure more accessible to end users, in both the laboratory and in the classroom. SAGEnext, integrated with advanced cyberinfrastructure tools, will transform the way today&apos;s scientists and future scientists manage the scale and complexity of their data, enabling them to more rapidly address problems of national priority - such as global climate change or homeland security - which benefits all mankind. These same tools can better communicate scientific concepts to public policy and government officials, and via museum exhibitions, to the general public.", "ProgramReferenceCode": ["7433", "8009", "9150", "9251", "8211"], "OrganizationZip": "968222234"}, {"OrganizationCity": "ATHENS", "OrganizationState": "GA", "Title": "SI2-SSE: Software Infrastructure for Revealing Gene and Genome Evolution, Anchored by Enhancement of Multiple Genome Alignment Software MCSCAN", "ProgramManager": "Daniel Katz", "State": "GA", "Program": ["CROSS-EF ACTIVITIES", "Software Institutes"], "ProgramElementCode": ["7275", "8004"], "StartDate": "02/01/2014", "OrganizationStreet": "310 East Campus Rd", "EndDate": "01/31/2017", "PrincipalInvestigator": "Xiyin Wang", "ARRAAmount": null, "OrganizationPhone": "7065425939", "AwardNumber": "1339727", "PIEmailAddress": "wangxy@uga.edu", "NSFDirectorate": "CSE", "Co-PIName": "Andrew Paterson", "LastAmendmentDate": "09/16/2013", "Organization": "University of Georgia Research Foundation Inc", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "493692", "Abstract": "The software MCSCAN, used to align multiple genomes, will be enhanced to contribute to deciphering the structure and evolutionary trajectories of eukaryotic genomes and genes, in particular addressing consequences of recursive whole-genome duplications. Burgeoning sets of eukaryotic genome sequences provide the foundation for a new spectrum of investigations into the functional and evolutionary consequences of gene and genome duplication, as well as the means to clarify knowledge of relationships among particular genes. The current software can only align small numbers of genomes; and layers of duplicated blocks produced by different genome duplication events are not readily deconvoluted, thus failing to provide crucial information toward understanding evolutionary trajectories of genomes and gene families. The enhanced software will mitigate these limitations.&lt;br/&gt;&lt;br/&gt;The enhanced software will greatly help researchers to reconstruct the evolutionary trajectories of genomes and gene families, including the singularly challenging genomes of angiosperms and other taxa that have experienced polyploidization events. In particular, multiple alignment (of an expanded number of genomes) will be preceded by a multiple-way comparison of homologous regions at the DNA level, which will provide a holographic grasp of layers of homology produced by different duplication events. To reflect the evolutionary trajectories of structural changes, genomes will be input in a stepwise manner, with those of simple structures first. The resulting multiple alignment will much more accurately depict evolutionary relationships between chromosomal regions from diverse genomes, and easily be visualized and understood by users. The core part of the software will be implemented using the C++ programming language while the visualization module will be developed in Python language. The multiple and pairwise alignment information will be stored in MySQL or SQLite databases. The software will be tamed to work under multiple operating systems, including MS Windows, UNIX and Linux. Online service will be developed using the Django Web framework and jQuery (a concise JavaScript Library), and added to our NSF-supported PGDD. The software will be formed by several independent modules, which can be freely used by other researchers. A to-be-constructed web server accompanying the software will show figures illustrating genome structures, comparison between different plants, and evolutionary changes inferred to have occurred over millions of years. These intuitive visual resources will benefit researchers seeking to understand the evolution of plants, as well as elementary and middle school students, and readers at local libraries. The program will regularly host visitors from other institutions, countries, and the public. The enhanced software and related results in genomic analysis will be reported in academic conferences.", "ProgramReferenceCode": ["7433", "8005"], "OrganizationZip": "306021589"}, {"OrganizationCity": "College Station", "OrganizationState": "TX", "Title": "Collaborative Research: SI2-SSE: A Petascale Numerical Library for Multiscale Phenomena Simulations", "ProgramManager": "Daniel Katz", "State": "TX", "Program": ["Software Institutes", "CDS&E"], "ProgramElementCode": ["8004", "8084"], "StartDate": "10/01/2013", "OrganizationStreet": "TEES State Headquarters Bldg.", "EndDate": "09/30/2016", "PrincipalInvestigator": "Diego Donzis", "ARRAAmount": null, "OrganizationPhone": "9798477635", "AwardNumber": "1339773", "PIEmailAddress": "donzis@tamu.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "09/11/2013", "Organization": "Texas A&M Engineering Experiment Station", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "139879", "Abstract": "Multiscale phenomena are a grand challenge for theory, simulations and experiments. It is difficult to understand phenomena at both highest and lowest scales, as well as all those in between. This challenge shows up in diverse fields. One of the long-standing problems in cosmology is understanding cosmological structure formation. In computer simulations of this process the challenge is increasing grid resolution while retaining the essential physics. In all-atom molecular dynamics simulations of enzymes the challenge is simulating systems with a large number of atoms while resolving long-range interactions and having sufficiently high throughput. Such simulations are critical in understanding important biological processes and eventually designing new drugs. Another prime example of multiscale phenomena is turbulent flows, a rich and complex subject of great relevance to many of the main technological issues of the day, including climate, energy, and the management of oil and biohazards. Understanding turbulent flows is critical for design of new transportation vehicles, improving efficiency of combustion processes and managing their environment pollution. Here simulations have been historically limited, and remain so, due to extremely high computational cost, even using the high-end computational systems available to researchers today. Reducing this cost, and efficiently using the computational resources, often requires specialized expertise, as well as significant development time and cost many research groups cannot afford. This project will develop a powerful suite of critical software components to provide tools for performing simulations of multiscale phenomena. &lt;br/&gt;&lt;br/&gt;The suite will implement state-of-the-art techniques for reducing communication cost, which has become the most important contributing factor to the total simulation cost, especially at larger scales. It will provide a flexible set of features that will make it usable in a great number of codes across the disciplines. In particular, the library will include user-friendly interfaces for Fourier transforms, spectral and compact differentiation in three dimensions, in addition to widely used communication routines (transposes, halo exchanges). This combination of emphasis on scalable performance and richness of features makes this suite unique among other libraries in existence today. Given the extraordinary challenge of simulations of multiscale phenomena, this library will provide a realistic path towards the Exascale. The suite will be available under an open source license. User outreach will be undertaken through the project website, mailing list, user surveys and presentations.", "ProgramReferenceCode": ["7433", "8005", "8084"], "OrganizationZip": "778454645"}, {"OrganizationCity": "West Lafayette", "OrganizationState": "IN", "Title": "SI2-SSE: Analyze Visual Data from Worldwide Network Cameras", "ProgramManager": "Daniel Katz", "State": "IN", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "08/01/2015", "OrganizationStreet": "Young Hall", "EndDate": "07/31/2018", "PrincipalInvestigator": "Yung-Hsiang Lu", "ARRAAmount": null, "OrganizationPhone": "7654941055", "AwardNumber": "1535108", "PIEmailAddress": "yunglu@purdue.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "02/03/2016", "Organization": "Purdue University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "515989", "Abstract": "Many network cameras have been deployed for a wide range of purposes, such as monitoring traffic, evaluating air pollution, observing wildlife, and watching landmarks. The data from these cameras can provide rich information about the natural environment and human activities. To extract valuable information from this network of cameras, complex computer programs are needed to retrieve data from the geographically distributed cameras and to analyze the data. This project creates a open source software infrastructure by solving many problems common to different types of analysis programs. By using this infrastructure, researchers can focus on scientific discovery, not writing computer programs. This project can improve efficiency and thus reduce the cost for running programs analyzing large amounts of data. This infrastructure promotes education because students can obtain an instantaneous view of the network cameras and use the visual information to understand the world. Better understanding of the world may encourage innovative solutions for many pressing issues, such as better urban planning and lower air pollution. This project can enhance diversity through multiple established programs that encourage underrepresented minorities to pursue careers in science and engineering. &lt;br/&gt;&lt;br/&gt;This project will combine: (1) the ability to retrieve data from many heterogeneous and distributed cameras, (2) the management of computational and storage resources using cloud computing, and (3) improved performance by reducing data movement, balancing loads among multiple cloud instances, and enhancing data-level parallelism. The project provides an application programming interface (API) that hides the underlying sophisticated infrastructure. This infrastructure will handle both real-time streaming data and archival data in a uniform way, so that the same analysis programs can be reused. This project has four major components: (1) a web-based user interface, (2) a database that stores the details about the network cameras, (3) a resource manager that allocates cloud instances, and (4) a computational engine that execute the programs written by users. The service-oriented architecture will allow new functions to be integrated more easily by the research community.", "ProgramReferenceCode": ["7433", "8005", "9251"], "OrganizationZip": "479072114"}, {"OrganizationCity": "Piscataway", "OrganizationState": "NJ", "Title": "SI2-SSE: RADICAL Cybertools: Scalable, Interoperable and Sustainable Tools for Science", "ProgramManager": "Rajiv Ramnath", "State": "NJ", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "01/01/2015", "OrganizationStreet": "33 Knightsbridge Road", "EndDate": "12/31/2017", "PrincipalInvestigator": "Shantenu Jha", "ARRAAmount": null, "OrganizationPhone": "8489320150", "AwardNumber": "1440677", "PIEmailAddress": "shantenu.jha@rutgers.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "07/24/2014", "Organization": "Rutgers University New Brunswick", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "499916", "Abstract": "To support science and engineering applications that are the basis of many societal and intellectual challenges in the 21st century, there is a need for comprehensive, balanced, and flexible distributed cyberinfrastructure (DCI). The process of designing and deploying such large-scale DCI, however, presents a critical and challenging research agenda. One specific challenge is to produce tools that provide a step change in the sophistication of problems that can be investigated using DCI, while being extensible, easy to deploy and use, as well as being compatible with a variety of other established tools. RADICAL Cybertools will meet these requirements by providing an abstractions-based suite of well-defined capabilities that are architected to support scalable, interoperable and sustainable science on a range of high-performance and distributed computing infrastructure. RADICAL Cybertools builds upon important theoretical advances, production-software-development best practices, and carefully-analyzed usage and programming models. RADICAL Cybertools is posed to play a role in grand-challenge problems, ranging from personalized medicine and health to understanding long-term global and regional climate. All software developed through the project will be open source and will be licensed under the MIT License (MIT). Version control on the SVN repository will be accessible via http://radical.rutgers.edu.&lt;br/&gt;&lt;br/&gt;Existing and current utilization of RADICAL Cybertools is built upon preliminary research prototypes of RADICAL Cybertools. There is a significant difference, however, in the quality and capability required to support scalable end-usage science, compared to that of a research prototype. It is the aim of this project to bridge this gap between the ability to serve as a research prototype versus the challenges of supporting scalable end-usage science. This will be achieved by addressing existing limitations of usability, functionality, and scalability. We will do so by utilizing conceptual and theoretical advances in the understanding of distributed systems and middleware, resulting in a scalable architecture and robust design. We will employ advances in performance engineering, data-intensive methods, and cyberinfrastructure to deliver the next generation of RADICAL Cybertools. This project will take the existing research prototypes of RADICAL Cybertools to the next level towards becoming a hardened, extensible, and sustainable tool that will support a greater number of users, application types, and resource types.", "ProgramReferenceCode": ["7433", "8005"], "OrganizationZip": "088543925"}, {"OrganizationCity": "Piscataway", "OrganizationState": "NJ", "Title": "Collaborative Research: SI2-CHE: ExTASY Extensible Tools for Advanced Sampling and analYsis", "ProgramManager": "Evelyn M. Goldfield", "State": "NJ", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes"], "ProgramElementCode": ["1253", "8004"], "StartDate": "09/01/2013", "OrganizationStreet": "33 Knightsbridge Road", "EndDate": "08/31/2016", "PrincipalInvestigator": "Shantenu Jha", "ARRAAmount": null, "OrganizationPhone": "8489320150", "AwardNumber": "1265788", "PIEmailAddress": "shantenu.jha@rutgers.edu", "NSFDirectorate": "MPS", "LastAmendmentDate": "08/28/2013", "Organization": "Rutgers University New Brunswick", "AwardInstrument": "Standard Grant", "NsfOrganization": "CHE", "AwardedAmountToDate": "617005", "Abstract": "Collaborative Research: SI2-CHE&lt;br/&gt;ExTASY Extensible Tools for Advanced Sampling and analYsis&lt;br/&gt;&lt;br/&gt;An international team consisting of Cecilia Clementi(Rice University), Mauro Maggioni (Duke University) Shantenu Jha (Rutgers University), Glenn Martyna (BM T. J. Watson Laboratory ), Charlie Laughton (University of Nottingham), Ben Leimkuhler ( University of Edinburgh), Iain Bethune (University of Edinburgh) and Panos Parpas(Imperial College) are supported through the SI2-CHE program for the development of ExTASY -- Extensible Toolkit for Advanced Sampling and analYsis, -- a conceptual and software framework that provides a step-change in the sampling of the conformational space of macromolecular systems. Specifically, ExTASY is a lightweight toolkit to enable first-class support for ensemble-based simulations and their seamless integration with dynamic analysis capabilities and ultra-large time step integration methods, whilst being extensible to other community software components via well-designed and standard interfaces. &lt;br/&gt;&lt;br/&gt; The primary impacts of this project are in the biological sciences. This software advances our understanding of biologically important systems, as it can be used to obtain fast and accurate sampling of the conformational dynamics of stable proteins ? a prerequisite for the accurate prediction of thermodynamic parameters and biological functions. It also allows tackling systems like intrinsically disordered proteins, which can be beyond the reach of classical structural biology. Along with the research itself, the PIs are involved with outreach programs to attract high school students to science.", "ProgramReferenceCode": ["5918", "5946", "5950", "7433", "8009"], "OrganizationZip": "088543925"}, {"OrganizationCity": "Houston", "OrganizationState": "TX", "Title": "SI2-SSI: Collaborative Research: Scalable Infrastructure for Enabling Multiscale and Multiphysics Applications in Fluid Dynamics, Solid Mechanics, and Fluid-Structure Interaction", "ProgramManager": "Rajiv Ramnath", "State": "TX", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes", "CDS&E-MSS", "CDS&E"], "ProgramElementCode": ["1253", "8004", "8069", "8084"], "StartDate": "08/01/2015", "OrganizationStreet": "6100 MAIN ST", "EndDate": "07/31/2020", "PrincipalInvestigator": "Matthew Knepley", "ARRAAmount": null, "OrganizationPhone": "7133484820", "AwardNumber": "1607042", "PIEmailAddress": "knepley@rice.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "01/28/2016", "Organization": "William Marsh Rice University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "262655", "Abstract": "Many biological and biomedical systems involve the interaction of a flexible structure and a fluid. These systems range from the writhing and coiling of DNA, to the beating and pumping of cilia and flagella, to the flow of blood in the body, to the locomotion of fish, insects, and birds. This project aims to develop advanced software infrastructure for performing dynamic computer simulations of such biological and biomedical systems. To facilitate the deployment of this software in a range of scientific and engineering applications, this project will develop new software capabilities in concert with new computer models that use the software. Specific application domains to be advanced in this project include models of aquatic locomotion that can be used to understand the neural control of movement and ultimately to develop new treatments for neurological pathologies such as spinal cord injuries, and models that simulate the interaction between the electrophysiology of the heart and the contractions of the heart that pump blood throughout the body, which could lead to improved approaches to treating heart disease. The software to be developed within the project is freely available online and is used by a number of independent research groups in a variety of scientific and engineering domains. It is being actively used in projects that model different aspects of cardiovascular dynamics, such as platelet aggregation and the dynamics of natural and prosthetic heart valves, and in projects that study other biological problems, including cancer dynamics, insect flight, aquatic locomotion, and the dynamics of phytoplankton. The software is also being applied to non-biological problems, including nanoscale models of colloidal suspensions and models of active particles. The improved methods and software to be developed in this project will thereby have a broad and sustained impact on a large number of ongoing research efforts in the biological and biomedical sciences and other scientific and engineering disciplines.&lt;br/&gt;&lt;br/&gt;The immersed boundary (IB) method is a broadly applicable framework for modeling and simulating fluid-structure interaction (FSI). The IB method was introduced to model the fluid dynamics of heart valves, and subsequent development initially focused on simulating cardiac fluid dynamics. This methodology is broadly useful, however, and has been applied to a variety of problems in which a fluid flow interacts with immersed structures, including elastic bodies, bodies with known or prescribed deformational kinematics, and rigid bodies. Extensions of the IB method have also been developed to model electrophysiological systems and systems with chemically active structures. To improve the efficiency of the IB method, the PI has developed adaptive versions of the IB method that employ structured adaptive mesh refinement (AMR) to deploy high spatial resolution only where needed. These methods have been implemented within the IBAMR software framework, which provides parallel implementations of the IB method and its extensions that leverage high-quality computational libraries including SAMRAI, PETSc, and libMesh. This project will further extend the IBAMR software by implementing modeling and discretization technologies required by the research applications of current and prospective users of the software, by developing improved solver infrastructure facilitated by the implementation of native support for structured AMR discretizations in the PETSc library, and by integrating with existing high-quality software tools for model development, deployment, and analysis. IBAMR is freely distributed online and is used within a number of independent research groups both to the further development of the IB method and also to its application to simulate diverse problems in fluid dynamics and FSI. By enhancing IBAMR, this project will also enhance the ability of these and other researchers to construct detailed models without requiring those researchers to develop the significant software infrastructure needed to perform such simulations. This project will also develop general-purpose support for AMR discretizations in PETSc, a software library with thousands of active users, ~400 downloads per month, and numerous applications. The work of this project will help to grow the IBAMR user community of students and researchers by developing UI tools for building models, running simulations, and analyzing results. Students will be actively engaged in all aspects of the project, including code, method, and model development.", "ProgramReferenceCode": ["7433", "8004", "8009", "8084"], "OrganizationZip": "770051827"}, {"OrganizationCity": "Chicago", "OrganizationState": "IL", "Title": "SI2-SSI: Collaborative Research: Scalable Infrastructure for Enabling Multiscale and Multiphysics Applications in Fluid Dynamics, Solid Mechanics, and Fluid-Structure Interaction", "ProgramManager": "Rajiv Ramnath", "State": "IL", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes", "CDS&E-MSS", "CDS&E"], "ProgramElementCode": ["1253", "8004", "8069", "8084"], "StartDate": "08/01/2015", "OrganizationStreet": "5801 South Ellis Avenue", "EndDate": "02/29/2016", "PrincipalInvestigator": "Matthew Knepley", "ARRAAmount": null, "OrganizationPhone": "7737028669", "AwardNumber": "1450339", "PIEmailAddress": "knepley@rice.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/11/2015", "Organization": "University of Chicago", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "262655", "Abstract": "Many biological and biomedical systems involve the interaction of a flexible structure and a fluid. These systems range from the writhing and coiling of DNA, to the beating and pumping of cilia and flagella, to the flow of blood in the body, to the locomotion of fish, insects, and birds. This project aims to develop advanced software infrastructure for performing dynamic computer simulations of such biological and biomedical systems. To facilitate the deployment of this software in a range of scientific and engineering applications, this project will develop new software capabilities in concert with new computer models that use the software. Specific application domains to be advanced in this project include models of aquatic locomotion that can be used to understand the neural control of movement and ultimately to develop new treatments for neurological pathologies such as spinal cord injuries, and models that simulate the interaction between the electrophysiology of the heart and the contractions of the heart that pump blood throughout the body, which could lead to improved approaches to treating heart disease. The software to be developed within the project is freely available online and is used by a number of independent research groups in a variety of scientific and engineering domains. It is being actively used in projects that model different aspects of cardiovascular dynamics, such as platelet aggregation and the dynamics of natural and prosthetic heart valves, and in projects that study other biological problems, including cancer dynamics, insect flight, aquatic locomotion, and the dynamics of phytoplankton. The software is also being applied to non-biological problems, including nanoscale models of colloidal suspensions and models of active particles. The improved methods and software to be developed in this project will thereby have a broad and sustained impact on a large number of ongoing research efforts in the biological and biomedical sciences and other scientific and engineering disciplines.&lt;br/&gt;&lt;br/&gt;The immersed boundary (IB) method is a broadly applicable framework for modeling and simulating fluid-structure interaction (FSI). The IB method was introduced to model the fluid dynamics of heart valves, and subsequent development initially focused on simulating cardiac fluid dynamics. This methodology is broadly useful, however, and has been applied to a variety of problems in which a fluid flow interacts with immersed structures, including elastic bodies, bodies with known or prescribed deformational kinematics, and rigid bodies. Extensions of the IB method have also been developed to model electrophysiological systems and systems with chemically active structures. To improve the efficiency of the IB method, the PI has developed adaptive versions of the IB method that employ structured adaptive mesh refinement (AMR) to deploy high spatial resolution only where needed. These methods have been implemented within the IBAMR software framework, which provides parallel implementations of the IB method and its extensions that leverage high-quality computational libraries including SAMRAI, PETSc, and libMesh. This project will further extend the IBAMR software by implementing modeling and discretization technologies required by the research applications of current and prospective users of the software, by developing improved solver infrastructure facilitated by the implementation of native support for structured AMR discretizations in the PETSc library, and by integrating with existing high-quality software tools for model development, deployment, and analysis. IBAMR is freely distributed online and is used within a number of independent research groups both to the further development of the IB method and also to its application to simulate diverse problems in fluid dynamics and FSI. By enhancing IBAMR, this project will also enhance the ability of these and other researchers to construct detailed models without requiring those researchers to develop the significant software infrastructure needed to perform such simulations. This project will also develop general-purpose support for AMR discretizations in PETSc, a software library with thousands of active users, ~400 downloads per month, and numerous applications. The work of this project will help to grow the IBAMR user community of students and researchers by developing UI tools for building models, running simulations, and analyzing results. Students will be actively engaged in all aspects of the project, including code, method, and model development.", "ProgramReferenceCode": ["7433", "8004", "8009", "8084"], "OrganizationZip": "606375418"}, {"OrganizationCity": "Cincinnati", "OrganizationState": "OH", "Title": "Collaborative Research: SI2-SSI: Data-Intensive Analysis for High Energy Physics (DIANA/HEP)", "ProgramManager": "Daniel Katz", "State": "OH", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "COMPUTATIONAL PHYSICS", "Software Institutes"], "ProgramElementCode": ["1253", "7244", "8004"], "StartDate": "05/01/2015", "OrganizationStreet": "University Hall, Suite 530", "EndDate": "04/30/2019", "PrincipalInvestigator": "Michael Sokoloff", "ARRAAmount": null, "OrganizationPhone": "5135564358", "AwardNumber": "1450319", "PIEmailAddress": "sokoloff@physics.uc.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "05/07/2015", "Organization": "University of Cincinnati Main Campus", "AwardInstrument": "Continuing grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "805000", "Abstract": "Advanced software plays a fundamental role for large scientific projects. The primary goal of DIANA/HEP (Data Intensive ANAlysis for High Energy Physics) is developing state-of-the-art tools for experiments that acquire, reduce, and analyze petabytes of data. Improving performance, interoperability, and collaborative tools through modifications and additions to packages broadly used by the community will allow users to more fully exploit the data being acquired at CERN&apos;s Large Hadron Collider (LHC) and other facilities. These experiments are addressing questions at the heart of physics: What are the underlying constituents of matter? And how do they interact? With the discovery of the Higgs boson in 2012, the Standard Model of particle physics is complete. It provides an excellent description of known particles and forces. However, the most interesting questions remain open: What is the dark matter which pervades the universe? Does space-time have additional symmetries or extend beyond the 3 spatial dimensions we know? What is the mechanism stabilizing the Higgs boson mass from enormous quantum corrections? The next generation of experiments will collect exabyte-scale data samples to provide answers. Analyzing this data will require new and better tools. &lt;br/&gt;&lt;br/&gt;First, the project will provide the CPU and IO performance needed to reduce the iteration time so crucial to explore new ideas. It will develop software to effectively exploit emerging many- and multi-core hardware. It will establish infrastructure for a higher-level of collaborative analysis, building on the successful patterns used for the Higgs boson discovery and enabling a deeper communication between the theoretical community and the experimental community. DIANA?s products will sit in the ROOT framework, already used by the HEP community of more than 10000 particle and nuclear physicists. By improving interoperability with the larger scientific software ecosystem, DIANA will incorporate best practices and algorithms from other disciplines into HEP. Similarly, the project will make its computing insights, tools, and novel ideas related to collaborative analysis, standards for data preservation, and best practices for treating software as a research product available to the larger scientific community. Finally, to improve the quality of the next generation of software engineers in HEP, DIANA will host an annual workshop on analysis tools and establish a fellowship program.", "ProgramReferenceCode": ["7433", "8009", "8084"], "OrganizationZip": "452210222"}, {"OrganizationCity": "Cincinnati", "OrganizationState": "OH", "Title": "SI2-SSE: Scalable Big Data Clustering by Random Projection Hashing", "ProgramManager": "Daniel Katz", "State": "OH", "Program": ["SPECIAL PROJECTS - CCF", "Software Institutes"], "ProgramElementCode": ["2878", "8004"], "StartDate": "09/01/2014", "OrganizationStreet": "University Hall, Suite 530", "EndDate": "08/31/2017", "PrincipalInvestigator": "Philip Wilsey", "ARRAAmount": null, "OrganizationPhone": "5135564358", "AwardNumber": "1440420", "PIEmailAddress": "philip.wilsey@uc.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/08/2014", "Organization": "University of Cincinnati Main Campus", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "498127", "Abstract": "This project plans to develop a distributed algorithm for secure clustering of high dimensional data sets. Fields in health and biology are significantly benefited by data clustering scalability. Bioinformatic problems such as Micro Array clustering, Protein-Protein interaction clustering, medical resource decision making, medical image processing, and clustering of epidemiological events all serve to benefit from larger dataset sizes. The algorithm under development, called Random Projection Hash or RPHash, utilizes aspects of locality sensitive hashing (LSH) and multi-probe random projection for computational scalability and linear achievable gains from parallel speed. Furthermore, RPHash provides data anonymization through destructive manipulation of the data preventing de-anonymization attacks beyond standard best practices database security methods. RPHash will be deployable on commercially available cloud resources running the Hadoop (MRv2) implementation of MapReduce. The exploitation of general purpose cloud processing solutions allows researchers to scale their processing needs using virtually limitless commercial processing resources.&lt;br/&gt;&lt;br/&gt;The RPHash algorithm uses various recent techniques in data mining along with a new approach toward achieving algorithmic scalability on distributed systems. The basic intuition of RPHash is to combine multi-probe random projection with discrete space quantization. Regions of high density are then regarded as centroid candidates. To follow common parameterized, k-means methods, the top k regions will be selected. The focus on a randomized, and thus non-deterministic, clustering algorithm is somewhat uncommon in computing, but common for ill-posed, combinatorially restrictive problems such as clustering and partitioning. Despite theoretical results showing that k-means has an exponential worst case complexity, many real world problems tend to fair much better under k-means and other similar algorithms.", "ProgramReferenceCode": ["7433", "8004", "8005", "2878"], "OrganizationZip": "452210222"}, {"OrganizationCity": "Stony Brook", "OrganizationState": "NY", "Title": "Collaborative Research: SI2-SSI: Task-Based Environment for Scientific Simulation at Extreme Scale (TESSE)", "ProgramManager": "Daniel Katz", "State": "NY", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "DMR SHORT TERM SUPPORT", "Software Institutes"], "ProgramElementCode": ["1253", "1712", "8004"], "StartDate": "05/15/2015", "OrganizationStreet": "WEST 5510 FRK MEL LIB", "EndDate": "04/30/2018", "PrincipalInvestigator": "Robert Harrison", "ARRAAmount": null, "OrganizationPhone": "6316329949", "AwardNumber": "1450344", "PIEmailAddress": "robert.harrison@stonybrook.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "05/20/2015", "Organization": "SUNY at Stony Brook", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "599808", "Abstract": "This project, TESSE, is developing a new-generation programming environment that uniquely address the needs of emerging computational models in chemistry and other fields, and allowing these models to reap the benefits of the computer hardware of tomorrow. The project thus is bringing closer the ability to predict properties of matter and design matter transformations of importance to the technological leadership and energy security of the US. TESSE&apos;s impact has scientific, cyberinfrastructure, and interdisciplinary educational aspects. TESSE helps carry widely used scientific applications (NWChem, MADNESS, MPQC, etc.) into a whole new generation of many-core and accelerated architectures, while at the same time dramatically improving programmer productivity. In terms of cyberinfrastructure, TESSE&apos;s PaRSEC runtime system delivers capabilities that many libraries and advanced applications will require for high performance on large scale and hybridized systems.&lt;br/&gt;&lt;br/&gt;The goals of this project, Task-based Environment for Scientific Simulation at Extreme Scale (TESSE), are to design and demonstrate via substantial scientific simulations within chemistry and other disciplines a prototype software framework that provides a groundbreaking response to the twin problems of portable performance and programmer productivity for advanced scientific applications on emerging massively-parallel, hybrid, many-core systems. TESSE will create a viable foundation for a new generation of science codes, one which enables even more rapid exploration of new physical models, provides greatly enhanced performance portability through directed acyclic graph (DAG) scheduling and auto-tuned kernels, and works towards full interoperability between major chemistry packages through compatible runtimes and data structures. TESSE will mature to become a standard, widely available, community-based and broadly-applicable parallel programming environment complementing and rivaling MPI/OpenMP. This is needed due to the widely appreciated shortfalls of existing mainstream programming models and the already huge successes of the existing DAG-based runtimes that are the foundation of the next generation of NSF- and DOE-supported (Sca)LAPACK high-performance linear algebra libraries.", "ProgramReferenceCode": ["7433", "8009", "8084", "9216"], "OrganizationZip": "117940001"}, {"OrganizationCity": "Cambridge", "OrganizationState": "MA", "Title": "SI2-SSI: Collaborative Research: A Computational Materials Data and Design Environment", "ProgramManager": "Daniel Katz", "State": "MA", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "DMR SHORT TERM SUPPORT", "CHEMISTRY PROJECTS", "Software Institutes"], "ProgramElementCode": ["1253", "1712", "1991", "8004"], "StartDate": "10/01/2012", "OrganizationStreet": "77 MASSACHUSETTS AVE", "EndDate": "09/30/2017", "PrincipalInvestigator": "Gerbrand Ceder", "ARRAAmount": null, "OrganizationPhone": "6172531000", "AwardNumber": "1147503", "PIEmailAddress": "gceder@mit.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "09/20/2012", "Organization": "Massachusetts Institute of Technology", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "450000", "Abstract": "TECHNICAL SUMMARY&lt;br/&gt;The Office of Cyberinfrastructure, Division of Materials Research, and Chemistry Division contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation. This award supports development of new theory and tools to enable rapid and efficient calculation of atomic level material properties. The incredible advances in computing power and tools of atomic scale simulation have now made it possible to predict critical properties for existing and new materials without experimental input. However, present simulation approaches typically require researchers to perform many steps by hand, which is both slow and error prone compared to what a computer can do. Through computer codes that automate the tasks in first principles modeling human bottlenecks can be removed and predictive capabilities of first principles simulation techniques can be accelerated by orders of magnitude. Such a high-throughput computing approach will enable generation of critical materials data on an unprecedented scale and open new doors for material science.&lt;br/&gt;&lt;br/&gt;The team will develop tools for the specific challenges of predicting point defect properties, atomic diffusion, and surface stability, with a focus on automating steps to enable computations on a massive scale. The PIs will use state-of-the-art first principles quantum mechanical methods. Best practices for treating the multiple issues of charged defect calculations, for example convergence with cell size and band gap errors, will be refined and automated for rapid execution. Similarly, tools to identify diffusion pathways and determine their barriers will be streamlined to allow users to quickly identify transport properties of new systems. New theoretical approaches to modeling charged surfaces will be developed to enable simulation of surfaces in more realistic environments. This award will support prediction of properties that play a critical role in advancing a wide range of technologies, from improving semiconductors for next generation computers to better fuel cells for more efficient energy conversion. Software tools and data produced by this effort will enable researchers to predict properties for thousands of materials with almost no human effort, accelerating the pace at which researchers can develop new materials technologies.&lt;br/&gt;&lt;br/&gt;Software and data developed from this award will be shared with academic and industrial researchers through modules on the web, scientific journals and presentations at national and international conferences. This award supports two workshops to educate researchers about the latest opportunities to use high-throughput computing of atomic scale properties for materials development. Students will be trained to work at the critical interface of the computer and physical sciences, supporting a generation of scientists who use modern computers to their fullest potential to develop new understanding and technology.&lt;br/&gt;&lt;br/&gt;NON-TECHNICAL SUMMARY&lt;br/&gt;The Office of Cyberinfrastructure, Division of Materials Research, and Chemistry Division contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation. This award supports development of new theory and tools to enable rapid and efficient calculation of atomic level material properties. The incredible advances in computing power and tools of atomic scale simulation have now made it possible to predict critical properties for existing and new materials without experimental input. However, present simulation approaches typically require researchers to perform many steps by hand, which is both slow and error prone compared to what a computer can do. Through computer codes that automate the tasks in first-principles modeling human bottlenecks can be removed and predictive capabilities of first principles simulation techniques can be accelerated by orders of magnitude. Such a high-throughput computing approach will enable generation of critical materials data on an unprecedented scale and open new doors for material science.&lt;br/&gt;&lt;br/&gt;The team will develop tools for the specific challenges of predicting point defect properties, atomic diffusion, and surface stability, with a focus on automating steps to enable computations on a massive scale. These properties play a critical role in advancing a wide range of technologies, from improving semiconductors for next generation computers to better fuel cells for more efficient energy conversion. Software tools and data produced by this effort will enable researchers to predict properties for thousands of materials with almost no human effort, accelerating the pace at which researchers can develop new materials technologies.&lt;br/&gt;&lt;br/&gt;Software and data developed from this award will be shared with academic and industrial researchers through modules on the web, scientific journals and presentations at national and international conferences. In particular, this award will support two workshops to educate researchers about the latest opportunities to use high-throughput computing of atomic scale properties for materials development. This award will train students to work at the critical interface of the computer and physical sciences, supporting a generation of scientists who use modern computers to their fullest potential to develop new understanding and technology.", "ProgramReferenceCode": ["1982", "7237", "7433", "7569", "7644", "8009", "9216", "9263", "1253", "1712", "1991", "8004"], "OrganizationZip": "021394301"}, {"OrganizationCity": "Cambridge", "OrganizationState": "MA", "Title": "Collaborative Research: SI2-CHE:Developing First Principles Monte Carlo Methods for Reactive Phase and Sorption Equilibria in the CP2k Software Suite", "ProgramManager": "Evelyn M. Goldfield", "State": "MA", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes"], "ProgramElementCode": ["1253", "8004"], "StartDate": "04/15/2013", "OrganizationStreet": "77 MASSACHUSETTS AVE", "EndDate": "03/31/2016", "PrincipalInvestigator": "Troy Van Voorhis", "ARRAAmount": null, "OrganizationPhone": "6172531000", "AwardNumber": "1265624", "PIEmailAddress": "tvan@MIT.EDU", "NSFDirectorate": "MPS", "LastAmendmentDate": "04/02/2013", "Organization": "Massachusetts Institute of Technology", "AwardInstrument": "Standard Grant", "NsfOrganization": "CHE", "AwardedAmountToDate": "137775", "Abstract": "An international research team consisting of Ilja Siepmann, Ben Lynch (University of Minnesota), Neeraj Rai (Mississippi State University), Troy Van Voorhis (Massachusetts Institute of Technology), Ben Slater (University College London), Michiel Sprik (University of Cambridge), Adam Carter (Edinburgh Parallel Computing Centre), J&#252;rg Hutter (University of Zurich), I-Feng Kuo (Lawrence Livermore National Laboratory), Christopher Mundy (Pacific Northwest National Laboratory), Joost VandeVondele (ETH Zurich), and Rodolphe Vuilleumier (University Pierre &amp; Marie Curie Paris) is collaborating to develop and implement new theoretical methods in the CP2K computational chemistry software suite. These new methodologies enable the predictive modeling of reactive multi-phase systems, including free energy landscapes and product yields, where the system interactions are described by Kohn-Sham density functional theory with van der Waals and hybrid functionals. Markov chain Monte Carlo approaches utilizing smart moves with asymmetric underlying matrices, such as the aggregation-volume-bias and configurational-bias Monte Carlo methods, and the Gibbs ensemble framework are employed for efficient exploration of the phase space for reactive single- and multi-phase equilibria in bulk and in confinement. The U.S. based research team is supported jointly by the Chemistry Division in MPS and the Office of Cyberinfrastucture. Funds for the UK based research team are provided by the EPSRC.&lt;br/&gt;&lt;br/&gt;The software infrastructure is advanced by the development of efficient and accurate methodologies for reactive phase and sorption equilibria that are applicable to chemical processes in diverse science and engineering applications. The extensively validated methodologies will be incorporated into the open-source CP2K software suite to make them available to a large user base. The new software can be used to identify optimal reaction conditions and separation processes for sustainable chemistry. The collaborative research team plans to use the new software for the investigation of reactive processes that address critical needs of society (fertilizers for food supply, fuels from renewable sources, and environmentally benign chemical processes).", "ProgramReferenceCode": ["5918", "5946", "5950", "7433", "8009", "8650"], "OrganizationZip": "021394301"}, {"OrganizationCity": "NOTRE DAME", "OrganizationState": "IN", "Title": "SI2-SSI: Collaborative Research: STORM: A Scalable Toolkit for an Open Community Supporting Near Realtime High Resolution Coastal Modeling", "ProgramManager": "Rajiv Ramnath", "State": "IN", "Program": ["PHYSICAL OCEANOGRAPHY", "SPECIAL PROJECTS - CISE", "SPECIAL PROJECTS - CCF", "Software Institutes", "EarthCube"], "ProgramElementCode": ["1610", "1714", "2878", "8004", "8074"], "StartDate": "10/01/2014", "OrganizationStreet": "940 Grace Hall", "EndDate": "09/30/2018", "PrincipalInvestigator": "Joannes Westerink", "ARRAAmount": null, "OrganizationPhone": "5746317432", "AwardNumber": "1339738", "PIEmailAddress": "jjw@nd.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Tim Stitt", "Damrongsak Wirasaet"], "LastAmendmentDate": "10/20/2014", "Organization": "University of Notre Dame", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "730000", "Abstract": "The ADCIRC coastal circulation and storm surge model has a long standing track record of extremely high societal impact. It has been used to define risk (e.g., 100-yr, 500-yr coastal flood levels) for the FEMA National Flood Insurance Program in coastal states from New York to Texas, it has been used to design the multi-billion dollar Hurricane and Storm Damage Risk Reduction System around greater New Orleans and southern Louisiana by the Army Corps of Engineers, it is currently run operationally by NOAA/NWS National Center for Environmental Prediction to forecast storm surge, to name just a few of its current and recent applications. Thus there is a well-established user network in place to convert improvements in ADCIRC into significant broader impacts. The proposed research provides transformative intellectual contributions that focus on applying new parallelization schemes to enable major advances in the algorithms, implementation and utilization of the ADCIRC model. The broadening of ADCIRC to a multi-algorithmic framework and the resulting performance gains that are anticipated will help ensure ADCIRC&apos;s sustainability as a core community model for at least the next 20 years. In addition, the proposed collaboration will impact computer science by serving as a high impact use case to inform the design of new approaches to efficient scalable computing. Together, the advancements in coastal modeling and parallelization technology will make a significant contribution to the science of modeling and HPC. The results of the proposed research will be disseminated to a wider community through ongoing educational outreach activities at the participating organizations as well as through refereed conference and journal papers, and invited presentations. The involvement of graduate students and post-doctoral fellows will be crucial towards the success of this project. The PIs have a long history of training and mentoring students and post-docs in computational science and engineering, coastal engineering and marine science. The recruitment and involvement of underrepresented groups in these efforts has always been a high priority. In addition, aspects of the proposed research will be incorporated into the curricula of several courses taught by the PIs in the areas of finite element methods, scientific computation, hydrology and oceanography.&lt;br/&gt;The aim of this project is to broaden the ADCIRC coastal circulation and storm surge model from a successful, but somewhat static coastal modeling tool that is tied to a single solution algorithm and the MPI parallelization paradigm, to a dynamic computational platform that is comprised of multiple solution algorithms, that readily admits new solution algorithms and that is built on a transformational new parallelization scheme that will allow us to scale to at least 256k compute cores on modern high performance computing (HPC) systems. We will do this by creating a living, evolving coastal modeling framework that will continue to lead the community in merging physical science / engineering and high performance computing and we will make the framework available to the broader community as a sustainable long term solution for its coastal modeling needs. In addition we will utilize these advancements in the highly demanding coastal storm surge forecasting system that we presently operate to demonstrate both improved robustness and speed of the model solution. We expect this effort will shorten the time required to provide reliable forecasting results and improve our ability to provide highly resolved, accurate, and physically complete predictions on an unprecedented scale. Concurrently, it should enable the use of smaller resources for simulations of increased scale which improves the usability and widens the applicability of ADCIRC in a broader community. The development of tightly integrated web-oriented products like CERA (www.coastalemergency.org) will enable the wide and timely dissemination of forecast modeling results to reach a broad audience.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "465565708"}, {"OrganizationCity": "Logan", "OrganizationState": "UT", "Title": "Collaborative Research: SI2-SSI: An Interactive Software Infrastructure for Sustaining Collaborative Community Innovation in the Hydrologic Sciences", "ProgramManager": "Daniel Katz", "State": "UT", "Program": ["ADVANCES IN BIO INFORMATICS", "ECOSYSTEM STUDIES", "METHOD, MEASURE & STATS", "HYDROLOGIC SCIENCES", "ENVIRONMENTAL SUSTAINABILITY", "PetaApps", "CDI TYPE I"], "ProgramElementCode": ["1165", "1181", "1333", "1579", "7643", "7691", "7750"], "StartDate": "07/01/2012", "OrganizationStreet": "Sponsored Programs Office", "EndDate": "06/30/2017", "PrincipalInvestigator": "David Tarboton", "ARRAAmount": null, "OrganizationPhone": "4357971226", "AwardNumber": "1148453", "PIEmailAddress": "dtarb@usu.edu", "NSFDirectorate": "CSE", "Co-PIName": ["David Maidment", "Richard Hooper", "Daniel Ames", "Jonathan Goodall", "Jennifer Arrigo"], "LastAmendmentDate": "07/31/2014", "Organization": "Utah State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "2401939", "Abstract": "Water, its quality, quantity, accessibility, and management, is crucial to society. However, our ability to model and quantitatively understand the complex interwoven environmental processes that control water and its availability is severely hampered by inadequate tools related to hydrologic data discovery, systems integration, modeling/ simulation, and education. This project develops sustainable cyberinfrastructure for better access to water-related data and models in the hydrologic sciences, enabling hydrologists and other associated communities to collaborate and combine data and models from multiple sources. It will provide new ways in which hydrologic knowledge is created and applied to better understand water availability, quality, and dynamics. It will also help to provide a more comprehensive understanding of the interactions between natural and engineered aspects of the water cycle. These goals will be achieved through the development of interoperable cyberinfrastructure tools and the creation of an online collaborative environment, called HydroShare, which enables scientists to easily discover and access hydrologic and related data and models, retrieve them to their desktop, and perform analyses in a high performance computing environment. The software to be developed will take advantage of existing NSF cyberinfrastructure (iRODS, HUBzero, CSDMS, CUAHSI HIS) and be created as open source code. Its development will be end user-driven. In terms of broader impacts, the project builds essential infrastructure for science by developing software tools and computing environments to allow better understanding of the impacts of climate change (i.e., floods, droughts, biofuels, etc.) and to allow improved water resource development and the management of freshwater resources both above and below ground. Resulting software will be made publicly available and provides a strong student and workforce training/education component. In addition, the project supports an institution in an EPSCoR state and engages, as a PI, a person who is from a group under-represented in the sciences and engineering.", "ProgramReferenceCode": ["145E", "1579", "7433", "8009", "1165", "1181", "1333", "7643", "7691", "7750", "8004"], "OrganizationZip": "843221415"}, {"OrganizationCity": "Logan", "OrganizationState": "UT", "Title": "SI2-SSE: Interdisciplinary Software Infrastructure for Differential Geometry, Lie Theory and their Applications", "ProgramManager": "Daniel Katz", "State": "UT", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "DYNAMICAL SYSTEMS", "Software Institutes", "CDS&E-MSS"], "ProgramElementCode": ["1253", "7478", "8004", "8069"], "StartDate": "06/01/2012", "OrganizationStreet": "Sponsored Programs Office", "EndDate": "05/31/2016", "PrincipalInvestigator": "Ian Anderson", "ARRAAmount": null, "OrganizationPhone": "4357971226", "AwardNumber": "1148331", "PIEmailAddress": "ian.anderson@usu.edu", "NSFDirectorate": "CSE", "Co-PIName": "Charles Torre", "LastAmendmentDate": "05/29/2012", "Organization": "Utah State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "360845", "Abstract": "The goal of this proposal is to further develop symbolic software for the field of differential geometry and those areas of mathematics, physics and engineering where &lt;br/&gt;differential geometry plays an essential role. The proposed work will: provide new functionalities requested by the user community, complete packages currently under &lt;br/&gt;development, redesign critical components for improved computational efficiency, develop upgrades of existing algorithms and code whose performance does not support the demands of research. Specific objectives include [1] the development of a new coordinate-free computational environment for work with abstract differential forms and for tensor analysis on homogeneous spaces; [2] software for the structure theory of real/complex Lie algebras and their representations; [3] implementation of the theory of Young tableaux for tensors with symmetry to address resource and performance problems arising in large tensor computations; [4] new programs for symbolic computations for sub-manifold theory in Riemannian geometry, complex manifolds and Kahler geometry, and symplectic geometry; [5] a comprehensive new package for exterior differential systems; and [6] expansion of various data-bases of Lie algebras, differential equations, and exact solutions in general relativity.&lt;br/&gt;&lt;br/&gt;Of all the core disciplines in mathematics, differential geometry is unique in that it interfaces with so many other subjects in pure mathematics, applied mathematics, physics, engineering, and even computer science. The PI&apos;s DifferentialGeometry (DG) software package has laid the foundation for a single, unified symbolic computational environment for research and teaching in differential geometry and its many application areas. The goal of this proposal is to add new computational environments to address specific application needs, to add basic functionalities that will bring various sub-packages to maturity, to upgrade routines with performance limitation, and significantly extend the DG data-bases of Lie algebras, group actions, integrable systems, and solutions of the Einstein equations. Earlier versions of this software have established a significant user community. Community feedback has dictated much of the specific program agenda in this proposal. A unique partnership between Utah State University and Maplesoft insures that the DG software meets the high standards of reliability, ease of use, documentation and support, and longevity that a extended user community (with diverse levels of symbolic computational experience) demands.&lt;br/&gt;&lt;br/&gt;While originially designed as a research tool, DG also provides an innovative approach to teaching differential geometry and its applications in the classroom. All developments in DG are implemented with this in mind.&lt;br/&gt;&lt;br/&gt;The PI will host a workshop at Utah State University entitled: Symbolic Methods in Differential Geometry, Lie Theory and Applications. This workshop will consist of &lt;br/&gt;hands-on training sessions, and lectures on applications of symbolic methods to problems in differential geometry. This workshop will also provide an ideal &lt;br/&gt;venue to survey participant research interests to drive future code development.&lt;br/&gt;&lt;br/&gt;Student involvement at the undergraduate and graduate levels is an important component of this project. The experience gained in working with computer algebra systems in general, and differential geometry in particular, is valuable to the student for future educational activities and/or future employment.", "ProgramReferenceCode": ["7433", "7683", "8005", "9150", "1253", "7478", "8004"], "OrganizationZip": "843221415"}, {"OrganizationCity": "CHAMPAIGN", "OrganizationState": "IL", "Title": "SI2-SSE: AttackTagger: Early Threat Detection for Scientific Cyberinfrastructure", "ProgramManager": "Rajiv Ramnath", "State": "IL", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "09/01/2015", "OrganizationStreet": "SUITE A", "EndDate": "08/31/2018", "PrincipalInvestigator": "Alexander Withers", "ARRAAmount": null, "OrganizationPhone": "2173332187", "AwardNumber": "1535070", "PIEmailAddress": "alexw1@illinois.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Ravishankar Iyer", "Randal Butler", "Zbigniew Kalbarczyk", "Adam Slagell"], "LastAmendmentDate": "09/08/2015", "Organization": "University of Illinois at Urbana-Champaign", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "499136", "Abstract": "The cyber infrastructure that supports science research (such as the cyberinfrastructure that provides access to unique scientific instrumentation such as a telescope, or an array of highly distributed sensors placed in the field, or a computational supercomputing center) faces the daunting challenge of defending against cyber attacks. Modest to medium research project teams have little cyber security expertise to defend against the increasingly diverse, advanced and constantly evolving attacks. Even larger facilities that have with security expertise are often overwhelmed with the amount of security log data they need to analyze in order to identify attackers and attacks, which is the first step to defending against them. The challenges of the traditional approach of identifying an attacker are amplified by the lack of tools and time to detect attacks skillfully hidden in the noise of ongoing network traffic. The challenge is not necessarily in deploying additional monitoring but to identify this malicious traffic by utilizing all available information found in the plethora of security, network, and system logs that are already being actively collected. This project proposes to build and deploy, is needed in research environments, an advanced log analysis tool, named AttackTagger, that can scale to be able to address the dramatic increase in security log data, and detect emerging threat patterns in today&apos;s constantly evolving security landscape. AttackTagger will make science research in support of national priorities more secure.&lt;br/&gt;&lt;br/&gt;AttackTagger will be a sophisticated log analysis tool designed to find potentially malicious activity, such as credential theft, by building factor graph models for advanced pattern matching. AttackTagger will integrate with existing security software so as to be easily deployable within existing security ecosystems and to offload processing and computational work onto better suited components. It can consume a wide variety of system and network security logs. AttackTagger accomplishes advanced pattern matching by utilizing a Factor Graph model, which is a type of probabilistic graphical model that can describe complex dependencies among random variables using an undirected graph representation, specifically a bipartite graph. The bipartite graph representation consists of variable nodes representing random variables, factor nodes representing local functions (or factor functions , and edges connecting the two types of nodes. Variable dependencies in a factor graph are expressed using a global function, which is factored into a product of local functions. In the practice of the security domain, using factor graphs is more flexible to define relations among the events and the user state compared to Bayesian Network and Markov Random Field approaches. Specifically, using factor graphs allows capturing sequential relation among events and enables integration of the external knowledge, e.g., expert knowledge or a user profile.", "ProgramReferenceCode": ["7433", "8005", "8004"], "OrganizationZip": "618207473"}, {"OrganizationCity": "CHAMPAIGN", "OrganizationState": "IL", "Title": "SI2-SSE: yt: Reusable Components for Simulating, Analyzing and Visualizing Astrophysical Systems", "ProgramManager": "Rajiv Ramnath", "State": "IL", "Program": ["EXTRAGALACTIC ASTRON & COSMOLO", "OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes"], "ProgramElementCode": ["1217", "1253", "8004"], "StartDate": "11/01/2014", "OrganizationStreet": "SUITE A", "EndDate": "09/30/2016", "PrincipalInvestigator": "Matthew Turk", "ARRAAmount": null, "OrganizationPhone": "2173332187", "AwardNumber": "1535651", "PIEmailAddress": "mjturk@illinois.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "03/09/2015", "Organization": "University of Illinois at Urbana-Champaign", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "427003", "Abstract": "Computational modeling of astrophysical phenomena has grown in sophistication and realism, leading to a diversity of complex simulation platforms, each utilizing its own mechanism and format for representing particles and fluids. Similarly, most of the data analysis is conducted with tools developed in isolation and targeted to a specific simulation platform or research domain; very little systematic and direct technology transfer between astrophysical researchers exists. The yt project is a parallel analysis and visualization toolkit designed to support a collaborative community of researchers as they focus on answering physical questions, rather than the technical mechanics of reading, processing and visualizing data formats. This project will enable the development of advanced, physics-based modules that apply universally across simulation codes, advancing scientific inquiry and enabling more efficient utilization of computational and human resources. In doing so, it will help advance a myriad of research goals from the study of black hole binaries to the growth of cosmic structure. In addition, the project will serve as a touchstone for collaboration and cross-code utilization between many groups studying diverse phenomena. Moreover, the project will be developed through a community-oriented process, engaging a wide range of participants.&lt;br/&gt;&lt;br/&gt;The infrastructure development in this research will enable these capabilities by broadening the applicable simulation platforms within yt, enabling cross-code utilization of microphysical solvers and physics modules and in situ analysis, and developing collaborative platforms for the exploration of astrophysical datasets. In particular, it will develop the capabilities of yt in three primary mechanisms. The first is to enable support for additional, fundamentally different simulation platforms such as smoothed particle hydrodynamics, unstructured mesh, and non-Cartesian coordinate systems. The second is to provide simulation instrumentation components to ease the process of developing simulation codes, interfacing and exchanging technology between those simulation codes, and to enable deeper, on-the-fly integration of astrophysical simulation codes with yt and other analysis toolkits. The final focus is on developing interface components to enable collaborative and interactive exploration of data utilizing web-based platforms. An explicit goal of this SI2-SSE project is the development of collaborative relationships between scientists, furthering the development of the field as a whole. By conducting all business in the open with a focus on developing and encouraging collaborative, welcoming environments for contributors and researchers, this SSE will help to foster a level playing field that is more accessible to all parties, particularly women and underrepresented minorities. An explicit milestone of this project is to streamline the process of conducting direct outreach through scientific visualization, greatly expanding the domains and individuals engaged in STEM-based public outreach.", "ProgramReferenceCode": ["1206", "7433", "8005", "8004"], "OrganizationZip": "618207473"}, {"OrganizationCity": "CHAMPAIGN", "OrganizationState": "IL", "Title": "SI2-SSI: Collaborative Research: Scalable, Extensible, and Open Framework for Ground and Excited State Properties of Complex Systems", "ProgramManager": "Rajiv Ramnath", "State": "IL", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "DMR SHORT TERM SUPPORT", "SPECIAL PROJECTS - CCF", "CYBERINFRASTRUCTURE", "Software Institutes", "CDS&E"], "ProgramElementCode": ["1253", "1712", "2878", "7231", "8004", "8084"], "StartDate": "10/01/2013", "OrganizationStreet": "SUITE A", "EndDate": "09/30/2018", "PrincipalInvestigator": "Laxmikant Kale", "ARRAAmount": null, "OrganizationPhone": "2173332187", "AwardNumber": "1339715", "PIEmailAddress": "kale@uiuc.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "09/10/2014", "Organization": "University of Illinois at Urbana-Champaign", "AwardInstrument": "Continuing grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "2383226", "Abstract": "Computer simulation plays a central role in helping us understand, predict, and engineer the physical and chemical properties of technological materials systems such as semiconductor devices, photovoltaic systems, chemical reactions and catalytic behavior. Despite significant progress in performing realistic simulations in full microscopic detail, some problems are currently out of reach: two examples are the modeling of electronic devices with multiple functional parts based on new materials such as novel low power computer switches that would revolutionize the Information Technology industry, and the photovoltaic activity of complex interfaces between polymers and inorganic nanostructures that would enhance US energy self-reliance. The research program of this collaborative software institute aims to create an open and effective scientific software package that can make efficient use of cutting-edge high performance computers (HPC) to solve challenging problems involving the physics and chemistry of materials. By having such software available, this software initiative will have multiple broad impacts. First, the community of materials scientists will be able to study next-generation problems in materials physics and chemistry, and computer science advances that enable the software will be demonstrated and made accessible for both communities which will help cross-fertilize further such collaborative efforts. Second, the capability of simulating and engineering more complex materials systems and technological devices could play a role in helping the US continue is competitive edge in science, technology, and education. Third, through training of young scientists, direct outreach to the broader scientific community through workshops and conferences, and educational programs ranging from secondary to graduate levels, the power, importance, and capabilities of computational modeling, materials science, and computer science methodologies that enable the science will be communicated to a broad audience. Finally, by enabling the refinement of existing materials systems as well as discovery of new materials systems, the resulting scientific advances can help broadly impact society via technological improvements: in terms of the two examples provided above, (a) the successful design of new electronic device paradigms helps significantly advance the digital revolution by permitting the introduction of smaller, more efficient, and more capable electronic circuits and information processing systems, and (b) successful creation of inexpensive, easy-to-fabricate, and durable photovoltaic materials and devices can lead to cleaner forms of energy production while reducing reliance on fossil fuels.&lt;br/&gt;&lt;br/&gt;The technical goal is to greatly enhance the open software tool OPENATOM to advance discovery in nanoscience and technology. OPENATOM will be delivered as a open, robust and validated software package capable of utilizing HPC architectures efficiently to describe the electronic structure of complex materials systems from first principles. In terms of describing electronic ground-states, OPENATOM will be enhanced by features such as improved configurational sampling methods, hybrid density functionals, and incorporation of fast super-soft pseudopotential techniques. In addition, the team will incorporate the many-body GW-BSE approach for electronic excitations that permits accurate computation of electronic energy levels, optical absorption and emission, and luminescence. Ultimately, such an extensible software framework will permit accurate electronic structure computations to employ effectively future HPC platforms with 10,000,000 cores.", "ProgramReferenceCode": ["7433", "7569", "8009", "8084", "9216", "9263"], "OrganizationZip": "618207473"}, {"OrganizationCity": "CHAMPAIGN", "OrganizationState": "IL", "Title": "SI2-SSI: CyberGIS Software Integration for Sustained Geospatial Innovation", "ProgramManager": "Cheryl L. Eavey", "State": "IL", "Program": ["METHOD, MEASURE & STATS", "GEOGRAPHY AND SPATIAL SCIENCES", "CROSS-DIRECTORATE ACTIV PROGR", "Software Institutes", "Sustainable Energy Pathways"], "ProgramElementCode": ["1333", "1352", "1397", "8004", "8026"], "StartDate": "10/01/2010", "OrganizationStreet": "SUITE A", "EndDate": "09/30/2016", "PrincipalInvestigator": "Shaowen Wang", "ARRAAmount": null, "OrganizationPhone": "2173332187", "AwardNumber": "1047916", "PIEmailAddress": "shaowen@illinois.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Timothy Nyerges", "Nancy Wilkins-Diehr", "Luc Anselin", "Budhendra Bhaduri"], "LastAmendmentDate": "07/31/2015", "Organization": "University of Illinois at Urbana-Champaign", "AwardInstrument": "Continuing grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "4804821", "Abstract": "Originally developed by geographers in the mid-1960s, Geographic Information Systems (GIS) have flourished since that time. In the foreseeable future, GIS software will continue to play essential roles for breaking through scientific challenges in numerous fields and improving decision-making practices with broad societal impacts. However, fulfilling such roles is increasingly dependent on the ability to handle very large spatiotemporal data sets and complex analysis software based on synthesizing computational and spatial thinking enabled by cyberinfrastructure, which conventional GIS-based software approaches do not provide. This project will establish CyberGIS as a fundamentally new software framework comprising a seamless integration of cyberinfrastructure, GIS, and spatial analysis/modeling capabilities. Specifically, the project will: 1) engage a multidisciplinary community through a participatory approach in evolving CyberGIS software requirements; 2) integrate and sustain a core set of composable, interoperable, manageable, and reusable CyberGIS software elements based on community-driven and open source strategies; 3) empower high-performance and scalable CyberGIS by exploiting spatial characteristics of data and analytical operations for achieving unprecedented capabilities for geospatial knowledge discovery; 4) enhance an online geospatial problem solving environment to allow for the contribution, sharing, and learning of CyberGIS software by numerous users, which fosters the development of education, outreach, and training programs crosscutting multiple disciplines; 5) deploy and test CyberGIS software by linking with national and international cyberinfrastructure to achieve scalability to significant sizes of geospatial problems, cyberinfrastructure resources, and user communities; and 6) evaluate and improve the CyberGIS framework through domain science applications and vibrant partnerships to gain better understanding of the complexity of coupled human-natural systems.&lt;br/&gt;&lt;br/&gt;The CyberGIS software framework will shift the current paradigm of GIS and associated spatial analysis/modeling software to create scalable and sustainable software ecosystems while achieving groundbreaking scientific advances in understanding coupled human-natural systems that would be impossible otherwise. These advances will, for example, dramatically advance the understanding of disaster preparedness and response and impacts of global climate change. This framework will empower high-performance and collaborative geospatial problem solving and serve as a key driver for the interoperability of international cyberinfrastructure based on broad engagement of user communities related to GIS for both research and education purposes. The project will establish an industrial partnership with the Environmental Systems Research Institute (ESRI), collaborations with the Department of Energy&apos;s Oak Ridge National Laboratory (ORNL) and the U.S. Geological Survey (USGS) National Map Project, and international partnerships with several institutions in Australia, China, and the United Kingdom to effectively extend the benefits to the nation and society in significant ways.", "ProgramReferenceCode": ["004Z", "1333", "1352", "7433", "7556", "7969", "8004", "8009", "1397"], "OrganizationZip": "618207473"}, {"OrganizationCity": "Fort Collins", "OrganizationState": "CO", "Title": "SI2-SSE: Developing Sustainable Software Elements to Support the Growing Field of Public Participation in Scientific Research", "ProgramManager": "Daniel Katz", "State": "CO", "Program": ["CROSS-DIRECTORATE ACTIV PROGR", "AISL", "Software Institutes"], "ProgramElementCode": ["1397", "7259", "8004"], "StartDate": "11/01/2013", "OrganizationStreet": "601 S Howes St", "EndDate": "10/31/2016", "PrincipalInvestigator": "Gregory Newman", "ARRAAmount": null, "OrganizationPhone": "9704916355", "AwardNumber": "1339707", "PIEmailAddress": "Gregory.Newman@ColoState.Edu", "NSFDirectorate": "CSE", "Co-PIName": ["Melinda Laituri", "Stacy Lynn"], "LastAmendmentDate": "08/08/2013", "Organization": "Colorado State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "493076", "Abstract": "Across the globe, citizen science projects are becoming increasingly poised to address social and environmental challenges and answer broad scientific questions. Although rapidly increasing in number, these projects need easy-to-use software tools for data management, analysis, and visualization to be successful. This project transforms how citizen science projects unfold locally, regionally, and globally by creating software that supports the full spectrum of project activities. It empowers projects to ask and answer their own local questions while contributing data critical to larger-scale issues. These tools will allow projects to announce training events; track volunteers; create datasheets; enter, review, analyze, and visualize data; publish reports; discover resources; integrate data; and ensure that data are contributed to repositories (e.g., DataONE, NEON, GBIF, HydroShare, and EOL). Tools will be made available to citizen science projects and will be delivered as reusable software elements for use in existing websites; as website features on CitSci.org; and as Application Programming Interface (API) services and mobile applications.&lt;br/&gt;&lt;br/&gt;The tools will expand the national reach, local appeal, computational abilities, visualization techniques, statistical analysis capabilities, and interoperability of the nations? cyber-infrastructure. Using participatory design and agile methods, the project will: (1) develop reusable software elements that citizen science organizations can embed into their own websites, (2) harden and expand the functionality and capabilities of CitSci.org through new website features, and (3) extend the APIs of CitSci.org and develop associated mobile applications to increase system and tool interoperability. The target user communities will include citizen science project coordinators. It will deliver customizable tools and services related to all project activities and engage projects across a wide array of disciplines. Project coordinators will be able to customize all tools developed to suit their specific project needs. Adoption and use of the tools developed will create a cyber-ready workforce capable of collecting, contributing, and applying high quality ecological, geophysical, social, and human health related observations to solve real-world problems. These broader impacts will help the citizen science community better understand effective models of public engagement to ensure more impactful application of citizen science to societal challenges.", "ProgramReferenceCode": ["7433", "7477", "8009"], "OrganizationZip": "805232002"}, {"OrganizationCity": "Eugene", "OrganizationState": "OR", "Title": "SI2-SSI: Collaborative Research: A Glass Box Approach to Enabling Open, Deep Interactions in the HPC Toolchain", "ProgramManager": "Daniel Katz", "State": "OR", "Program": ["INFORMATION TECHNOLOGY RESEARC", "Software Institutes"], "ProgramElementCode": ["1640", "8004"], "StartDate": "06/01/2012", "OrganizationStreet": "5219 UNIVERSITY OF OREGON", "EndDate": "05/31/2016", "PrincipalInvestigator": "Allen Malony", "ARRAAmount": null, "OrganizationPhone": "5413465131", "AwardNumber": "1148346", "PIEmailAddress": "malony@cs.uoregon.edu", "NSFDirectorate": "CSE", "Co-PIName": "Sameer Shende", "LastAmendmentDate": "05/29/2012", "Organization": "University of Oregon Eugene", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "926667", "Abstract": "Parallel computing has entered the mainstream with increasingly large multicore processors and powerful accelerator devices. These compute engines, coupled with tighter integration of faster interconnection fabrics, are drivers for the next-generation high end computing (HEC) machines. However, the computing potential of HEC machines is delivered only through productive parallel program development and efficient parallel execution. This project enables application developers to improve performance on future HEC machines for their scientific and engineering processes. This project challenges the current model for parallel application development via &quot;black box&quot; tools and services. Instead, the project offers an open, transparent software infrastructure -- a Glass Box system -- for creating and tuning large-scale, parallel applications. `Opening up&apos; the tools and services used to create and evaluate peta- and exa-scale codes involves developing interfaces and methods that make tool-internal information and available for new performance management services that improve developer productivity and code efficiency.&lt;br/&gt;&lt;br/&gt;The project will explore the information that can be shared &apos;across the software stack&apos;. Methods will be developed for analyzing program information, performance data and tool knowledge. The resulting Glass Box system will allow developers to better assess the performance of their parallel codes. Tool creators can use the performance data to create new analysis and optimization techniques. System developers can also better manage multicore and machine resources at runtime, using JIT compilation and binary code editing to exploit the evolving hardware. Working with the `Keeneland&apos; NSF Track II machine and our industry partners, the project will create new performance monitoring tools, compiler methods and system-level resource management techniques. The effort is driven by the large-scale codes running on today&apos;s petascale machines. Its broader impact is derived from the interactions with technology developers and application scientists as well as from its base in three universities with diverse student populations.", "ProgramReferenceCode": ["8009", "7433", "8004"], "OrganizationZip": "974035219"}, {"OrganizationCity": "Eugene", "OrganizationState": "OR", "Title": "SI2-SSI: Collaborative Research: A Software Infrastructure for MPI Performance Engineering: Integrating MVAPICH and TAU via the MPI Tools Interface", "ProgramManager": "Daniel Katz", "State": "OR", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "09/01/2015", "OrganizationStreet": "5219 UNIVERSITY OF OREGON", "EndDate": "08/31/2019", "PrincipalInvestigator": "Sameer Shende", "ARRAAmount": null, "OrganizationPhone": "5413465131", "AwardNumber": "1450471", "PIEmailAddress": "sameer@cs.uoregon.edu", "NSFDirectorate": "CSE", "Co-PIName": "Allen Malony", "LastAmendmentDate": "08/31/2015", "Organization": "University of Oregon Eugene", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1200000", "Abstract": "Message-Passing Interface (MPI) continues to dominate the supercomputing landscape, being the primary parallel programming model of choice. A large variety of scientific applications in use today are based on MPI. On the current and next-generation High-End Computing (HEC) systems, it is essential to understand the interaction between time-critical applications and the underlying MPI implementations in order to better optimize them for both scalability and performance. Current users of HEC systems develop their applications with high-performance MPI implementations, but analyze and fine tune the behavior using standalone performance tools. Essentially, each software component views the other as a blackbox, with little sharing of information or access to capabilities that might be useful in optimization strategies. Lack of a standardized interface that allows interaction between the profiling tool and the MPI library has been a big impediment. The newly introduced MPI_T interface in the MPI-3 standard provides a simple mechanism that allows MPI implementers to expose variables representing configuration parameters or performance measurements from within the implementation for the benefit of tools, tuning frameworks, and other support libraries. However, few performance analysis and tuning tools take advantage of the MPI_T interface and none do so to dynamically optimize at execution time. This research and development effort aims to build a software infrastructure for MPI performance engineering using the new MPI_T interface.&lt;br/&gt;&lt;br/&gt;With the adoption of MPI_T in the MPI standard, it is now possible to take positive steps to realize close interaction between and integration of MPI libraries and performance tools. This research, undertaken by a team of computer scientists from OSU and UO representing the open source MVAPICH and TAU projects, aims to create an open source integrated software infrastructure built on the MPI_T interface which defines the API for interaction and information interchange to enable fine grained performance optimizations for HPC applications. The challenges addressed by the project include: 1) enhancing existing support for MPI_T in MVAPICH to expose a richer set of performance and control variables; 2) redesigning TAU to take advantage of the new MPI_T variables exposed by MVAPICH; 3) extending and enhancing TAU and MVAPICH with the ability to generate recommendations and performance engineering reports; 4) proposing fundamental design changes to make MPI libraries like MVAPICH ``reconfigurable&apos;&apos; at runtime; and 5) adding support to MVAPICH and TAU for interactive performance engineering sessions. The framework will be validated on a variety of HPC benchmarks and applications. The integrated middleware and tools will be made publicly available to the community. The research will have a significant impact on enabling optimizations of HPC applications that have previously been difficult to provide. As a result, it will contribute to deriving &quot;best practice&quot; guidelines for running on next-generation Multi-Petaflop and Exascale systems. The research directions and their solutions will be used in the curriculum of the PIs to train undergraduate and graduate students.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "974035219"}, {"OrganizationCity": "Houston", "OrganizationState": "TX", "Title": "SI2-SSI: Collaborative Research: A Glass Box Approach to Enabling Open, Deep Interactions in the HPC Toolchain", "ProgramManager": "Daniel Katz", "State": "TX", "Program": ["INFORMATION TECHNOLOGY RESEARC", "Software Institutes"], "ProgramElementCode": ["1640", "8004"], "StartDate": "06/01/2012", "OrganizationStreet": "4800 Calhoun Boulevard", "EndDate": "05/31/2016", "PrincipalInvestigator": "Barbara Chapman", "ARRAAmount": null, "OrganizationPhone": "7137435773", "AwardNumber": "1148052", "PIEmailAddress": "barbara.chapman@stonybrook.edu", "NSFDirectorate": "CSE", "Co-PIName": "Edgar Gabriel", "LastAmendmentDate": "05/29/2012", "Organization": "University of Houston", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "926666", "Abstract": "Parallel computing has entered the mainstream with increasingly large multicore processors and powerful accelerator devices. These compute engines, coupled with tighter integration of faster interconnection fabrics, are drivers for the next-generation high end computing (HEC) machines. However, the computing potential of HEC machines is delivered only through productive parallel program development and efficient parallel execution. This project enables application developers to improve performance on future HEC machines for their scientific and engineering processes. This project challenges the current model for parallel application development via &quot;black box&quot; tools and services. Instead, the project offers an open, transparent software infrastructure -- a Glass Box system -- for creating and tuning large-scale, parallel applications. `Opening up&apos; the tools and services used to create and evaluate peta- and exa-scale codes involves developing interfaces and methods that make tool-internal information and available for new performance management services that improve developer productivity and code efficiency.&lt;br/&gt;&lt;br/&gt;The project will explore the information that can be shared &apos;across the software stack&apos;. Methods will be developed for analyzing program information, performance data and tool knowledge. The resulting Glass Box system will allow developers to better assess the performance of their parallel codes. Tool creators can use the performance data to create new analysis and optimization techniques. System developers can also better manage multicore and machine resources at runtime, using JIT compilation and binary code editing to exploit the evolving hardware. Working with the `Keeneland&apos; NSF Track II machine and our industry partners, the project will create new performance monitoring tools, compiler methods and system-level resource management techniques. The effort is driven by the large-scale codes running on today&apos;s petascale machines. Its broader impact is derived from the interactions with technology developers and application scientists as well as from its base in three universities with diverse student populations.", "ProgramReferenceCode": ["7433", "8009", "8004"], "OrganizationZip": "772042015"}, {"OrganizationCity": "Boulder", "OrganizationState": "CO", "Title": "Collaborative Research: SI2-SSI: Landlab: A Flexible, Open-Source Modeling Framework for Earth-Surface Dynamics", "ProgramManager": "Daniel Katz", "State": "CO", "Program": ["GEOMORPHOLOGY & LAND USE DYNAM", "Software Institutes", "EarthCube"], "ProgramElementCode": ["7458", "8004", "8074"], "StartDate": "08/01/2015", "OrganizationStreet": "3100 Marine Street, Room 481", "EndDate": "07/31/2020", "PrincipalInvestigator": "Gregory Tucker", "ARRAAmount": null, "OrganizationPhone": "3034926221", "AwardNumber": "1450409", "PIEmailAddress": "gtucker@cires.colorado.edu", "NSFDirectorate": "CSE", "Co-PIName": "Daniel Hobley", "LastAmendmentDate": "07/14/2015", "Organization": "University of Colorado at Boulder", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "789777", "Abstract": "Earth scientists discover and predict the behavior of the natural world in part through the use of computer models. Models are used to study a wide range of phenomena, such as soil erosion, flooding, plant growth, landslide occurrence, and many other processes. Models can be used to develop scientific understanding by comparing model calculations with the real world. They can also be used to make predictions about how nature will behave under certain conditions. In the areas of geosciences that deal with the earth&apos;s surface, one bottleneck in the development and use of models is the effort required to build, test, and debug the necessary software. This project contributes to scientific research and discovery by creating open source software tools that scientists can use to more efficiently create, modify, or combine computer models that represent portions of earth&apos;s surface and the processes occurring thereon. The project combines software development with user training and web-based resources, so that the products will be openly accessible, well documented, and widely disseminated within the relevant scientific communities. The project also contributes to K-12, undergraduate, and graduate-level education in computational modeling and earth-surface processes.&lt;br/&gt;&lt;br/&gt;This project catalyzes research in earth-surface dynamics by developing a software framework that enables rapid creation, refinement, and reuse of two-dimensional (2D) numerical models. The phrase earth-surface dynamics refers to a remarkably diverse group of science and engineering fields that deal with our planet&apos;s surface and near-surface environment: its processes, its management, and its responses to natural and human-made perturbations. Scientists who want to use an earth-surface model often build their own unique model from the ground up, re-coding the basic building blocks of their model rather than taking advantage of codes that have already been written. Whereas the end result may be novel software programs, many person-hours are lost rewriting existing code, and the resulting software is often idiosyncratic, poorly documented, and unable to interact with other software programs in the same scientific community and beyond, leading to lost opportunities for exploring an even wider array of scientific questions than those that can be addressed using a single model. The Landlab model framework seeks to eliminate these redundancies and lost opportunities, and simultaneously lower the bar for entry into numerical modeling, by creating a user- and developer-friendly software library that provides scientists with the fundamental building blocks needed for modeling earth-surface dynamics. The framework takes advantage of the fact that nearly all surface-dynamics models share a set of common software elements, despite the wide range of processes and scales that they encompass. Providing these elements in the context of a popular scientific programming environment, with strong user support and community engagement, contributes to accelerating progress in the diverse sciences of the earth&apos;s surface.&lt;br/&gt;&lt;br/&gt;The Landlab modeling framework is designed so that grid creation, data storage, and sharing of data among process components is done for the user. The framework is generic enough so that the coupling of two process components, whether they are squarely within a geoscience subdiscipline, or they cross subdisciplines, is the same. This architecture makes it easy to explore a wide range of questions in the geosciences without the need for much coding. Further, the model code is primarily written in Python, a language that is relatively easy for casual programmers to learn. Because of these attributes, the Landlab modeling framework has the potential to add computational modeling to the toolbox of a wide array of geoscientists, and to clear a path for trans-disciplinary earth-surface modeling that explores societally relevant topics, such as land-cover changes in response to climate change, as well as modeling of topics that currently require the coupling of sophisticated but harder-to-use models, such as sediment source-to-sink dynamics. The project will generate several proof-of-concept studies that are interesting in their own right, and demonstrate the capabilities of the modeling framework. Community engagement is fostered by presenting clinics and demonstrations at professional venues aimed at hydrologists, sedimentologists, critical zone scientists, and the broader earth and environmental sciences community. The team also supports visits by individual scientists for on-site training beyond these clinics. Input/output tools allow compatibility with data from relevant NSF-supported research. The project supports four graduate students and three postdoctoral researchers, and also includes modeling workshops for fifth to seventh grade girls in a community that has a greater than 50% minority population.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "803031058"}, {"OrganizationCity": "PASADENA", "OrganizationState": "CA", "Title": "Collaborative: SI2-CHE: Development and Deployment of Chemical Software for Advanced Potential Energy Surfaces", "ProgramManager": "Evelyn M. Goldfield", "State": "CA", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes"], "ProgramElementCode": ["1253", "8004"], "StartDate": "07/01/2014", "OrganizationStreet": "1200 E California Blvd", "EndDate": "04/30/2016", "PrincipalInvestigator": "Paul Nerenberg", "ARRAAmount": null, "OrganizationPhone": "6263956219", "AwardNumber": "1453123", "PIEmailAddress": "psn@caltech.edu", "NSFDirectorate": "MPS", "LastAmendmentDate": "09/03/2014", "Organization": "California Institute of Technology", "AwardInstrument": "Standard Grant", "NsfOrganization": "CHE", "AwardedAmountToDate": "38417", "Abstract": "An international team consisting of Teresa Head-Gordon and Martin Head-Gordon (University of California, Berkeley), Paul Nerenberg (Claremont McKenna College), David Case (Rutgers University), Jay Ponder (Washington University), Mark Tuckerman (New York University) with their UK collaborators: Lorna Smith and Neil Chue Hong (University of Edinburgh), Chris-Kriton Skylaris and Jonathan W. Essex (University of Southampton), Ilian Todorov (Daresbury Laboratory), Mario Antonioletti (EPCC) are are supported through the SI2-CHE program to develop and deploy robust and sustainable software for advanced potential energy surfaces. The greater accuracy introduced by improvements in the new generation of potential energy surfaces opens up several challenges in their manifestation as algorithms and software on current or emergent hardware platforms that in turn limits their wide adoption by the computational chemistry community. The research team is overcoming these obstacles via multiple but integrated directions: (1) to optimally implement advanced potential energy surfaces across multi-core and GPU enabled systems, (2) to develop a hierarchy of advanced polarizable models that alter the tradeoff between accuracy and computational speed,(3) to create new multiple time stepping methods; (4) to write a Quantum Mechanics/Molecular Mechanics (QM/MM ) application programing interface (API) that fully supports mutual polarization, (5) to adopt software best practices to ensure growth of a self-sustaining community and (6) to provide exemplar calculations with the new software in the several emerging application areas.&lt;br/&gt;&lt;br/&gt;Molecular simulation and quantum chemistry software is an integral part of chemistry and chemical biology, and has been broadly adopted by academic researchers and industry scientists. Next generation scientific breakthroughs that utilize chemical software will be enabled by the deployment of state of the art theoretical models and algorithms that are translated into a sustainable software framework rapidly implemented on emergent high performance computing platforms. Potential energy surfaces describe the interactions between atoms. Advanced and highly accurate potential energy surfaces encounter software-related obstacles that inhibit their application to grand challenge chemistry problems. This UK and US consortium, representing a broad cross section of the computational chemistry software community, is working to directly tackle these obstacles. This US and UK collaboration between universities and High Performance Computing centers works to endure that chemical software investments made in advanced potential energy surface models has a long term payoff in community sustainability and the training of the next generation of scientists. Outreach and training workshops are organized around the emergence of the advanced potential energy software including an introductory molecular simulation software boot camp for undergraduate students.&lt;br/&gt;&lt;br/&gt;The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC.", "ProgramReferenceCode": ["5918", "5946", "5950", "7433", "8009"], "OrganizationZip": "911250600"}, {"OrganizationCity": "Lubbock", "OrganizationState": "TX", "Title": "Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities", "ProgramManager": "Daniel Katz", "State": "TX", "Program": ["PHYSICAL & DYNAMIC METEOROLOGY", "Software Institutes", "EarthCube"], "ProgramElementCode": ["1525", "8004", "8074"], "StartDate": "08/01/2015", "OrganizationStreet": "349 Administration Bldg", "EndDate": "07/31/2018", "PrincipalInvestigator": "Brian Ancell", "ARRAAmount": null, "OrganizationPhone": "8067423884", "AwardNumber": "1450177", "PIEmailAddress": "brian.ancell@ttu.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/04/2015", "Organization": "Texas Tech University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "166428", "Abstract": "Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.&lt;br/&gt;&lt;br/&gt;The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of &quot;nuclei,&quot; which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities.", "ProgramReferenceCode": ["4444", "7433", "8009"], "OrganizationZip": "794091035"}, {"OrganizationCity": "PASADENA", "OrganizationState": "CA", "Title": "SI2-SSE: The Next Generation of the Montage Mosaic Engine", "ProgramManager": "Rajiv Ramnath", "State": "CA", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", null, "Software Institutes"], "ProgramElementCode": ["1253", "1798", "8004"], "StartDate": "10/01/2014", "OrganizationStreet": "1200 E California Blvd", "EndDate": "09/30/2016", "PrincipalInvestigator": "Graham Berriman", "ARRAAmount": null, "OrganizationPhone": "6263956219", "AwardNumber": "1440620", "PIEmailAddress": "gbb@ipac.caltech.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/14/2014", "Organization": "California Institute of Technology", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "499902", "Abstract": "Images produced by the new generation of astronomical instruments are addressing fundamental questions about the Universe, such as the formation of the very first galaxies after the Big Bang, and the very first stages of the formation of stars in massive dust clouds in our Galaxy. Exploiting this new generation of data is difficult because the data sets they produce are sufficiently complex and large as to demand new approaches to data processing that lag far behind developments in instrumentation. A growing community is working to rectify this state-of-affairs. This project will deliver software tools that will aggregate data from the new instruments into images of large scale regions of the sky so that astronomers can fully study scientific questions such as those identified above. This approach of studying aggregated images, or mosaics, is a powerful tool in astronomy. The project will deliver the next generation of an existing mosaic-building engine, Montage, which is in wide use in astronomy and in educational activities. It will support processing of the new data sets such that they can be visualized in immersive tools such as the World Wide Telescope, widely used in developing innovative approaches to education, and such that that they can generate data used by Citizen Science services such as Zooniverse. Montage will come be bundled with a set of tools that will enable astronomers to process massive collections images on powerful &quot;cloud computing&quot; platforms. These tools will be applicable to many data-intensive problems in fields such as earthquake prediction, DNA sequencing, and climate modeling. Finally, Montage is in wide use in developing and testing national cyberinfrastructure to benefit the U.S. science community. We anticipate that the next-generation Montage will be used in the same way to develop ever more powerful cyberinfrastructure as data volumes grow rapidly in all fields.&lt;br/&gt;&lt;br/&gt;In greater detail, the project will deliver the next generation of the Montage image mosaic engine, which will offer new capabilities that respond to the changing astronomy data and computing landscapes. These capabilities, requested by the user community, are: 1. Support for mosaicking of data cubes, now routinely generated by modern instrumentation; 2. Support for two widely used sky-partitioning schemes, HEALPix and TOAST; 3. An API to enable users to call Montage directly in Python and other languages. The work to develop memory management and subsetting techniques to support mosaicking will be available for others to use and extend. Support for HEALPix will enable integration and analysis of far-infrared, cosmic background data sets with other image data sets. TOAST will enable essentially any image data set to be incorporated into the WWT. Montage will be bundled with a turnkey package of open source tools that provision resources and run applications on cloud platforms. This package will build on knowledge gained in creating data products at scale with cloud platforms. These tools will bring cloud computing to scientists who have little system configuration knowledge, one of the biggest barriers to entry; these tools are general purpose and will be applicable to data intensive applications in may fields. Thus Montage will provide powerful new capabilities to astronomers, to projects analyzing data at scale to create new data products, and to scientists in data-intensive fields outside astronomy. The next-generation toolkit will inherit the sustainable Montage architecture, which has attracted a large user base among astronomers, E/PO specialists, and computer technologists. Montage is written in C, is portable across all common Unix platforms, highly scalable and delivered as components that are easy to incorporate into pipelines and processing environments. Montage is the only mosaic engine with all these characteristics. The project will use the evolutionary delivery lifecycle model. The code will be a made accessible on the GitHub repository, and released as Open Source code with a BSD 3-clause license. A Users&apos; Panel will advise on detailed specifications.", "ProgramReferenceCode": ["1206", "7433", "8005"], "OrganizationZip": "911250600"}, {"OrganizationCity": "WASHINGTON", "OrganizationState": "DC", "Title": "SI2-SSI: REAL-TIME LARGE-SCALE PARALLEL INTELLIGENT CO2 DATA ASSIMILATION SYSTEM", "ProgramManager": "Daniel Katz", "State": "DC", "Program": ["ADVANCES IN BIO INFORMATICS", "INTERFAC PROCESSES & THERMODYN", "ICER", "Software Institutes"], "ProgramElementCode": ["1165", "1414", "7699", "8004"], "StartDate": "09/01/2012", "OrganizationStreet": "1530 P ST NW", "EndDate": "08/31/2016", "PrincipalInvestigator": "Anna Michalak", "ARRAAmount": null, "OrganizationPhone": "2023876400", "AwardNumber": "1342076", "PIEmailAddress": "michalak@stanford.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "03/06/2015", "Organization": "Carnegie Institution of Washington", "AwardInstrument": "Continuing grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1667410", "Abstract": "Intellectual Merit: This proposal seeks to address this need by creating a state-of-the-art autonomous software platform for real-time integration of in-situ and satellite-based atmospheric CO2 measurements within a Data Assimilation (DA) system for producing estimates of global land and oceanic CO2 exchange at weekly to bi-weekly intervals. The proposed software infrastructure will be capable of autonomous processing of large volumes of data through a multi-stage pipeline, without the delays conventionally associated with such processing. Within the DA component, we will provide options for multiple DA algorithms for estimating global CO2 exchange. Users will, for the first time, have the capability to use these multiple methods as part of a single system for comparing estimates of CO2 exchange, and to obtain an improved understanding of the relative advantages of the various DA methods. As part of the analysis component of the software, we will build a carbon-climate surveillance system by drawing from a range of techniques in pattern recognition and high-dimensional statistical inference. This system will be able to detect and analyze localized variations in CO2 exchange within any user-specified spatio-temporal window. In addition, summaries of the CO2 exchange will be provided at annual and monthly temporal scales for continents and countries.&lt;br/&gt;&lt;br/&gt;Broader Impacts: This software can be used by researchers and governmental institutions for evaluating both the natural components of the carbon cycle and anthropogenic carbon emissions, as well as in the design of new satellites for improved monitoring of CO2. All data and software will be publicly available and open-source development platforms will be used whenever possible. The algorithm prototypes developed as part of this project will be used in undergraduate and graduate courses at the University of Michigan, and will be made available online for educators at other institutions. Finally, the project will train three graduate students, with a focus on developing their cross-disciplinary skills in the field of Earth science, statistics, computer science, and atmospheric science.", "ProgramReferenceCode": null, "OrganizationZip": "200051910"}, {"OrganizationCity": "CHAPEL HILL", "OrganizationState": "NC", "Title": "SI2-SSI: Collaborative Research: STORM: A Scalable Toolkit for an Open Community Supporting Near Realtime High Resolution Coastal Modeling", "ProgramManager": "Rajiv Ramnath", "State": "NC", "Program": ["PHYSICAL OCEANOGRAPHY", "SPECIAL PROJECTS - CISE", "SPECIAL PROJECTS - CCF", "Software Institutes", "EarthCube"], "ProgramElementCode": ["1610", "1714", "2878", "8004", "8074"], "StartDate": "10/01/2014", "OrganizationStreet": "104 AIRPORT DR STE 2200", "EndDate": "09/30/2018", "PrincipalInvestigator": "Richard Luettich", "ARRAAmount": null, "OrganizationPhone": "9199663411", "AwardNumber": "1339723", "PIEmailAddress": "rick_luettich@unc.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/26/2014", "Organization": "University of North Carolina at Chapel Hill", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "759047", "Abstract": "The ADCIRC coastal circulation and storm surge model has a long standing track record of extremely high societal impact. It has been used to define risk (e.g., 100-yr, 500-yr coastal flood levels) for the FEMA National Flood Insurance Program in coastal states from New York to Texas, it has been used to design the multi-billion dollar Hurricane and Storm Damage Risk Reduction System around greater New Orleans and southern Louisiana by the Army Corps of Engineers, it is currently run operationally by NOAA/NWS National Center for Environmental Prediction to forecast storm surge, to name just a few of its current and recent applications. Thus there is a well-established user network in place to convert improvements in ADCIRC into significant broader impacts. The proposed research provides transformative intellectual contributions that focus on applying new parallelization schemes to enable major advances in the algorithms, implementation and utilization of the ADCIRC model. The broadening of ADCIRC to a multi-algorithmic framework and the resulting performance gains that are anticipated will help ensure ADCIRC&apos;s sustainability as a core community model for at least the next 20 years. In addition, the proposed collaboration will impact computer science by serving as a high impact use case to inform the design of new approaches to efficient scalable computing. Together, the advancements in coastal modeling and parallelization technology will make a significant contribution to the science of modeling and HPC. The results of the proposed research will be disseminated to a wider community through ongoing educational outreach activities at the participating organizations as well as through refereed conference and journal papers, and invited presentations. The involvement of graduate students and post-doctoral fellows will be crucial towards the success of this project. The PIs have a long history of training and mentoring students and post-docs in computational science and engineering, coastal engineering and marine science. The recruitment and involvement of underrepresented groups in these efforts has always been a high priority. In addition, aspects of the proposed research will be incorporated into the curricula of several courses taught by the PIs in the areas of finite element methods, scientific computation, hydrology and oceanography.&lt;br/&gt;The aim of this project is to broaden the ADCIRC coastal circulation and storm surge model from a successful, but somewhat static coastal modeling tool that is tied to a single solution algorithm and the MPI parallelization paradigm, to a dynamic computational platform that is comprised of multiple solution algorithms, that readily admits new solution algorithms and that is built on a transformational new parallelization scheme that will allow us to scale to at least 256k compute cores on modern high performance computing (HPC) systems. We will do this by creating a living, evolving coastal modeling framework that will continue to lead the community in merging physical science / engineering and high performance computing and we will make the framework available to the broader community as a sustainable long term solution for its coastal modeling needs. In addition we will utilize these advancements in the highly demanding coastal storm surge forecasting system that we presently operate to demonstrate both improved robustness and speed of the model solution. We expect this effort will shorten the time required to provide reliable forecasting results and improve our ability to provide highly resolved, accurate, and physically complete predictions on an unprecedented scale. Concurrently, it should enable the use of smaller resources for simulations of increased scale which improves the usability and widens the applicability of ADCIRC in a broader community. The development of tightly integrated web-oriented products like CERA (www.coastalemergency.org) will enable the wide and timely dissemination of forecast modeling results to reach a broad audience.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "275991350"}, {"OrganizationCity": "Washington", "OrganizationState": "DC", "Title": "Collaborative: SI2-SSE - A Plug-and-Play Software Platform of Robotics-Inspired Algorithms for Modeling Biomolecular Structures and Motions", "ProgramManager": "Daniel Katz", "State": "DC", "Program": ["Software Institutes", "CDS&E", "SPECIAL PROJECTS - CCF"], "ProgramElementCode": ["8004", "8084", "2878"], "StartDate": "02/01/2015", "OrganizationStreet": "620 Michigan Ave.N.E.", "EndDate": "01/31/2018", "PrincipalInvestigator": "Erion Plaku", "ARRAAmount": null, "OrganizationPhone": "2026355000", "AwardNumber": "1440587", "PIEmailAddress": "plaku@cua.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/08/2014", "Organization": "Catholic University of America", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "215476", "Abstract": "This project aims to develop a novel plug-and-play platform of open-source software elements to advance algorithmic research in molecular biology. The focus is on addressing the algorithmic impasse faced by computational chemists and biophysicists in structure-function related problems involving dynamic biomolecules central to our biology. The software platform resulting from this project provides the critical software infrastructure to support transformative research in molecular biology and computer science that benefits society at large by advancing our modeling capabilities and in turn our understanding of the role of biomolecules in critical mechanisms in a living and diseased cell.&lt;br/&gt;&lt;br/&gt;The project addresses the current impasse on the length and time scales that can be afforded in biomolecular modeling and simulation. It does so by integrating cutting-edge knowledge from two different research communities, computational chemists and biophysicists focused on detailed physics-based simulations, and AI researchers focused on efficient search and optimization algorithms. The software elements integrate sophisticated energetic models and molecular representations with powerful search and optimization algorithms for complex modular systems inspired from robot motion planning. The plug-and-play feature of the software platform supports putting together novel algorithms, such as wrapping Molecular Dynamics or Monte Carlo as local search operators within larger robotics-inspired exploration frameworks, and adding emerging biomolecular representations, models, and search techniques even beyond the timeline of this project.", "ProgramReferenceCode": ["7433", "8005", "8004", "9216", "8084", "2878"], "OrganizationZip": "200640001"}, {"OrganizationCity": "West Lafayette", "OrganizationState": "IN", "Title": "Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science", "ProgramManager": "Daniel Katz", "State": "IN", "Program": ["Software Institutes", "OFFICE OF MULTIDISCIPLINARY AC"], "ProgramElementCode": ["8004", "1253"], "StartDate": "08/01/2015", "OrganizationStreet": "Young Hall", "EndDate": "07/31/2019", "PrincipalInvestigator": "Lyudmila Slipchenko", "ARRAAmount": null, "OrganizationPhone": "7654941055", "AwardNumber": "1450088", "PIEmailAddress": "lslipchenko@purdue.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "06/24/2015", "Organization": "Purdue University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "600000", "Abstract": "Computational chemistry has become ubiquitous throughout the fields of science and engineering. Among the many uses of computational chemistry codes are to aid in the design of new materials with specific properties, to understand protein folding dynamics related to the human body, to understand the mechanisms of enzyme catalysis to produce biofuels, and to identify and characterize gaseous species in the atmosphere. Because computational chemistry simulations can require large amounts of computer, memory and disk space, one focus of this research is to reduce the demands on computer resources by exploiting methods that can efficiently fragment large molecules into smaller pieces and at the same time take advantage of computers that have many thousands of computer cores. The principal investigators are developers of several computational chemistry programs, each of which has unique functionalities that have taken multiple person-years to develop and implement. In order to minimize further development efforts and to maximize the utility of the unique features, a second key focus of this research will be to develop the ability of each program to access the unique features and data of the other programs. A major bottleneck in the effort to construct high performance computers with more and more compute cores is the power that is required to drive such systems. One way that the research team will address the power issue is to explore the utility of low power architectures, such as graphical processing units for computational chemistry calculations. All of the newly developed codes will be made available to the user community by web download at no cost.&lt;br/&gt;&lt;br/&gt;This project presents an integrated computational science approach to very high quality electronic structure and dynamics calculations that will (a) be broadly accessible to both the development and applications communities, (b) have the capability to address problems of great interest, such as the properties of liquids and solvent effects, and photochemical/photobiological dynamics with high accuracy, (c) provide interoperability and sustainability into the foreseeable future, and (d) solve bottlenecks including power consumption using accelerators. A primary goal is to provide to the broad community new, accurate approaches that may be easily used, with a clear path forward to further code improvement and development. As part of the proposed effort, in addition to the usual outlets of journal articles and public presentations, the investigators will organize workshops at prominent national meetings, so that the expertise and software developed will be available to as broad a group of users as possible. All of the developed codes will be available on the web for easy downloads. The proposed research will develop new paradigms for interoperability, with a focus on the highly popular program suites GAMESS, NWChem, PSI4 and the AIMS dynamics code. Also included will be a cloud-based client-server model, a common quantum chemistry driver and novel data management approaches. The integration of the codes will be accomplished by making use of the combined expertise of the PIs in developing interoperable methods and data interfaces in computational science. In addition, several new methods will be developed, including novel explicit (R12) correlation methods that will be integrated with the most accurate levels of theory: multi-reference and the most accurate and novel coupled cluster methods, as well as the derivation and implementation of analytic derivatives. Applicability to large molecular systems will be made feasible by drawing upon novel fragmentation methods that scale nearly perfectly to the petascale.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "479072114"}, {"OrganizationCity": "Boulder", "OrganizationState": "CO", "Title": "Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities", "ProgramManager": "Daniel Katz", "State": "CO", "Program": ["Software Institutes", "PHYSICAL & DYNAMIC METEOROLOGY", "EarthCube"], "ProgramElementCode": ["8004", "1525", "8074"], "StartDate": "08/01/2015", "OrganizationStreet": "3090 Center Green Drive", "EndDate": "07/31/2018", "PrincipalInvestigator": "Mohan Ramamurthy", "ARRAAmount": null, "OrganizationPhone": "3034971000", "AwardNumber": "1450180", "PIEmailAddress": "mohan@ucar.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/04/2015", "Organization": "University Corporation For Atmospheric Res", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "98702", "Abstract": "Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.&lt;br/&gt;&lt;br/&gt;The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of &quot;nuclei,&quot; which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities.", "ProgramReferenceCode": ["7433", "8009", "4444"], "OrganizationZip": "803012252"}, {"OrganizationCity": "Atlanta", "OrganizationState": "GA", "Title": "SI2-SSI: Collaborative Research: A Glass Box Approach to Enabling Open, Deep Interactions in the HPC Toolchain", "ProgramManager": "Daniel Katz", "State": "GA", "Program": ["Software Institutes", "INFORMATION TECHNOLOGY RESEARC"], "ProgramElementCode": ["8004", "1640"], "StartDate": "06/01/2012", "OrganizationStreet": "Office of Sponsored Programs", "EndDate": "05/31/2016", "PrincipalInvestigator": ["Karsten Schwan", "Matthew Wolf"], "ARRAAmount": null, "OrganizationPhone": "4048944819", "AwardNumber": "1148310", "PIEmailAddress": ["schwan@cc.gatech.edu", "mwolf@cc.gatech.edu"], "NSFDirectorate": "CSE", "Co-PIName": ["Sudhakar Yalamanchili", "Greg Eisenhauer", "Matthew Wolf"], "LastAmendmentDate": "12/16/2015", "Organization": "Georgia Tech Research Corporation", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "926666", "Abstract": "Parallel computing has entered the mainstream with increasingly large multicore processors and powerful accelerator devices. These compute engines, coupled with tighter integration of faster interconnection fabrics, are drivers for the next-generation high end computing (HEC) machines. However, the computing potential of HEC machines is delivered only through productive parallel program development and efficient parallel execution. This project enables application developers to improve performance on future HEC machines for their scientific and engineering processes. This project challenges the current model for parallel application development via &quot;black box&quot; tools and services. Instead, the project offers an open, transparent software infrastructure -- a Glass Box system -- for creating and tuning large-scale, parallel applications. `Opening up&apos; the tools and services used to create and evaluate peta- and exa-scale codes involves developing interfaces and methods that make tool-internal information and available for new performance management services that improve developer productivity and code efficiency.&lt;br/&gt;&lt;br/&gt;The project will explore the information that can be shared &apos;across the software stack&apos;. Methods will be developed for analyzing program information, performance data and tool knowledge. The resulting Glass Box system will allow developers to better assess the performance of their parallel codes. Tool creators can use the performance data to create new analysis and optimization techniques. System developers can also better manage multicore and machine resources at runtime, using JIT compilation and binary code editing to exploit the evolving hardware. Working with the `Keeneland&apos; NSF Track II machine and our industry partners, the project will create new performance monitoring tools, compiler methods and system-level resource management techniques. The effort is driven by the large-scale codes running on today&apos;s petascale machines. Its broader impact is derived from the interactions with technology developers and application scientists as well as from its base in three universities with diverse student populations.", "ProgramReferenceCode": ["8009", "7433", "8004"], "OrganizationZip": "303320420"}, {"OrganizationCity": "La Jolla", "OrganizationState": "CA", "Title": "Collaborative Research: SI2-SSI: The Community-Driven Big-CZ Software System for Integration and Analysis of Bio- and Geoscience Data in the Critical Zone", "ProgramManager": "Daniel Katz", "State": "CA", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "12/01/2013", "OrganizationStreet": "Office of Contract & Grant Admin", "EndDate": "11/30/2016", "PrincipalInvestigator": "Ilya Zaslavsky", "ARRAAmount": null, "OrganizationPhone": "8585344896", "AwardNumber": "1339793", "PIEmailAddress": "zaslavsk@sdsc.edu", "NSFDirectorate": "CSE", "Co-PIName": "David Valentine", "LastAmendmentDate": "09/20/2013", "Organization": "University of California-San Diego", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "433911", "Abstract": "The Critical Zone (CZ) science community takes as its charge the effort to integrate theory, models and data from the multitude of disciplines collectively studying processes on the Earth&apos;s surface. The Critical Zone is Earth&apos;s permeable near-surface layer - from the atmosphere at the vegetation&apos;s canopy to the lower boundary of actively circulating groundwaters. The Critical Zone was a term coined by the National Research Council&apos;s Basic Research Opportunities in the Earth Sciences (BROES) Report (2001) to highlight the imperative for a new approach to thoroughly multi-disciplinary research on the zone of the Earth?s surface that is critical to sustaining terrestrial life on our planet. In January 2013, 103 members of the CZ community met for the CZ-EarthCube Domain Workshop (NSF Award #1252238) to prioritize the CZ community&apos;s key science drivers, key computational and information technology (&quot;cyber&quot;) challenges and key cyber needs. They identified that the central scientific challenge of the critical zone science community is to develop a &quot;grand unifying theory&quot; of the critical zone through a theory-model-data fusion approach. Work participants unanimously described that the key missing need of this approach was a future cyberinfrastructure for seamless 4D visual exploration of the integrated knowledge (data, model outputs and interpolations) from all the bio and geoscience disciplines relevant to critical zone structure and function, similar to today?s ability to easily explore historical satellite imagery and photographs of the earth&apos;s surface using Google Earth. This project takes the first &quot;BiG&quot; steps toward answering that need.&lt;br/&gt;&lt;br/&gt;The overall goal of this project is to co-develop with the CZ science and broader community, including natural resource managers and stakeholders, a web-based integration and visualization environment for joint analysis of cross-scale bio and geoscience processes in the critical zone (BiG CZ), spanning experimental and observational designs. Our Project Objectives are to: (1) Engage the CZ and broader community to co-develop and deploy the BiG CZ software stack; (2) Develop the BiG CZ Portal web application for intuitive, high-performance map-based discovery, visualization, access and publication of data by scientists, resource managers, educators and the general public; (3) Develop the BiG CZ Toolbox to enable cyber-savvy CZ scientists to access BiG CZ Application Programming Interfaces (APIs); and (4) Develop the BiG CZ Central software stack to bridge data systems developed for multiple critical zone domains into a single metadata catalog. The entire BiG CZ Software system will be developed on public repositories as a modular suite of fully open source software projects. It will be built around a new Observations Data Model Version 2.0 (ODM2) that is being developed by members of the BiG CZ project team, with community input, under separate funding (NSF Award #1224638).", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "920930934"}, {"OrganizationCity": "Buffalo", "OrganizationState": "NY", "Title": "SI2-SSI: Collaborative Research: Building Sustainable Tools and Collaboration for Volcanic and Related Hazards", "ProgramManager": "Daniel Katz", "State": "NY", "Program": ["PETROLOGY AND GEOCHEMISTRY", "DEEP EARTH PROCESSES SECTION", "Software Institutes", "Front in Earth Sys Dynamics", "EarthCube"], "ProgramElementCode": ["1573", "7571", "8004", "8016", "8074"], "StartDate": "10/01/2013", "OrganizationStreet": "402 Crofts Hall", "EndDate": "09/30/2018", "PrincipalInvestigator": "Abani Patra", "ARRAAmount": null, "OrganizationPhone": "7166452634", "AwardNumber": "1339765", "PIEmailAddress": "abani@buffalo.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Marcus Bursik", "Greg Valentine", "Tevfik Kosar", "Matthew Jones"], "LastAmendmentDate": "08/25/2015", "Organization": "SUNY at Buffalo", "AwardInstrument": "Continuing grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1300921", "Abstract": "This project is focused on creating and upgrading software infrastructure for a large community of scientists engaged in volcanology research and associated hazard analysis. Specifically, the project will reengineer three widely used tools (TITAN2D - block and ash flows, TEPHRA and Puff - ash transport and dispersal) and develop support for workflows that use these tools to analyze risk from volcanic hazards. Reengineering will encompass modularization so researchers may easily experiment with different modeling approaches, incorporation of techniques to make the tools efficient on new computing architectures like GPUs and many-core chips. The workflows are intended to tackle the challenges of managing complex and often large data flows associated with these tools in validation processes and in probabilistic inference based on the outcomes of the modeling. The tools and workflows will be made available using the popular vhub.org platform. This project will help provide a standard well managed hardware/software platform and approaches to standardize the documentation associated with input data, source code, and output data. This will ensure that model calculations are reproducible.&lt;br/&gt;&lt;br/&gt;The wider use of these high fidelity tools and their use in mitigating hazards is likely to have a significant effect on hazard analysis and management. The project will also engage in several major workshops and in training activities. Project personnel will also engage in the Earthcube initiative - popularizing computational methodologies, online access and dissemination mechanisms through the VHub platform.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "142607016"}, {"OrganizationCity": "Los Angeles", "OrganizationState": "CA", "Title": "SI2-SSI: Distributed Workflow Management Research and Software in Support of Science", "ProgramManager": "Daniel Katz", "State": "CA", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "04/01/2012", "OrganizationStreet": "University Park", "EndDate": "03/31/2017", "PrincipalInvestigator": "Ewa Deelman", "ARRAAmount": null, "OrganizationPhone": "2137407762", "AwardNumber": "1148515", "PIEmailAddress": "deelman@isi.edu", "NSFDirectorate": "CSE", "Co-PIName": "Miron Livny", "LastAmendmentDate": "08/30/2012", "Organization": "University of Southern California", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "2153597", "Abstract": "This award funds the enhancement of state-of-the-art workflow technologies and their promotion within a broad range of scientific domains. The overarching goal is to advance scientific discovery by providing scientists with tools that can manage computations on national cyberinfrastructure in a way that is reliable and scalable. &lt;br/&gt;&lt;br/&gt;The key technology supported by this award is the Pegasus Workflow Management System (Pegasus). This program of work includes the development, support, and maintenance of Pegasus. Pegasus allows users to declaratively describe their workflow, then makes a plan that maps this description onto the available execution resources and executes the plan. This approach is scalable, reliable, and supports applications running on campus resources, clouds, and national cyberinfrastructure. &lt;br/&gt;&lt;br/&gt;The work conducted under this award will 1) enhance the sustainability of the Pegasus software through the expanded adoption of sound software engineering practices and improved usability, 2) enhance core capabilities, especially in the area of data management, to meet user requirements and make Pegasus easier to integrate into end-to-end scientific environments, 3) promote the adoption of workflow management technologies within domain and computer sciences. &lt;br/&gt;&lt;br/&gt;Intellectual Merit: Pegasus WMS brings innovative and powerful frameworks to the desk of the scientist. Through close collaboration with a broad community of engaged users, experimentation in large-scale distributed computing is made possible. This experimentation supports the development of new scientific workflow management concepts, frameworks and technologies. The proposed work also supports scientific reproducibility by providing a workflow management system that integrates and automates data, metadata, and provenance management functions. &lt;br/&gt;&lt;br/&gt;Broader Impact: Pegasus WMS has been adopted by scientists from different domains and has been integrated into end-user environments such as workflow composition tools and portals. The program of outreach and education facilitated by this award will expand the impact of Pegasus through tutorials, workshops, meetings with potential users, and online materials. The proposed interface enhancements will allow more end-user environments to leverage Pegasus? capabilities and will extend the impact of Pegasus to a broader spectrum of users.", "ProgramReferenceCode": ["8009", "8004"], "OrganizationZip": "900890001"}, {"OrganizationCity": "Avondale", "OrganizationState": "PA", "Title": "Collaborative Research: SI2-SSI: The Community-Driven BiG CZ Software System for Integration and Analysis of Bio- and Geoscience Data in the Critical Zone", "ProgramManager": "Daniel Katz", "State": "PA", "Program": ["ECOSYSTEM STUDIES", "EAR", "GEOBIOLOGY & LOW TEMP GEOCHEM", "CZO: CRITICAL ZONE OBSER SOLIC", "Software Institutes", "EarthCube"], "ProgramElementCode": ["1181", "6898", "7295", "7693", "8004", "8074"], "StartDate": "12/01/2013", "OrganizationStreet": "970 Spencer Road", "EndDate": "11/30/2016", "PrincipalInvestigator": "Anthony Aufdenkampe", "ARRAAmount": null, "OrganizationPhone": "6102682153", "AwardNumber": "1339834", "PIEmailAddress": "aufdenkampe@stroudcenter.org", "NSFDirectorate": "CSE", "Co-PIName": ["Kerstin Lehnert", "Robert Cheetham", "Jeffery Horsburgh", "Emilio Mayorga"], "LastAmendmentDate": "09/20/2013", "Organization": "Stroud Water Research Center", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1366089", "Abstract": "The Critical Zone (CZ) science community takes as its charge the effort to integrate theory, models and data from the multitude of disciplines collectively studying processes on the Earth&apos;s surface. The Critical Zone is Earth&apos;s permeable near-surface layer - from the atmosphere at the vegetation&apos;s canopy to the lower boundary of actively circulating groundwaters. The Critical Zone was a term coined by the National Research Council&apos;s Basic Research Opportunities in the Earth Sciences (BROES) Report (2001) to highlight the imperative for a new approach to thoroughly multi-disciplinary research on the zone of the Earth?s surface that is critical to sustaining terrestrial life on our planet. In January 2013, 103 members of the CZ community met for the CZ-EarthCube Domain Workshop (NSF Award #1252238) to prioritize the CZ community&apos;s key science drivers, key computational and information technology (&quot;cyber&quot;) challenges and key cyber needs. They identified that the central scientific challenge of the critical zone science community is to develop a &quot;grand unifying theory&quot; of the critical zone through a theory-model-data fusion approach. Work participants unanimously described that the key missing need of this approach was a future cyberinfrastructure for seamless 4D visual exploration of the integrated knowledge (data, model outputs and interpolations) from all the bio and geoscience disciplines relevant to critical zone structure and function, similar to today?s ability to easily explore historical satellite imagery and photographs of the earth&apos;s surface using Google Earth. This project takes the first &quot;BiG&quot; steps toward answering that need.&lt;br/&gt;&lt;br/&gt;The overall goal of this project is to co-develop with the CZ science and broader community, including natural resource managers and stakeholders, a web-based integration and visualization environment for joint analysis of cross-scale bio and geoscience processes in the critical zone (BiG CZ), spanning experimental and observational designs. Our Project Objectives are to: (1) Engage the CZ and broader community to co-develop and deploy the BiG CZ software stack; (2) Develop the BiG CZ Portal web application for intuitive, high-performance map-based discovery, visualization, access and publication of data by scientists, resource managers, educators and the general public; (3) Develop the BiG CZ Toolbox to enable cyber-savvy CZ scientists to access BiG CZ Application Programming Interfaces (APIs); and (4) Develop the BiG CZ Central software stack to bridge data systems developed for multiple critical zone domains into a single metadata catalog. The entire BiG CZ Software system will be developed on public repositories as a modular suite of fully open source software projects. It will be built around a new Observations Data Model Version 2.0 (ODM2) that is being developed by members of the BiG CZ project team, with community input, under separate funding (NSF Award #1224638).", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "193119514"}, {"OrganizationCity": "KNOXVILLE", "OrganizationState": "TN", "Title": "Collaborative Research: SI2-SSE: UT Wrangler: Understanding the Software Needs of High End Computer Users", "ProgramManager": "Rajiv Ramnath", "State": "TN", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "10/01/2013", "OrganizationStreet": "1 CIRCLE PARK", "EndDate": "03/31/2016", "PrincipalInvestigator": "Mark Fahey", "ARRAAmount": null, "OrganizationPhone": "8659743466", "AwardNumber": "1339690", "PIEmailAddress": "markrfahey@uchicago.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "09/13/2013", "Organization": "University of Tennessee Knoxville", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "259931", "Abstract": "This research addresses two important questions: what software do researchers actually use on high-end computers, and how successful are they in their efforts to use it? It is a plan to improve our understanding of individual users&apos; software needs, then leverage that understanding to help stakeholders conduct business in a more efficient, effective, and systematic way. The signature product, UTWrangler, builds on work that is already improving the user experience and enhancing support programs for thousands of users on twelve supercomputers across the United States and Europe. For the first time, complete, accurate, detailed, and continuous ground truth information about software needs, trends, and issues at the level of the individual job are being delivered.&lt;br/&gt;&lt;br/&gt;UTWrangler will instrument, monitor, and analyze individual jobs on high-end computers to generate a picture of the compilers, libraries, and other software that users need to run their jobs successfully. It will highlight the products our researchers need and do not need, and alert users and support staff to the root causes of software configuration issues as soon as the problems occur. UTWrangler&apos;s prototypes prove its value and future impact: simplifying end users&apos; workflows; improving support, training and documentation; saving money; and helping administrators prioritize maintenance of their large base of installed software. UTWrangler will build on the capabilities of its prototypes, providing a robust, sustainable, second generation mechanism that will help the computational research community make the most effective use of limited computing cycles and labor hours. And UTWrangler will mitigate the difficulties new users encounter, reporting configuration problems as soon as jobs begin, and identifying opportunities to improve documentation, education and outreach programs.", "ProgramReferenceCode": ["7433", "8005", "9150"], "OrganizationZip": "379960003"}, {"OrganizationCity": "Chicago", "OrganizationState": "IL", "Title": "SI2-SSE: Collaborative Research: Software Elements for Transfer and Analysis of Large-Scale Scientific Data", "ProgramManager": "Rajiv Ramnath", "State": "IL", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "09/01/2013", "OrganizationStreet": "5801 South Ellis Avenue", "EndDate": "08/31/2016", "PrincipalInvestigator": "Rajkumar Kettimuthu", "ARRAAmount": null, "OrganizationPhone": "7737028669", "AwardNumber": "1339798", "PIEmailAddress": "kettimut@mcs.anl.gov", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/29/2013", "Organization": "University of Chicago", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "99995", "Abstract": "As science has become increasingly data-driven, and as data volumes and velocity are increasing, scientific advance in many areas will only be feasible if critical `big-data&apos; problems are addressed - and even more importantly, software tools embedding these solutions are readily available to the scientists. Particularly, the major challenge being faced by current data-intensive scientific research efforts is that while the dataset sizes continue to grow rapidly, neither among network bandwidths, memory capacity of parallel machines, memory access speeds, and disk bandwidths are increasing at the same rate.&lt;br/&gt;Building on top of recent research at Ohio State University, which includes work on automatic data virtualization, indexing methods for scientific data, and a novel bit-vectors based sampling method, the goal of this project is to fully develop, disseminate, deploy, and support robust software elements addressing challenges in data transfers and analysis. The prototypes that have been already developed at Ohio State are being extended into two robust software elements: an extention of GridFTP (Grid Partial-File Transport Protocol)that allows users to specify a subset of the file to be transferred, avoiding unnecessary transfer of the entire file; and Parallel Readers for NetCDF and HDF5 for Paraview and VTK, data subsetting and sampling tools for NetCDF and HDF5 that perform data selection and sampling at the I/O level, and in parallel.&lt;br/&gt;This project impacts a number of scientific areas, i.e., any area that involves big (and growing) dataset sizes and need for data transfers and/or visualization. This project also contributes to computer science research in `big data&apos;, including scientific (array-based) databases, and visualization. Another contribution will be towards preparation of the broad science and engineering research community for big data handling and analytics.", "ProgramReferenceCode": ["7433", "8005"], "OrganizationZip": "606375418"}, {"OrganizationCity": "FAIRFAX", "OrganizationState": "VA", "Title": "Collaborative: SI2-SSE - A Plug-and-Play Software Platform of Robotics-Inspired Algorithms for Modeling Biomolecular Structures and Motions", "ProgramManager": "Daniel Katz", "State": "VA", "Program": ["CDS&E", "Software Institutes", "SPECIAL PROJECTS - CCF"], "ProgramElementCode": ["8084", "8004", "2878"], "StartDate": "02/01/2015", "OrganizationStreet": "4400 UNIVERSITY DR", "EndDate": "01/31/2018", "PrincipalInvestigator": "Amarda Shehu", "ARRAAmount": null, "OrganizationPhone": "7039932295", "AwardNumber": "1440581", "PIEmailAddress": "ashehu@gmu.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/08/2014", "Organization": "George Mason University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "217288", "Abstract": "This project aims to develop a novel plug-and-play platform of open-source software elements to advance algorithmic research in molecular biology. The focus is on addressing the algorithmic impasse faced by computational chemists and biophysicists in structure-function related problems involving dynamic biomolecules central to our biology. The software platform resulting from this project provides the critical software infrastructure to support transformative research in molecular biology and computer science that benefits society at large by advancing our modeling capabilities and in turn our understanding of the role of biomolecules in critical mechanisms in a living and diseased cell.&lt;br/&gt;&lt;br/&gt;The project addresses the current impasse on the length and time scales that can be afforded in biomolecular modeling and simulation. It does so by integrating cutting-edge knowledge from two different research communities, computational chemists and biophysicists focused on detailed physics-based simulations, and AI researchers focused on efficient search and optimization algorithms. The software elements integrate sophisticated energetic models and molecular representations with powerful search and optimization algorithms for complex modular systems inspired from robot motion planning. The plug-and-play feature of the software platform supports putting together novel algorithms, such as wrapping Molecular Dynamics or Monte Carlo as local search operators within larger robotics-inspired exploration frameworks, and adding emerging biomolecular representations, models, and search techniques even beyond the timeline of this project.", "ProgramReferenceCode": ["7433", "8005", "8004", "9216", "8084", "2878"], "OrganizationZip": "220304422"}, {"OrganizationCity": "Austin", "OrganizationState": "TX", "Title": "SI2-SSI: Collaborative Research: STORM: A Scalable Toolkit for an Open Community Supporting Near Realtime High Resolution Coastal Modeling", "ProgramManager": "Rajiv Ramnath", "State": "TX", "Program": ["Software Institutes", "OFFICE OF MULTIDISCIPLINARY AC", "EarthCube", "CDS&E-MSS", "PHYSICAL OCEANOGRAPHY", "SPECIAL PROJECTS - CISE", "SPECIAL PROJECTS - CCF"], "ProgramElementCode": ["8004", "1253", "8074", "8069", "1610", "1714", "2878"], "StartDate": "10/01/2014", "OrganizationStreet": "101 E. 27th Street, Suite 5.300", "EndDate": "09/30/2018", "PrincipalInvestigator": "Clinton Dawson", "ARRAAmount": null, "OrganizationPhone": "5124716424", "AwardNumber": "1339801", "PIEmailAddress": "clint@ices.utexas.edu", "NSFDirectorate": "CSE", "Co-PIName": "Craig Michoski", "LastAmendmentDate": "02/27/2015", "Organization": "University of Texas at Austin", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "540012", "Abstract": "The ADCIRC coastal circulation and storm surge model has a long standing track record of extremely high societal impact. It has been used to define risk (e.g., 100-yr, 500-yr coastal flood levels) for the FEMA National Flood Insurance Program in coastal states from New York to Texas, it has been used to design the multi-billion dollar Hurricane and Storm Damage Risk Reduction System around greater New Orleans and southern Louisiana by the Army Corps of Engineers, it is currently run operationally by NOAA/NWS National Center for Environmental Prediction to forecast storm surge, to name just a few of its current and recent applications. Thus there is a well-established user network in place to convert improvements in ADCIRC into significant broader impacts. The proposed research provides transformative intellectual contributions that focus on applying new parallelization schemes to enable major advances in the algorithms, implementation and utilization of the ADCIRC model. The broadening of ADCIRC to a multi-algorithmic framework and the resulting performance gains that are anticipated will help ensure ADCIRC&apos;s sustainability as a core community model for at least the next 20 years. In addition, the proposed collaboration will impact computer science by serving as a high impact use case to inform the design of new approaches to efficient scalable computing. Together, the advancements in coastal modeling and parallelization technology will make a significant contribution to the science of modeling and HPC. The results of the proposed research will be disseminated to a wider community through ongoing educational outreach activities at the participating organizations as well as through refereed conference and journal papers, and invited presentations. The involvement of graduate students and post-doctoral fellows will be crucial towards the success of this project. The PIs have a long history of training and mentoring students and post-docs in computational science and engineering, coastal engineering and marine science. The recruitment and involvement of underrepresented groups in these efforts has always been a high priority. In addition, aspects of the proposed research will be incorporated into the curricula of several courses taught by the PIs in the areas of finite element methods, scientific computation, hydrology and oceanography.&lt;br/&gt;The aim of this project is to broaden the ADCIRC coastal circulation and storm surge model from a successful, but somewhat static coastal modeling tool that is tied to a single solution algorithm and the MPI parallelization paradigm, to a dynamic computational platform that is comprised of multiple solution algorithms, that readily admits new solution algorithms and that is built on a transformational new parallelization scheme that will allow us to scale to at least 256k compute cores on modern high performance computing (HPC) systems. We will do this by creating a living, evolving coastal modeling framework that will continue to lead the community in merging physical science / engineering and high performance computing and we will make the framework available to the broader community as a sustainable long term solution for its coastal modeling needs. In addition we will utilize these advancements in the highly demanding coastal storm surge forecasting system that we presently operate to demonstrate both improved robustness and speed of the model solution. We expect this effort will shorten the time required to provide reliable forecasting results and improve our ability to provide highly resolved, accurate, and physically complete predictions on an unprecedented scale. Concurrently, it should enable the use of smaller resources for simulations of increased scale which improves the usability and widens the applicability of ADCIRC in a broader community. The development of tightly integrated web-oriented products like CERA (www.coastalemergency.org) will enable the wide and timely dissemination of forecast modeling results to reach a broad audience.", "ProgramReferenceCode": ["7433", "8009", "8251"], "OrganizationZip": "787121532"}, {"OrganizationCity": "Chicago", "OrganizationState": "IL", "Title": "SI2-SSI: CRESCAT, A Computational Research Ecosystem for Scientific Collaboration on Ancient Topics, Spanning the Full Data Life Cycle", "ProgramManager": "Rajiv Ramnath", "State": "IL", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "09/01/2015", "OrganizationStreet": "5801 South Ellis Avenue", "EndDate": "08/31/2019", "PrincipalInvestigator": "David Schloen", "ARRAAmount": null, "OrganizationPhone": "7737028669", "AwardNumber": "1450455", "PIEmailAddress": "d-schloen@uchicago.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Thomas Levy", "Kathleen Morrison", "Hakizumwami B. Runesha"], "LastAmendmentDate": "08/28/2015", "Organization": "University of Chicago", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1500000", "Abstract": "This project integrates, tests, and documents a suite of interoperable software tools to support collaborative research. The tools are collectively called CRESCAT (Computational Research Ecosystem for Scientific Collaboration on Ancient Topics). The initial focus is on disciplines that deal with dynamic interactions and structural changes within spatially situated populations over long time spans in the past, e.g., paleobiology, archaeology, and economic history. Despite their differences, these disciplines have similar computational needs for modeling and analyzing data. Moreover, the same software can be used in many other disciplines, enabling economies of scale by building and maintaining a common set of interoperable tools to serve a wide range of researchers, while spanning the full research data life cycle, consisting of (1) acquisition, (2) integration, (3) analysis, (4) publication, and (5) archiving of data. An intuitive graphical user interface is provided for end-user researchers to work with their data in all stages of the life cycle without cumbersome manual data transfers and transformations. The project will address a major computational problem that affects many scientific disciplines due to the challenge of integrating and analyzing data of diverse origins based on heterogeneous spatial, temporal, and taxonomic ontologies. Thus it will have a broad impact in the sciences and beyond by showing how to represent explicitly the full variability of individual judgments and the divergent conceptualizations and terminologies through which those judgments are expressed, with explicit attribution of each observation, interpretation, and conceptual ontology to a particular named person or group. Unlike many computational tools for scientific research, which assume a degree of ontological consensus that does not exist, CRESCAT conforms to actual research practices. It does not impose a standardized ontology, thereby ignoring or suppressing the inevitable disagreements and conflicting interpretations that arise among researchers. Instead, it represents ontological diversity, observational uncertainty, and interpretive disagreement explicitly within a larger common framework in which end users can query, analyze, and compare the full range of observations, interpretations, and terminologies to inform their own judgments about the evidence. CRESCAT is designed to allow scientific disagreements and observational and interpretive uncertainties to be represented digitally in a way that exposes these differences themselves as data for analysis and debate. Thus, in addition to the practical goal of building a more efficient shared framework for advanced research, the proposed work will provoke theoretical reflection about how computational tools should relate to scientific practice.&lt;br/&gt;&lt;br/&gt;The CRESCAT project is an interdisciplinary collaboration between computer scientists, paleobiologists, geoscientists, archaeologists, economic historians, and other social scientists. The goal is to demonstrate the value of an integrative software ecosystem that spans the social and natural sciences and can facilitate any research characterized by overlapping models of temporal and spatial relations or by conflicting terminologies and taxonomies. CRESCAT&apos;s representation of scientific knowledge eschews forced standardization, which is impractical in many cases due to lack of an enforcement mechanism and is also questionable in principle since divergent ontologies often legitimately reflect different theoretical assumptions and research agendas. Central to the CRESCAT suite of tools is an innovative data-integration system that represents explicitly both research data and the ontologies inherent in the data. An ontology is defined here as a conceptual model of entities and the relationships among them in a given domain of knowledge, in contrast to a schema,&amp;#148;which is the implementation of an ontology in logical data structures within a working system. CRESCAT&apos;s data-integration system operates at a level of abstraction sufficient to provide a predictable and efficiently queryable database structure based on an abstract global schema, which in turn is based on an upper ontology specified in terms of fundamental concepts and relationships applicable to all scientific and scholarly disciplines. The data-integration system is implemented in an enterprise-class XML/XQuery DBMS that serves as a data warehouse (using the non-relational graph data model), in which is stored diverse data from a wide range of research projects representing many disciplines. The terminology and conceptual distinctions of each research project are fully preserved. The approach to research data taken in the CRESCAT project is (1) coherent, tightly integrating software tools and data formats within a single analytical framework; (2) open-ended, interconnecting existing tools while allowing the addition of new tools in the future; (3) non-exclusive, in no way preventing its component tools from participating in other software ecosystems; (4) scalable, designed to handle large-scale data management, analysis, and visualization; and (5) sustainable, maintaining shared resources to meet common needs for software and technical support and thus enabling substantial economies of scale.", "ProgramReferenceCode": ["7433", "8009", "8004"], "OrganizationZip": "606375418"}, {"OrganizationCity": "ORLANDO", "OrganizationState": "FL", "Title": "SI2-SSE: LC/DC: Lockless Containers and Data Concurrency", "ProgramManager": "Daniel Katz", "State": "FL", "Program": ["SPECIAL PROJECTS - CCF", "Software Institutes"], "ProgramElementCode": ["2878", "8004"], "StartDate": "01/01/2015", "OrganizationStreet": "4000 CNTRL FLORIDA BLVD", "EndDate": "12/31/2017", "PrincipalInvestigator": "Damian Dechev", "ARRAAmount": null, "OrganizationPhone": "4078821120", "AwardNumber": "1440530", "PIEmailAddress": "dechev@eecs.ucf.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/13/2014", "Organization": "University of Central Florida", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "500000", "Abstract": "Multicore programming demands a change in the way we design and use fundamental algorithms and data structures. This research represents a forward-looking and pragmatic approach that will lead to the discovery of the key principles for effective data and resource management for multiprocessor application development. In the course of this project the PI will create new methodologies and tools for the design, verification, and effective use of lock-free and wait-free data structures and algorithms. The proposed methodology will allow for the construction and use of lightweight multiprocessor algorithms with minimal overhead by supporting only the minimal set of operations and guarantees required by the user&apos;s application. The ideas advanced in this work will allow first-of-a-kind technology that will deliver immense boost in performance and software reuse; thus productivity will increase for developers of commercial and scientific applications. This research will pave the way for tool-based specification and verification of nonblocking algorithms, which will help reliability of multiprocessor programs. &lt;br/&gt;&lt;br/&gt;This work will create novel multiprocessor data structures that provide wait-free and lock-free progress. A concurrent object is lock-free if it guarantees that some thread makes progress. A wait-free algorithm guarantees that all threads make progress, thus eliminating performance bottlenecks and entire classes of safety hazards such as starvation, deadlock, and order violations. Unlike a sequential data structure, a concurrent container must maintain correctness when multiple threads are performing its operations. Achieving this correctness adversely affects the complexity and performance of the operations. As a result, users of concurrent containers are often forced to sacrifice functionality or safety guarantees to achieve desired performance. Here, the PI will introduce the use of alternative function models that will deliver high performance in parts of the program that require less functionality and more functionality in other fragments of the program that need it. The deliverables of this research include: a collection of formally verified multiprocessor data structure designs including queues, vectors, ring buffers, sets, and hash maps; a wait-free database; a multiple resource lock manager; a set of unified concurrent APIs to assist the end users of the data structures; and a technique for specification of the key progress and correctness properties of these containers. All software developed under this project will be released under BSD License and will be made available to the broad research and development community.", "ProgramReferenceCode": ["7433", "8005", "8004"], "OrganizationZip": "328168005"}, {"OrganizationCity": "Ann Arbor", "OrganizationState": "MI", "Title": "Collaborative Research: SI2-SSE: Pythia Network Diagnosis Infrastructure (PuNDIT)", "ProgramManager": "Daniel Katz", "State": "MI", "Program": ["Software Institutes", "Campus Cyberinfrastrc (CC-NIE)"], "ProgramElementCode": ["8004", "8080"], "StartDate": "09/01/2014", "OrganizationStreet": "3003 South State St. Room 1062", "EndDate": "08/31/2016", "PrincipalInvestigator": "Shawn McKee", "ARRAAmount": null, "OrganizationPhone": "7347636438", "AwardNumber": "1440571", "PIEmailAddress": "smckee@umich.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/14/2014", "Organization": "University of Michigan Ann Arbor", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "231187", "Abstract": "In today&apos;s world of distributed collaborations of scientists, there are many challenges to providing effective infrastructures to couple these groups of scientists with their shared computing and storage resources. The Pythia Network Diagnostic InfrasTructure (PuNDIT) project will integrate and scale research tools and create robust code suitable for operational needs to address the difficult challenge of automating the detection and location of network problems. &lt;br/&gt;&lt;br/&gt;PuNDIT will build upon the de-facto standard perfSONAR network measurement infrastructure to gather and analyze complex real-world network topologies coupled with their corresponding network metrics to identify possible signatures of network problems from a set of symptoms. For example if the symptoms suggest a router along the path has buffers configured too small for high performance, Pythia will return a diagnosis of &quot;Small Buffer&quot;. If symptoms indicate non-congestive packet-loss for a particular network segment, the user can be notified of a possible &quot;Bad Network Segment&quot;. A primary goal for PuNDIT is to convert complex network metrics into easily understood diagnoses in an automated way.", "ProgramReferenceCode": ["7433", "8005"], "OrganizationZip": "481091274"}, {"OrganizationCity": "MANHATTAN", "OrganizationState": "KS", "Title": "SI2-CHE: CCP-SAS - Collaborative Computing consortium for advanced analyses of structural data in chemical biology and soft condensed matter", "ProgramManager": "Evelyn M. Goldfield", "State": "KS", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes"], "ProgramElementCode": ["1253", "8004"], "StartDate": "06/01/2013", "OrganizationStreet": "2 FAIRCHILD HALL", "EndDate": "05/31/2017", "PrincipalInvestigator": "Jianhan Chen", "ARRAAmount": null, "OrganizationPhone": "7855326804", "AwardNumber": "1265850", "PIEmailAddress": "jianhanc@ksu.edu", "NSFDirectorate": "MPS", "LastAmendmentDate": "05/31/2013", "Organization": "Kansas State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "CHE", "AwardedAmountToDate": "224492", "Abstract": "An international team consisting of Paul Butler (University of Tennessee, Knoxville and NIST), Jianhan Chen (Kansas State University), Emre Brooks (University of Texas Health Science Center, San Antonio) and their UK collaborators led by Stephen J. Perkins (University College, London) are supported through the SI2-CHE program. The object of this project is to provide a web-based GUI front-end with a high-performance back-end to increase the accessibility of advanced atomistic modeling of scattering data by novice users. Advanced analysis modules and new simulation methods including implicit solvent models are being developed to increase the accuracy of scattering calculations and simulation protocols. Software is being developed and disseminated with continual feedback from the research teams&apos; large international user-base. These efforts leverage an international user and developer community that use high-performance computing resources on a wide-range of cutting-edge chemical problems. &lt;br/&gt;&lt;br/&gt;A typical bench scientist purifies and characterizes samples, collects the small angle x-ray scattering (SAXS), small angle neutron scattering (SANS), or analytical ultracentrifugation (AUC) data, and interprets the results using simplistic models. It is rare that the same individual also has the skills to use advanced atomistic simulation software. The CCP-SAS project is focused on developing an easy-to-use modeling package that enables users to generate physically accurate atomistic models, calculate scattering profiles and compare results to experimental scattering data sets in a single web-based software suite. This enables a broad range of scattering scientists to access often complicated simulation and scattering analysis methods seamlessly thus providing a significant acceleration to the discovery process. &lt;br/&gt;&lt;br/&gt;The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC.", "ProgramReferenceCode": ["5918", "5946", "5950", "7433", "8009", "9150"], "OrganizationZip": "665061103"}, {"OrganizationCity": "LOS ANGELES", "OrganizationState": "CA", "Title": "SI2-SSI: Particle-In-Cell and Kinetic Simulation Center", "ProgramManager": "Daniel Katz", "State": "CA", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "PHYSICS AT THE INFO FRONTIER", "Software Institutes"], "ProgramElementCode": ["1253", "7553", "8004"], "StartDate": "09/01/2013", "OrganizationStreet": "11000 Kinross Avenue, Suite 211", "EndDate": "08/31/2018", "PrincipalInvestigator": "Warren Mori", "ARRAAmount": null, "OrganizationPhone": "3107940102", "AwardNumber": "1339893", "PIEmailAddress": "mori@physics.ucla.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Frank Tsung", "Michail Tzoufras", "Russel Caflisch", "Viktor Decyk"], "LastAmendmentDate": "07/16/2015", "Organization": "University of California-Los Angeles", "AwardInstrument": "Continuing grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "3600000", "Abstract": "Computer simulations using the particle-in-cell (PIC) method are widely used for basic and applied research involving plasma physics applications. For example, simulations that calculate the self-consistent interaction of charged particles aid in the development of new accelerator technologies, new radiation sources, are used in magnetic and inertial fusion research, and help understand the physics of the solar wind. The Particle-in-Cell and Kinetic Simulation Software Center (PICKSC) at UCLA will aim to significantly broaden the impact of PIC simulations by making available and documenting illustrative software programs for different computing hardware, a flexible Framework for rapid construction of parallelized PIC programs, and several distinct production programs. This project will also include activities on developing and comparing different PIC algorithms and documenting best practices for developing and using PIC programs. The activities fostered by this project will bring together an interdisciplinary team of faculty, research staff, post-doctoral scholars, and graduate students. Important goals of this project include also the development of educational software for undergraduate and graduate courses in plasma physics and computer science and will, to build a large community of users through outreach and an annual workshop. &lt;br/&gt;&lt;br/&gt;The broader impact of the activities fostered by this project will be significant. A well-documented set of components and example codes for running on large computers and a set of basic production codes will allow students and researchers from all levels and many disciplines to understand the inner workings of optimized PIC and kinetic simulation software used to model plasmas. It will allow them to build their own software, or to make independent comparisons against commercially available codes, against their own codes, and against published simulation data. The availability of production codes to more researchers will increase the rate of scientific discovery. The software will allow computer scientists who are developing tools that allow existing software to use next generation hardware to compare their performances against highly optimized codes, and will provide new code developers a test-bed of parallelized and optimized software for performance comparison. Furthermore, this projet will make state-of-the-art research software available for education, both for physics and computer science courses, will help train the next generation of plasma physicists (in many sub disciplines) and computational scientists. Interactive tools based on simpler skeleton codes will also be useful for undergraduate and high school education. Documenting examples of best practices will save graduate students and new researchers significant time in learning how to best employ PIC simulations.", "ProgramReferenceCode": ["7433", "7483", "8009", "8084"], "OrganizationZip": "900952000"}, {"OrganizationCity": "GAINESVILLE", "OrganizationState": "FL", "Title": "SI2-SSE Collaborative Research: Molecular Simulations of Polymer Nanostructures in the Cloud", "ProgramManager": "Daniel Katz", "State": "FL", "Program": ["DMR SHORT TERM SUPPORT", "Software Institutes", "CDS&E"], "ProgramElementCode": ["1712", "8004", "8084"], "StartDate": "10/22/2015", "OrganizationStreet": "1 UNIVERSITY OF FLORIDA", "EndDate": "09/30/2017", "PrincipalInvestigator": "Coray Colina", "ARRAAmount": null, "OrganizationPhone": "3523923516", "AwardNumber": "1613155", "PIEmailAddress": "colina@chem.ufl.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "11/10/2015", "Organization": "University of Florida", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "149283", "Abstract": "The use of polymers, their composites, and nanostructures is growing at a fast pace, both by displacing traditional materials and by enabling emerging technologies. Examples range from the all-composite airframe of the Boeing 787 and the new Airbus 350 to wearable electronics. This project aims to develop a software infrastructure to simulate these materials with atomic resolution and make these tools universally accessible and useful via on-line computing. These simulations have the potential to accelerate the development of optimized material formulations that can benefit society and reduce the associated costs by combining physical with computational experiments. Making these advanced tools available for free online simulations and complementing them with tutorials and educational material will encourage their use in the classroom and will impact the education of new generations of engineers and scientists familiar with these powerful tools that will be required to address tomorrow&apos;s challenges.&lt;br/&gt;&lt;br/&gt;The objective of this effort is to enable pervasive, reproducible molecular simulations of polymeric materials using state-of-the-art tools and with quantified uncertainties building on recent breakthroughs in molecular simulations, cyber-infrastructure and uncertainty quantification. The framework will consist of three main components: i) powerful simulation tools for polymer nano structures including: state-of-the-art molecular builders, a parallel MD engine with stencils that enable efficient structure relaxation and property characterization and post-processing codes; ii) a UQ framework to orchestrate the molecular simulations and propagate uncertainties in input parameters to predictions and compare the predictions to experimental values; iii) databases of force fields and molecular structures as well as predicted and experimental properties. The simulation framework will be deployed via NSF&apos;s nanoHUB where users will be able to run online simulations without downloading or installing any software while expert users will have the option to download, modify and contribute to the infrastructure. Usage of and contributions to the software framework will be facilitated and encouraged via online and in-person user guides, learning modules and research tutorials.", "ProgramReferenceCode": ["024E", "085E", "7237", "7433", "8005", "8400", "9216"], "OrganizationZip": "326112002"}, {"OrganizationCity": "Cambridge", "OrganizationState": "MA", "Title": "SI2-SSI: Collaborative Research: Bringing End-to-End Provenance to Scientists", "ProgramManager": "Daniel Katz", "State": "MA", "Program": ["SPECIAL PROJECTS - CCF", "Software Institutes"], "ProgramElementCode": ["2878", "8004"], "StartDate": "06/01/2015", "OrganizationStreet": "1033 MASSACHUSETTS AVE", "EndDate": "05/31/2018", "PrincipalInvestigator": "Margo Seltzer", "ARRAAmount": null, "OrganizationPhone": "6174955501", "AwardNumber": "1450277", "PIEmailAddress": "margo@eecs.harvard.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Emery Boose", "Aaron Ellison"], "LastAmendmentDate": "08/13/2015", "Organization": "Harvard University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1422728", "Abstract": "Reproducability is the cornerstone of scientific progress. Historically, scientists make their work reproducible by including a formulaic description of the experimental methodology used in an experiment. In an age of computational science, such descriptions no longer adequately describe scientific methodology. Instead, scientific reproducibility relies on a precise and actionable description of the data and programs used to conduct the research. Provenance is the name given to the description of how a digital artifact came to be in its present state. Provenance includes a precise specification of an experiment&apos;s input data and the programs or procedures applied to that data. Most computational platforms do not record such data provenance, making it difficult to ensure reproducability. This project addresses this problem through the development of tools that transparently and automatically capture data provenance as part of a scientist&apos;s normal computational workflow.&lt;br/&gt;&lt;br/&gt;An interdisciplinary team of computer scientists and ecologists have come together to develop tools to facilitate the capture, management, and query of data provenance -- the history of how a digital artifact came to be in its present state. Such data provenance improves the transparency, reliability, and reproducibility of scientific results. Most existing provenance systems require users to learn specialized tools and jargon and are unable to integrate provenance from different sources; these are serious obstacles to adoption by domain scientists. This project includes the design, development, deployment, and evaluation of an end-to-end system (eeProv) that encompasses the range of activity from original data analysis by domain scientists to management and analysis of the resulting provenance in a common framework with common tools. This project leverages and integrates development efforts on (1) an emerging system for generating provenance from a computing environment that scientists actually use (the R statistical language) with (2) an emerging system that utilizes a library of language and database adapters to store and manage provenance from virtually any source. Accomplishing the goals of this proposal requires fundamental research in resolving the semantic gap between provenance collected in different environments, capturing detailed provenance at the level of a programming language, defining precisely aspects of provenance required for different use cases, and making provenance accessible to scientists.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "021385366"}, {"OrganizationCity": "KNOXVILLE", "OrganizationState": "TN", "Title": "SI2-SSI: Collaborative Proposal: Performance Application Programming Interface for Extreme-Scale Environments (PAPI-EX)", "ProgramManager": "Daniel Katz", "State": "TN", "Program": ["Software Institutes", "CDS&E"], "ProgramElementCode": ["8004", "8084"], "StartDate": "09/01/2015", "OrganizationStreet": "1 CIRCLE PARK", "EndDate": "08/31/2019", "PrincipalInvestigator": "Jack Dongarra", "ARRAAmount": null, "OrganizationPhone": "8659743466", "AwardNumber": "1450429", "PIEmailAddress": "dongarra@icl.utk.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Anthony Danalis", "Heike Jagode"], "LastAmendmentDate": "08/27/2015", "Organization": "University of Tennessee Knoxville", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "2126446", "Abstract": "Modern High Performance Computing (HPC) systems continue to increase in size and complexity. Tools to measure application performance in these increasingly complex environments must also increase the richness of their measurements to provide insights into the increasingly intricate ways in which software and hardware interact. The PAPI performance-monitoring library has provided a clear, portable interface to the hardware performance counters available on all modern CPUs and some other components of interest (scattered across the chip and system). Widely deployed and widely used, PAPI has established itself as fundamental software infrastructure, not only for the scientific computing community, but for many industry users of HPC as well. But the radical changes in processor and system design that have occurred over the past several years pose new challenges to PAPI and the HPC software infrastructure as a whole. The PAPI-EX project integrates critical PAPI enhancements that flow from both governmental and industry research investments, focusing on processor and system design changes that are expected to be present in every extreme scale platform on the path to exascale computing.&lt;br/&gt;&lt;br/&gt;The primary impact of PAPI-EX is a direct function of the importance of the PAPI library. PAPI has been in predominant use by tool developers, major national HPC centers, system vendors, and application developers for over 15 years. PAPI-EX builds on that foundation. As important research infrastructure, the PAPI-EX project allows PAPI to continue to play its essential role in the face of the revolutionary changes in the design and scale of new systems. In terms of enhancing discovery and education, the list of partners working with PAPI-EX includes NSF computing centers, major tool developers, major system vendors, and individual community leaders, and this diverse group will help facilitate training sessions, targeted workshops, and mini-symposia at national and international meetings. Finally, the active promotion of PAPI by many major system vendors means that PAPI, and therefore PAPI-EX, will continue to deliver major benefits for government and industry in many domains.&lt;br/&gt;&lt;br/&gt;PAPI-EX addresses a hardware environment in which the cores of current and future multicore CPUs share various performance-critical resources (a.k.a., &apos;inter-core&apos; resources), including power management, on-chip networks, the memory hierarchy, and memory controllers between cores. Failure to manage contention for these &apos;inter-core&apos; resources has already become a major drag on overall application performance. Consequently, the lack of ability to reveal the actual behavior of these resources at a low level, has become very problematic for the users of the many performance tools (e.g., TAU, HPCToolkit, Open|SpeedShop, Vampir, Scalasca, CrayPat, Active Harmony, etc.). PAPI-EX enhances and extends PAPI to solve this critical problem and prepare it to play its well-established role in HPC performance optimization. Accordingly, PAPI-EX targets the following objectives: (1) Develop shared hardware counter support that includes system-wide and inter-core measurements; (2) Provide support for data-flow based runtime systems; (3) Create a sampling interface to record streams of performance data with relevant context; (4) Combine an easy-to-use tool for text-based application performance analysis with updates to PAPI?s high-level API to create a basic, ?out of the box? instrumentation API.", "ProgramReferenceCode": ["7433", "8009", "9150", "8084"], "OrganizationZip": "379960003"}, {"OrganizationCity": "La Jolla", "OrganizationState": "CA", "Title": "Collaborative Research SI2-SSE:Sustained Innovation in Acceleration of Molecular Dynamics on Future Computational Environments: Power to the People in the Cloud and on Accelerators", "ProgramManager": "Daniel Katz", "State": "CA", "Program": ["CHEMISTRY PROJECTS", "DMR SHORT TERM SUPPORT", "OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes"], "ProgramElementCode": ["1991", "1712", "1253", "8004"], "StartDate": "06/01/2012", "OrganizationStreet": "Office of Contract & Grant Admin", "EndDate": "05/31/2016", "PrincipalInvestigator": ["Amitava Majumdar", "Ross Walker"], "ARRAAmount": null, "OrganizationPhone": "8585344896", "AwardNumber": "1148276", "PIEmailAddress": ["majumdar@sdsc.edu", "rcw@sdsc.edu"], "NSFDirectorate": "CSE", "LastAmendmentDate": "06/24/2014", "Organization": "University of California-San Diego", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "320833", "Abstract": "This collaborative project between the San Diego Supercomputer Center at the University of California San Diego, the Quantum Theory Project at the University of Florida and industrial partners NVIDIA, Intel and Amazon is focused on developing innovative, comprehensive open source software element libraries for accelerating condensed phase Molecular Dynamics (MD) simulations of biomolecules using next generation accelerator hardware including Intel&apos;s MIC system and Graphics Processing Units (GPU). It will extend support to include all major MD techniques and develop open source accelerated analysis libraries. A priority is enhanced sampling techniques including Thermodynamic Integration, constant pH algorithms, Multi-Dimensional Hamiltonian Replica Exchange and Metadynamics. These elements will then be combined, in collaboration with Amazon to support MD as-a-service through easily accessible web front ends to cloud services, including Amazon&apos;s EC2 GPU hardware. Transitioning large scale MD workflows from requiring access to large supercomputer hardware to being accessible to all on desktop and cloud resources provides the critical software infrastructure to support transformative research in the fields of chemistry, life science, materials science, environmental and renewable energy.&lt;br/&gt;&lt;br/&gt;The software elements created through this project have an extremely broad impact. The integration of comprehensive support for next generation hardware acceleration into the AMBER software alone benefits a very large user base. With over 10,000 downloads of the latest AMBER Tools package from unique IPs and &gt;800 sites using the AMBER MD engines testify to the scope of the community of researchers this work impacts. The development of simple web based front ends for use of elastically scalable cloud resources makes simulations routine for all researchers. Meanwhile education and outreach efforts train the next generation of scientists not just in how to use the MD acceleration libraries and advanced MD simulation techniques developed here but also gets them thinking about how their approach can be transformed given that performance that was previously restricted to large scale supercomputers is now available on individual desktops.", "ProgramReferenceCode": ["8005", "7433", "9216", "9263", "7237", "7569", "7573", "7683"], "OrganizationZip": "920930934"}, {"OrganizationCity": "La Jolla", "OrganizationState": "CA", "Title": "Collaborative Research: SI2-SSE: A Petascale Numerical Library for Multiscale Phenomena Simulations", "ProgramManager": "Daniel Katz", "State": "CA", "Program": ["Software Institutes", "CDS&E"], "ProgramElementCode": ["8004", "8084"], "StartDate": "10/01/2013", "OrganizationStreet": "Office of Contract & Grant Admin", "EndDate": "09/30/2016", "PrincipalInvestigator": "Dmitry Pekurovsky", "ARRAAmount": null, "OrganizationPhone": "8585344896", "AwardNumber": "1339884", "PIEmailAddress": "dmitry@sdsc.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "09/11/2013", "Organization": "University of California-San Diego", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "358917", "Abstract": "Multiscale phenomena are a grand challenge for theory, simulations and experiments. It is difficult to understand phenomena at both highest and lowest scales, as well as all those in between. This challenge shows up in diverse fields. One of the long-standing problems in cosmology is understanding cosmological structure formation. In computer simulations of this process the challenge is increasing grid resolution while retaining the essential physics. In all-atom molecular dynamics simulations of enzymes the challenge is simulating systems with a large number of atoms while resolving long-range interactions and having sufficiently high throughput. Such simulations are critical in understanding important biological processes and eventually designing new drugs. Another prime example of multiscale phenomena is turbulent flows, a rich and complex subject of great relevance to many of the main technological issues of the day, including climate, energy, and the management of oil and biohazards. Understanding turbulent flows is critical for design of new transportation vehicles, improving efficiency of combustion processes and managing their environment pollution. Here simulations have been historically limited, and remain so, due to extremely high computational cost, even using the high-end computational systems available to researchers today. Reducing this cost, and efficiently using the computational resources, often requires specialized expertise, as well as significant development time and cost many research groups cannot afford. This project will develop a powerful suite of critical software components to provide tools for performing simulations of multiscale phenomena. &lt;br/&gt;&lt;br/&gt;The suite will implement state-of-the-art techniques for reducing communication cost, which has become the most important contributing factor to the total simulation cost, especially at larger scales. It will provide a flexible set of features that will make it usable in a great number of codes across the disciplines. In particular, the library will include user-friendly interfaces for Fourier transforms, spectral and compact differentiation in three dimensions, in addition to widely used communication routines (transposes, halo exchanges). This combination of emphasis on scalable performance and richness of features makes this suite unique among other libraries in existence today. Given the extraordinary challenge of simulations of multiscale phenomena, this library will provide a realistic path towards the Exascale. The suite will be available under an open source license. User outreach will be undertaken through the project website, mailing list, user surveys and presentations.", "ProgramReferenceCode": ["7433", "8005", "8084"], "OrganizationZip": "920930934"}, {"OrganizationCity": "MADISON", "OrganizationState": "WI", "Title": "Collaborative Research: SI2-SSE: Modules for Experiments in Stellar Astrophysics", "ProgramManager": "Daniel Katz", "State": "WI", "Program": ["Software Institutes", "STELLAR ASTRONOMY & ASTROPHYSC", "OFFICE OF MULTIDISCIPLINARY AC"], "ProgramElementCode": ["8004", "1215", "1253"], "StartDate": "01/01/2014", "OrganizationStreet": "21 North Park Street", "EndDate": "12/31/2016", "PrincipalInvestigator": "Richard Townsend", "ARRAAmount": null, "OrganizationPhone": "6082623822", "AwardNumber": "1339606", "PIEmailAddress": "townsend@astro.wisc.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/29/2013", "Organization": "University of Wisconsin-Madison", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "75888", "Abstract": "As the most commonly observed objects, stars remain at the forefront of astrophysical research. Technical advances in detectors, computer processing power, networks and data storage have enabled new sky surveys. Many of these search for transient events at optical wavelengths, such as the Palomar Transient Factory and Pan-STARRS1 that probe ever-larger areas of the sky and ever-fainter sources, opening up the vast discovery space of &quot;time domain astronomy&quot;. The recent Kepler and COROT space missions achieved nearly continuous monitoring of more than 100,000 stars. The stellar discoveries from these surveys include revelations about stellar evolution, rare stars, unusual explosion outcomes, and remarkably complex binary star systems. The immediate future holds tremendous promise, as both the space-based survey Gaia and the ground based Large Synoptic Survey Telescope come to fruition. This tsunami of data has created a new demand for a reliable and publicly available research and education tool in computational stellar astrophysics that will reap the full scientific benefits of these discoveries while also creating a collaborative environment where theory, computation and interpretation can come together to address critical scientific issues. This demand by the stellar community led to our release of the Modules for Experiments in Stellar Astrophysics (MESA) software project in 2011. MESA has driven, and will continue to drive with support from this award, innovation in the stellar community as well as the exoplanet, galactic, and cosmological communities. Educators have widely deployed MESA in their undergraduate and graduate stellar evolution courses because MESA is a community platform with an active support network for leading-edge scientific investigations. Stellar astrophysics research, and all the communities that rely on stellar astrophysics, will be significantly enhanced by sustaining innovative development of MESA.&lt;br/&gt;&lt;br/&gt;This award supports the Modules for Experiments in Stellar Astrophysics (MESA) software project and user community. MESA solves the 1D fully coupled structure and composition equations governing stellar evolution. It is based on an implicit finite difference scheme with adaptive mesh refinement and sophisticated timestep controls; state-of-the-art modules provide equation of state, opacity, nuclear reaction rates, element diffusion, boundary conditions, and changes to the mass of the star. MESA is an open source library that employs contemporary numerical approaches, supports shared memory parallelism based on OpenMP, and is written with present and future multi-core and multi-thread architectures in mind. MESA combines the robust, efficient, thread-safe numerical and physics modules for simulations of a wide range of stellar evolution scenarios ranging from very-low mass to massive stars. Innovations in MESA and its domain of applicability continues to grow, just recently extended to include giant planets, oscillations, and rotation. This project will sustain MESA as a key piece of software infrastructure for stellar astrophysics while building new scientific and educational networks.", "ProgramReferenceCode": ["7433", "8005", "1206"], "OrganizationZip": "537151218"}, {"OrganizationCity": "Durham", "OrganizationState": "NC", "Title": "Collaborative Research: SI2-SSI: ELSI-Infrastructure for Scalable Electronic Structure Theory", "ProgramManager": "Daniel Katz", "State": "NC", "Program": ["Software Institutes", "OFFICE OF MULTIDISCIPLINARY AC", "DMR SHORT TERM SUPPORT"], "ProgramElementCode": ["8004", "1253", "1712"], "StartDate": "06/15/2015", "OrganizationStreet": "2200 W. Main St, Suite 710", "EndDate": "05/31/2019", "PrincipalInvestigator": "Volker Blum", "ARRAAmount": null, "OrganizationPhone": "9196843030", "AwardNumber": "1450280", "PIEmailAddress": "volker.blum@duke.edu", "NSFDirectorate": "CSE", "Co-PIName": "Jianfeng Lu", "LastAmendmentDate": "06/11/2015", "Organization": "Duke University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1358608", "Abstract": "Predictive, so-called ab initio electronic structure calculations, particularly those based on the Kohn-Sham density functional theory (DFT) are now a widely used scientific workhorse with applications in virtually all sciences, and increasingly in engineering and industry. In materials science, they enable the computational (&quot;in silico&quot;) design of new materials with improved properties. In biological or pharmacological research, they provide molecular-level insights into the function of macromolecules or drugs. In the search for new energy solutions, they give molecular-level insights into new solar cell designs, catalytic processes, and many others. A key bottleneck in many applications and calculations is the &quot;cubic scaling wall&quot; of the so-called Kohn-Sham eigenvalue problem with system size (i.e., the effort increases by a factor of 1,000 if the model size increases by a factor of 10). This project will establish an open source software infrastructure &quot;ELSI&quot; that offers a common, practical interface to initially three complementary solution strategies to alleviate or overcome the difficulty associated with solving the Kohn-Sham eigenvalue problem. ELSI will enable a broad range of end user communities, centered around different codes with, often, unique features that tie a specialized group of scientists to that particular solution, to easily incorporate state-of-the-art solution strategies for a key problem they all share. By providing these effective, accessible solution strategies, we will open up major areas for electronic structure theory where DFT based predictive methodologies are not applicable today. This will in turn open doors for new development in materials science, chemistry, and all related areas. Commitments to support ELSI exist from some of the most important electronic structure developer communities, as well as from industry and government leaders in high-performance computing. Thus, we will create a strong U.S. based infrastructure that leverages the large user and developer base from a globally active community developing DFT methods for materials research.&lt;br/&gt;&lt;br/&gt;ELSI will support and enhance three state-of-the-art approaches, each best suited for a specific problem range: (i) The ELPA (EigensoLvers for Petascale Applications) library, a leading library for efficient, massively parallel solution of eigenvalue problems (for small- and mid-sized problems up to several 1,000s of atoms), (ii) the OMM (Orbital Minimization Method) in a recent re-implementation, which circumvents the eigenvalue problem by focusing on a reduced, auxiliary problem (for systems in the several 1,000s of atoms range), and (iii) the PEXSI (Pole EXpansion and Selective Inversion) library, a proven reduced scaling (at most quadratic scaling) solution for general systems (for problems with 1,000s of atoms and beyond). By establishing standardized interfaces in a style already familiar to many electronic structure developers, ELSI will enable production electronic structure codes that use it to significantly reduce the &quot;scaling wall&quot; of the eigenvalue problem. First, ELSI will help them make efficient use of the most powerful computational platforms available. The target platforms are current massively parallel computers and multicore architectures, GPU based systems and future manycore processors. Second, the project will make targeted methodological improvements to ELPA, OMM, and PEXSI, e.g., a more effective use of matrix sparsity towards very large systems. The focus on similar computational architectures and similar methodological enhancements will lead to significant cross-fertilization and synergy between these approaches.", "ProgramReferenceCode": ["7433", "8009", "9216", "8084"], "OrganizationZip": "277054010"}, {"OrganizationCity": "NOTRE DAME", "OrganizationState": "IN", "Title": "SI2-SSE: Development of Cassandra, A General, Efficient and Parallel Monte Carlo Multiscale Modeling Software Platform for Materials Research", "ProgramManager": "Daniel Katz", "State": "IN", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "DMR SHORT TERM SUPPORT", "Software Institutes"], "ProgramElementCode": ["1253", "1712", "8004"], "StartDate": "10/01/2013", "OrganizationStreet": "940 Grace Hall", "EndDate": "09/30/2016", "PrincipalInvestigator": "Edward Maginn", "ARRAAmount": null, "OrganizationPhone": "5746317432", "AwardNumber": "1339785", "PIEmailAddress": "ed@nd.edu", "NSFDirectorate": "CSE", "Co-PIName": "Jindal Shah", "LastAmendmentDate": "03/11/2015", "Organization": "University of Notre Dame", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "395133", "Abstract": "The properties of materials are the result of the interactions between the atoms that make up these materials. These properties can now be predicted with great accuracy, even for materials that have not yet existed in nature, by using advanced computational methods to study how the constituent atoms of the materials interact with one another and their environment. This field relies upon the existence of sophisticated software packages that enable researchers to conduct these simulations. There are two general approaches for simulating bulk materials: molecular dynamics and Monte Carlo, each of which is appropriate for certain problems. There are many molecular dynamics software packages available but almost no general purpose Monte Carlo codes. This project seeks to develop an efficient, general-purpose open source Monte Carlo code called Cassandra.&lt;br/&gt;&lt;br/&gt;To do this, the academic Monte Carlo code developed in the PI&apos;s group will be extended and enhanced. The code will be capable of simulating any type of molecule in bulk and heterogeneous environments. The code will contain a wide range of advanced features, making it useful for a range of problems. By providing a general purpose code to the research community and establishing a mechanism whereby users can add their own features and extend the code, this project will have a broad impact on the research community. It will enable non-experts to use Monte Carlo simulations to study new problems. It will enable experienced molecular modelers to utilize and contribute features to a single optimized and validated code, thereby alleviating the time and expense associated with developing specialized codes for individual applications. Because the code will be used in teaching and workshops, materials will be made available to educators to use the code in the classroom when teaching courses such as thermodynamics, molecular modeling and statistical mechanics.", "ProgramReferenceCode": ["7433", "7569", "8005", "9216", "9263"], "OrganizationZip": "465565708"}, {"OrganizationCity": "New Haven", "OrganizationState": "CT", "Title": "SI2-SSI: Collaborative Research: Scalable, Extensible, and Open Framework for Ground and Excited State Properties of Complex Systems", "ProgramManager": "Rajiv Ramnath", "State": "CT", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "DMR SHORT TERM SUPPORT", "CYBERINFRASTRUCTURE", "Software Institutes", "CDS&E"], "ProgramElementCode": ["1253", "1712", "7231", "8004", "8084"], "StartDate": "10/01/2013", "OrganizationStreet": "Office of Sponsored Projects", "EndDate": "09/30/2018", "PrincipalInvestigator": "Sohrab Ismail-Beigi", "ARRAAmount": null, "OrganizationPhone": "2037854689", "AwardNumber": "1339804", "PIEmailAddress": "sohrab.ismail-beigi@yale.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "09/04/2014", "Organization": "Yale University", "AwardInstrument": "Continuing grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1380247", "Abstract": "Computer simulation plays a central role in helping us understand, predict, and engineer the physical and chemical properties of technological materials systems such as semiconductor devices, photovoltaic systems, chemical reactions and catalytic behavior. Despite significant progress in performing realistic simulations in full microscopic detail, some problems are currently out of reach: two examples are the modeling of electronic devices with multiple functional parts based on new materials such as novel low power computer switches that would revolutionize the Information Technology industry, and the photovoltaic activity of complex interfaces between polymers and inorganic nanostructures that would enhance US energy self-reliance. The research program of this collaborative software institute aims to create an open and effective scientific software package that can make efficient use of cutting-edge high performance computers (HPC) to solve challenging problems involving the physics and chemistry of materials. By having such software available, this software initiative will have multiple broad impacts. First, the community of materials scientists will be able to study next-generation problems in materials physics and chemistry, and computer science advances that enable the software will be demonstrated and made accessible for both communities which will help cross-fertilize further such collaborative efforts. Second, the capability of simulating and engineering more complex materials systems and technological devices could play a role in helping the US continue is competitive edge in science, technology, and education. Third, through training of young scientists, direct outreach to the broader scientific community through workshops and conferences, and educational programs ranging from secondary to graduate levels, the power, importance, and capabilities of computational modeling, materials science, and computer science methodologies that enable the science will be communicated to a broad audience. Finally, by enabling the refinement of existing materials systems as well as discovery of new materials systems, the resulting scientific advances can help broadly impact society via technological improvements: in terms of the two examples provided above, (a) the successful design of new electronic device paradigms helps significantly advance the digital revolution by permitting the introduction of smaller, more efficient, and more capable electronic circuits and information processing systems, and (b) successful creation of inexpensive, easy-to-fabricate, and durable photovoltaic materials and devices can lead to cleaner forms of energy production while reducing reliance on fossil fuels.&lt;br/&gt;&lt;br/&gt;The technical goal is to greatly enhance the open software tool OPENATOM to advance discovery in nanoscience and technology. OPENATOM will be delivered as a open, robust and validated software package capable of utilizing HPC architectures efficiently to describe the electronic structure of complex materials systems from first principles. In terms of describing electronic ground-states, OPENATOM will be enhanced by features such as improved configurational sampling methods, hybrid density functionals, and incorporation of fast super-soft pseudopotential techniques. In addition, the team will incorporate the many-body GW-BSE approach for electronic excitations that permits accurate computation of electronic energy levels, optical absorption and emission, and luminescence. Ultimately, such an extensible software framework will permit accurate electronic structure computations to employ effectively future HPC platforms with 10,000,000 cores.", "ProgramReferenceCode": ["7433", "7569", "8009", "8084", "9216", "9263"], "OrganizationZip": "065208327"}, {"OrganizationCity": "AMHERST", "OrganizationState": "MA", "Title": "SI2-SSE: BenchLab: Open Community Tools and Infrastructure for Performance Research in Cloud, Mobile and Green Computing", "ProgramManager": "Daniel Katz", "State": "MA", "Program": ["SPECIAL PROJECTS - CCF", "Software Institutes"], "ProgramElementCode": ["2878", "8004"], "StartDate": "10/01/2013", "OrganizationStreet": "Research Administration Building", "EndDate": "09/30/2016", "PrincipalInvestigator": "Prashant Shenoy", "ARRAAmount": null, "OrganizationPhone": "4135450698", "AwardNumber": "1339839", "PIEmailAddress": "shenoy@cs.umass.edu", "NSFDirectorate": "CSE", "Co-PIName": "David Irwin", "LastAmendmentDate": "08/29/2013", "Organization": "University of Massachusetts Amherst", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "500000", "Abstract": "The emergence of large-scale Internet applications and services has driven a surge in research on cloud platforms, virtualization, data center architectures, and green computing. However, realistic performance evaluation of new research prototypes continues to be a major challenge. Home-grown performance evaluation tools that are often used by researchers are no longer able to capture the complexity of today&apos;s real systems and applications. To address this drawback, the project seeks to develop BenchLab, an open, flexible community infrastructure comprising applications, workloads and tools to enable realistic performance evaluation and benchmarking by systems researchers. BenchLab is an open framework where source code and workload datasets are freely available for modification and use by researchers for their specific experiments. The framework consists of a suite of server-side benchmark applications and workloads that represent cloud, mobile web, and green computing environments. BenchLab employs a modular, extensible architecture that is designed to support a range of server applications and workloads, with the ability to add support for newer applications and workloads and retire outdated ones. BenchLab is designed to be easy to use for experimental systems research &quot;at scale&quot; in commercial clouds, such as Amazon EC2, or virtualized clusters in laboratory settings. &lt;br/&gt; BenchLab will provide open tools and workloads for the research community to enable researchers to run larger and more realistic performance evaluation experiments that better emulate today&apos;s real-word systems. BenchLab will be incorporated into hands-on lab assignments to teach students the science and art of experimental performance evaluation.", "ProgramReferenceCode": ["7433", "8005"], "OrganizationZip": "010039242"}, {"OrganizationCity": "BOSTON", "OrganizationState": "MA", "Title": "SI2-SSE: Enhancement and Support of DMTCP for Adaptive, Extensible Checkpoint-Restart", "ProgramManager": "Daniel Katz", "State": "MA", "Program": ["SPECIAL PROJECTS - CCF", "Software Institutes"], "ProgramElementCode": ["2878", "8004"], "StartDate": "09/01/2014", "OrganizationStreet": "360 HUNTINGTON AVE", "EndDate": "08/31/2017", "PrincipalInvestigator": "Gene Cooperman", "ARRAAmount": null, "OrganizationPhone": "6173732508", "AwardNumber": "1440788", "PIEmailAddress": "gene@ccs.neu.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "11/17/2014", "Organization": "Northeastern University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "514427", "Abstract": "Society&apos;s increasingly complex cyberinfrastructure creates a concern for software robustness and reliability. Yet, this same complex infrastructure is threatening the continued use of fault tolerance. Consider when a single application or hardware device crashes. Today, in order to resume that application from the point where it crashed, one must also consider the complex subsystem to which it belongs. While in the past, many developers would write application-specific code to support fault tolerance for a single application, this strategy is no longer feasible when restarting the many inter-connected applications of a complex subsystem. This project will support a plugin architecture for transparent checkpoint-restart. Transparency implies that the software developer does not need to write any application-specific code. The plugin architecture implies that each software developer writes the necessary plugins only once. Each plugin takes responsibility for resuming any interrupted sessions for just one particular component. At a higher level, the checkpoint-restart system employs an ensemble of autonomous plugins operating on all of the applications of a complex subsystem, without any need for application-specific code.&lt;br/&gt;&lt;br/&gt;The plugin architecture is part of a more general approach called process virtualization, in which all subsystems external to a process are virtualized. It will be built on top of the DMTCP checkpoint-restart system. One simple example of process virtualization is virtualization of ids. A plugin maintains a virtualization table and arranges for the application code of the process to see only virtual ids, while the outside world sees the real id. Any system calls and library calls using this real id are extended to translate between real and virtual id. On restart, the real ids are updated with the latest value, and the process memory remains unmodified, since it contains only virtual ids. Other techniques employing process virtualization include shadow device drivers, record-replay logs, and protocol virtualization. Some targets of the research include transparent checkpoint-restart support for the InfiniBand network, for programmable GPUs (including shaders), for networks of virtual machines, for big data systems such as Hadoop, and for mobile computing platforms such as Android.", "ProgramReferenceCode": ["7433", "8004", "8005", "9251", "2878"], "OrganizationZip": "021155005"}, {"OrganizationCity": "Palo Alto", "OrganizationState": "CA", "Title": "Collaborative Research: SI2-SSI: A Sustainable Open Source Software Pipeline for Patient Specific Blood Flow Simulation and Analysis", "ProgramManager": "Daniel Katz", "State": "CA", "Program": ["Software Institutes", "CDS&E"], "ProgramElementCode": ["8004", "8084"], "StartDate": "07/01/2015", "OrganizationStreet": "3160 Porter Drive", "EndDate": "09/30/2017", "PrincipalInvestigator": "Alison Marsden", "ARRAAmount": null, "OrganizationPhone": "6507232300", "AwardNumber": "1562450", "PIEmailAddress": "amarsden@ucsd.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "09/21/2015", "Organization": "Stanford University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "812810", "Abstract": "The SimVascular package is a crucial research tool for cardiovascular modeling and simulation, and has contributed to numerous advances in personalized medicine, surgical planning and medical device design. SimVascular is currently the only comprehensive software package that provides a complete pipeline from medical image data segmentation to patient specific blood flow simulation and analysis. This software now forms the backbone in cardiovascular simulation research in a small but active group of domestic and international academic labs. However, since its original release there have been several critical barriers preventing wider adoption by new users, application to large-scale research studies, and educational access. These include 1) the cost and complications associated with embedded commercial components, 2) the need for more efficient geometric model construction tools, 3) lack of sustainable architecture and infrastructure, and 4) a lack of organized maintenance. &lt;br/&gt;&lt;br/&gt;This project is addressing the above roadblocks through the following aims: 1) create a sustainable and modular open source SimVascular 2.0 project housed at Stanford Simbios? simtk.org, with documentation, benchmarking and test suites, 2) provide alternatives to all commercial components in the first truly open source release of SimVascular, 3) improve the image segmentation methods and efficiency of model construction to enable high-throughput studies, and 4) enhance functionality by merging state of the art research in optimization, flow analysis, and multiscale modeling. The project leverages existing resources and infrastructure at simtk.org, and builds upon the significant previous investment that enabled the initial open source release of SimVascular. Access is further enhanced by cross-linking with the NIH funded Vascular Model Repository. This project will increase the user base and build a sustainable software platform supported by an active open source community. Releasing the first fully open source version of SimVascular will enable greater advances in cardiovascular medicine, provide open access to state of the art simulation tools for educational purposes, and facilitate training of young investigators. These efforts will also further promote diversity and attract students to science and engineering by leveraging this software to enable high school field trips to the UCSD StarCAVE to view simulation data using virtual reality.", "ProgramReferenceCode": ["7433", "8009", "8084"], "OrganizationZip": "943041212"}, {"OrganizationCity": "Davis", "OrganizationState": "CA", "Title": "Collaborative Research: SI2-SSE: Sage-Combinat: Developing and Sharing Open Source Software for Algebraic Combinatorics", "ProgramManager": "Daniel Katz", "State": "CA", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "OPERATIONS RESEARCH", "DYNAMICAL SYSTEMS", "Software Institutes", "CDS&E-MSS"], "ProgramElementCode": ["1253", "5514", "7478", "8004", "8069"], "StartDate": "06/01/2012", "OrganizationStreet": "OR/Sponsored Programs", "EndDate": "05/31/2016", "PrincipalInvestigator": "Anne Schilling", "ARRAAmount": null, "OrganizationPhone": "5307547700", "AwardNumber": "1147247", "PIEmailAddress": "anne@math.ucdavis.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "06/19/2012", "Organization": "University of California-Davis", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "216626", "Abstract": "Sage is an open source general purpose mathematical software system that has developed explosively within the last six years. Sage-Combinat is a subproject whose mission is &quot;to improve Sage as an extensible toolbox for computer exploration in (algebraic) combinatorics, and foster code sharing between researchers in this area&quot;. There is a long tradition of software packages for algebraic combinatorics. These have been crucial in the development of combinatorics since the 1960s. The originality of the Sage-Combinat project lies in successfully addressing the following simultaneous objectives. It offers a wide variety of interoperable and extensible tools, integrated in a general purpose mathematical software package, as needed for daily computer exploration in algebraic combinatorics; it is developed by a community of researchers spread around the world and across institutions; and it is open source and depends only on open source software. Among the proposers, Stein is founder and lead developer of Sage while Bump, Musiker, and Schilling are strong contributors to Sage-Combinat. Hivert and Thi&#180;ery (Paris-Sud, Orsay), founders and lead developers of Sage-Combinat, are both strongly affiliated with this project. Some of the research areas addressed in this project include symmetric functions, in particular Macdonald polynomials for arbitrary Cartan types and their nonsymmetric analogues, crystals, rigged configurations and combinatorial R-matrices, affine Weyl groups and Hecke algebras, cluster algebras, and posets.&lt;br/&gt;&lt;br/&gt;The project will develop Sage-Combinat in areas relevant to the ongoing research of the participants, together with relevant underlying infrastructure. The project will include three Sage Days workshops, and will be affiliated with a third scheduled workshop at ICERM. These workshops include a strong outreach component and have been a potent tool for connecting researchers and recruiting Sage users and developers. The grant will also fund a dedicated software development and computation server for Sage-Combinat, to be hosted in the Sage computation farm in Seattle. Emphasis will be placed on the development of thematic tutorials that will make the code accessible to new users. The proposal will also fund graduate student RA support, curriculum development, and other mentoring.", "ProgramReferenceCode": ["7433", "7683", "8005", "5514", "7478", "8004", "1253"], "OrganizationZip": "956186134"}, {"OrganizationCity": "TEMPE", "OrganizationState": "AZ", "Title": "Collaborative Research: SI2-SSE: Modules for Experiments in Stellar Astrophysics", "ProgramManager": "Daniel Katz", "State": "AZ", "Program": ["STELLAR ASTRONOMY & ASTROPHYSC", "OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes"], "ProgramElementCode": ["1215", "1253", "8004"], "StartDate": "01/01/2014", "OrganizationStreet": "ORSPA", "EndDate": "12/31/2016", "PrincipalInvestigator": "Francis Timmes", "ARRAAmount": null, "OrganizationPhone": "4809655479", "AwardNumber": "1339600", "PIEmailAddress": "fxt44@mac.com", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/29/2013", "Organization": "Arizona State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "306840", "Abstract": "As the most commonly observed objects, stars remain at the forefront of astrophysical research. Technical advances in detectors, computer processing power, networks and data storage have enabled new sky surveys. Many of these search for transient events at optical wavelengths, such as the Palomar Transient Factory and Pan-STARRS1 that probe ever-larger areas of the sky and ever-fainter sources, opening up the vast discovery space of &quot;time domain astronomy&quot;. The recent Kepler and COROT space missions achieved nearly continuous monitoring of more than 100,000 stars. The stellar discoveries from these surveys include revelations about stellar evolution, rare stars, unusual explosion outcomes, and remarkably complex binary star systems. The immediate future holds tremendous promise, as both the space-based survey Gaia and the ground based Large Synoptic Survey Telescope come to fruition. This tsunami of data has created a new demand for a reliable and publicly available research and education tool in computational stellar astrophysics that will reap the full scientific benefits of these discoveries while also creating a collaborative environment where theory, computation and interpretation can come together to address critical scientific issues. This demand by the stellar community led to our release of the Modules for Experiments in Stellar Astrophysics (MESA) software project in 2011. MESA has driven, and will continue to drive with support from this award, innovation in the stellar community as well as the exoplanet, galactic, and cosmological communities. Educators have widely deployed MESA in their undergraduate and graduate stellar evolution courses because MESA is a community platform with an active support network for leading-edge scientific investigations. Stellar astrophysics research, and all the communities that rely on stellar astrophysics, will be significantly enhanced by sustaining innovative development of MESA.&lt;br/&gt;&lt;br/&gt;This award supports the Modules for Experiments in Stellar Astrophysics (MESA) software project and user community. MESA solves the 1D fully coupled structure and composition equations governing stellar evolution. It is based on an implicit finite difference scheme with adaptive mesh refinement and sophisticated timestep controls; state-of-the-art modules provide equation of state, opacity, nuclear reaction rates, element diffusion, boundary conditions, and changes to the mass of the star. MESA is an open source library that employs contemporary numerical approaches, supports shared memory parallelism based on OpenMP, and is written with present and future multi-core and multi-thread architectures in mind. MESA combines the robust, efficient, thread-safe numerical and physics modules for simulations of a wide range of stellar evolution scenarios ranging from very-low mass to massive stars. Innovations in MESA and its domain of applicability continues to grow, just recently extended to include giant planets, oscillations, and rotation. This project will sustain MESA as a key piece of software infrastructure for stellar astrophysics while building new scientific and educational networks.", "ProgramReferenceCode": ["1206", "7433", "8005"], "OrganizationZip": "852816011"}, {"OrganizationCity": "SANTA BARBARA", "OrganizationState": "CA", "Title": "Collaborative Research: SI2-SSE: Modules for Experiments in Stellar Astrophysics", "ProgramManager": "Daniel Katz", "State": "CA", "Program": ["STELLAR ASTRONOMY & ASTROPHYSC", "OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes"], "ProgramElementCode": ["1215", "1253", "8004"], "StartDate": "01/01/2014", "OrganizationStreet": "Office of Research", "EndDate": "12/31/2016", "PrincipalInvestigator": "Lars Bildsten", "ARRAAmount": null, "OrganizationPhone": "8058934188", "AwardNumber": "1339581", "PIEmailAddress": "bildsten@itp.ucsb.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/29/2013", "Organization": "University of California-Santa Barbara", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "163586", "Abstract": "As the most commonly observed objects, stars remain at the forefront of astrophysical research. Technical advances in detectors, computer processing power, networks and data storage have enabled new sky surveys. Many of these search for transient events at optical wavelengths, such as the Palomar Transient Factory and Pan-STARRS1 that probe ever-larger areas of the sky and ever-fainter sources, opening up the vast discovery space of &quot;time domain astronomy&quot;. The recent Kepler and COROT space missions achieved nearly continuous monitoring of more than 100,000 stars. The stellar discoveries from these surveys include revelations about stellar evolution, rare stars, unusual explosion outcomes, and remarkably complex binary star systems. The immediate future holds tremendous promise, as both the space-based survey Gaia and the ground based Large Synoptic Survey Telescope come to fruition. This tsunami of data has created a new demand for a reliable and publicly available research and education tool in computational stellar astrophysics that will reap the full scientific benefits of these discoveries while also creating a collaborative environment where theory, computation and interpretation can come together to address critical scientific issues. This demand by the stellar community led to our release of the Modules for Experiments in Stellar Astrophysics (MESA) software project in 2011. MESA has driven, and will continue to drive with support from this award, innovation in the stellar community as well as the exoplanet, galactic, and cosmological communities. Educators have widely deployed MESA in their undergraduate and graduate stellar evolution courses because MESA is a community platform with an active support network for leading-edge scientific investigations. Stellar astrophysics research, and all the communities that rely on stellar astrophysics, will be significantly enhanced by sustaining innovative development of MESA.&lt;br/&gt;&lt;br/&gt;This award supports the Modules for Experiments in Stellar Astrophysics (MESA) software project and user community. MESA solves the 1D fully coupled structure and composition equations governing stellar evolution. It is based on an implicit finite difference scheme with adaptive mesh refinement and sophisticated timestep controls; state-of-the-art modules provide equation of state, opacity, nuclear reaction rates, element diffusion, boundary conditions, and changes to the mass of the star. MESA is an open source library that employs contemporary numerical approaches, supports shared memory parallelism based on OpenMP, and is written with present and future multi-core and multi-thread architectures in mind. MESA combines the robust, efficient, thread-safe numerical and physics modules for simulations of a wide range of stellar evolution scenarios ranging from very-low mass to massive stars. Innovations in MESA and its domain of applicability continues to grow, just recently extended to include giant planets, oscillations, and rotation. This project will sustain MESA as a key piece of software infrastructure for stellar astrophysics while building new scientific and educational networks.", "ProgramReferenceCode": ["1206", "7433", "8005"], "OrganizationZip": "931062050"}, {"OrganizationCity": "Tampa", "OrganizationState": "FL", "Title": "SI2-SSI: Collaborative Research: Building Sustainable Tools and Collaboration for Volcanic and Related Hazards", "ProgramManager": "Daniel Katz", "State": "FL", "Program": ["Software Institutes", "EarthCube"], "ProgramElementCode": ["8004", "8074"], "StartDate": "10/01/2013", "OrganizationStreet": "3702 Spectrum Blvd.", "EndDate": "09/30/2017", "PrincipalInvestigator": "Charles Connor", "ARRAAmount": null, "OrganizationPhone": "8139742897", "AwardNumber": "1339768", "PIEmailAddress": "cbconnor@usf.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "09/16/2013", "Organization": "University of South Florida", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "194869", "Abstract": "This project is focused on creating and upgrading software infrastructure for a large community of scientists engaged in volcanology research and associated hazard analysis. Specifically, the project will reengineer three widely used tools (TITAN2D - block and ash flows, TEPHRA and Puff - ash transport and dispersal) and develop support for workflows that use these tools to analyze risk from volcanic hazards. Reengineering will encompass modularization so researchers may easily experiment with different modeling approaches, incorporation of techniques to make the tools efficient on new computing architectures like GPUs and many-core chips. The workflows are intended to tackle the challenges of managing complex and often large data flows associated with these tools in validation processes and in probabilistic inference based on the outcomes of the modeling. The tools and workflows will be made available using the popular vhub.org platform. This project will help provide a standard well managed hardware/software platform and approaches to standardize the documentation associated with input data, source code, and output data. This will ensure that model calculations are reproducible.&lt;br/&gt;&lt;br/&gt;The wider use of these high fidelity tools and their use in mitigating hazards is likely to have a significant effect on hazard analysis and management. The project will also engage in several major workshops and in training activities. Project personnel will also engage in the Earthcube initiative - popularizing computational methodologies, online access and dissemination mechanisms through the VHub platform.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "336129446"}, {"OrganizationCity": "Seattle", "OrganizationState": "WA", "Title": "SI2-SSE: Algorithms and Tools for Data-Driven Executable Biology", "ProgramManager": "Daniel Katz", "State": "WA", "Program": ["ADVANCES IN BIO INFORMATICS", "Software Institutes"], "ProgramElementCode": ["1165", "8004"], "StartDate": "10/01/2015", "OrganizationStreet": "4333 Brooklyn Ave NE", "EndDate": "09/30/2018", "PrincipalInvestigator": "Rastislav Bodik", "ARRAAmount": null, "OrganizationPhone": "2065434043", "AwardNumber": "1535191", "PIEmailAddress": "bodik@uw.edu", "NSFDirectorate": "CSE", "Co-PIName": "Aditya Virendra Thakur", "LastAmendmentDate": "09/08/2015", "Organization": "University of Washington", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "499784", "Abstract": "This project seeks to understand the signaling mechanisms that control cellular activities such as cell division, cell growth, and cell differentiation. Errors in cellular signaling cause diseases such as cancer, autoimmunity, and diabetes. Accurate models of cellular signalling are thus necessary for rational drug design and other applications central to national health. This project focuses on inferring models from experimental data. Specifically, it is interested in models of protein signalling because proteins control and mediate the vast majority of biological processes in a living cell. The project follows of the approach of executable biology: models of cell signalling are computer programs, which allows executing the models on the computer and comparing the model behavior against the behavior of the living cell observed in the lab setting. Most importantly for this project, viewing models as programs will allow the team to harness the recent advances in automatic synthesis of computer programs for synthesis of models from experimental measurements of cells. &lt;br/&gt;&lt;br/&gt;The goal of the project is to provide biologists with a tool that synthesizes a variety of executable models from varied types of experimental data. To facilitate synthesis of mechanistic models from experimental data, the project will develop a family of modeling languages that will capture complex behaviors of biological systems, such as time and concurrency. The languages will be instances of the more general Boolean-Networks language. The team will investigate how to adjust the modeling abstraction based on the nature of available experimental data; the abstractions will be instantiated as suitably chosen languages from their language family. The modeling framework will be built by leveraging techniques from programming languages and formal methods such as meta-programming and constraint solving.", "ProgramReferenceCode": ["7433", "8005"], "OrganizationZip": "981950001"}, {"OrganizationCity": "Seattle", "OrganizationState": "WA", "Title": "Collaborative Research: SI2-SSI: Landlab: A Flexible, Open-Source Modeling Framework for Earth-Surface Dynamics", "ProgramManager": "Daniel Katz", "State": "WA", "Program": ["GEOMORPHOLOGY & LAND USE DYNAM", "Software Institutes", "EarthCube"], "ProgramElementCode": ["7458", "8004", "8074"], "StartDate": "08/01/2015", "OrganizationStreet": "4333 Brooklyn Ave NE", "EndDate": "07/31/2020", "PrincipalInvestigator": "Erkan Istanbulluoglu", "ARRAAmount": null, "OrganizationPhone": "2065434043", "AwardNumber": "1450412", "PIEmailAddress": "erkani@u.washington.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "07/14/2015", "Organization": "University of Washington", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "676836", "Abstract": "Earth scientists discover and predict the behavior of the natural world in part through the use of computer models. Models are used to study a wide range of phenomena, such as soil erosion, flooding, plant growth, landslide occurrence, and many other processes. Models can be used to develop scientific understanding by comparing model calculations with the real world. They can also be used to make predictions about how nature will behave under certain conditions. In the areas of geosciences that deal with the earth&apos;s surface, one bottleneck in the development and use of models is the effort required to build, test, and debug the necessary software. This project contributes to scientific research and discovery by creating open source software tools that scientists can use to more efficiently create, modify, or combine computer models that represent portions of earth&apos;s surface and the processes occurring thereon. The project combines software development with user training and web-based resources, so that the products will be openly accessible, well documented, and widely disseminated within the relevant scientific communities. The project also contributes to K-12, undergraduate, and graduate-level education in computational modeling and earth-surface processes.&lt;br/&gt;&lt;br/&gt;This project catalyzes research in earth-surface dynamics by developing a software framework that enables rapid creation, refinement, and reuse of two-dimensional (2D) numerical models. The phrase earth-surface dynamics refers to a remarkably diverse group of science and engineering fields that deal with our planet&apos;s surface and near-surface environment: its processes, its management, and its responses to natural and human-made perturbations. Scientists who want to use an earth-surface model often build their own unique model from the ground up, re-coding the basic building blocks of their model rather than taking advantage of codes that have already been written. Whereas the end result may be novel software programs, many person-hours are lost rewriting existing code, and the resulting software is often idiosyncratic, poorly documented, and unable to interact with other software programs in the same scientific community and beyond, leading to lost opportunities for exploring an even wider array of scientific questions than those that can be addressed using a single model. The Landlab model framework seeks to eliminate these redundancies and lost opportunities, and simultaneously lower the bar for entry into numerical modeling, by creating a user- and developer-friendly software library that provides scientists with the fundamental building blocks needed for modeling earth-surface dynamics. The framework takes advantage of the fact that nearly all surface-dynamics models share a set of common software elements, despite the wide range of processes and scales that they encompass. Providing these elements in the context of a popular scientific programming environment, with strong user support and community engagement, contributes to accelerating progress in the diverse sciences of the earth&apos;s surface.&lt;br/&gt;&lt;br/&gt;The Landlab modeling framework is designed so that grid creation, data storage, and sharing of data among process components is done for the user. The framework is generic enough so that the coupling of two process components, whether they are squarely within a geoscience subdiscipline, or they cross subdisciplines, is the same. This architecture makes it easy to explore a wide range of questions in the geosciences without the need for much coding. Further, the model code is primarily written in Python, a language that is relatively easy for casual programmers to learn. Because of these attributes, the Landlab modeling framework has the potential to add computational modeling to the toolbox of a wide array of geoscientists, and to clear a path for trans-disciplinary earth-surface modeling that explores societally relevant topics, such as land-cover changes in response to climate change, as well as modeling of topics that currently require the coupling of sophisticated but harder-to-use models, such as sediment source-to-sink dynamics. The project will generate several proof-of-concept studies that are interesting in their own right, and demonstrate the capabilities of the modeling framework. Community engagement is fostered by presenting clinics and demonstrations at professional venues aimed at hydrologists, sedimentologists, critical zone scientists, and the broader earth and environmental sciences community. The team also supports visits by individual scientists for on-site training beyond these clinics. Input/output tools allow compatibility with data from relevant NSF-supported research. The project supports four graduate students and three postdoctoral researchers, and also includes modeling workshops for fifth to seventh grade girls in a community that has a greater than 50% minority population.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "981950001"}, {"OrganizationCity": "Nashville", "OrganizationState": "TN", "Title": "Collaborative Research: SI2-SSI: Development of an Integrated Molecular Design Environment for Lubrication Systems (iMoDELS)", "ProgramManager": "Daniel Katz", "State": "TN", "Program": ["NANOSCALE: INTRDISCPL RESRCH T", "Software Institutes"], "ProgramElementCode": ["1674", "8004"], "StartDate": "10/01/2011", "OrganizationStreet": "Office of Sponsored Programs", "EndDate": "09/30/2016", "PrincipalInvestigator": "Peter Cummings", "ARRAAmount": null, "OrganizationPhone": "6158756070", "AwardNumber": "1047828", "PIEmailAddress": "peter.cummings@vanderbilt.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Gabor Karsai", "Akos Ledeczi", "Clare McCabe"], "LastAmendmentDate": "07/22/2014", "Organization": "Vanderbilt University", "AwardInstrument": "Continuing grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "2542681", "Abstract": "This project is focused on developing, deploying and distributing the Integrated Molecular Design Environment for Lubrication Systems (iMoDELS), an open-source simulation and design environment (SDE) that encapsulates the expertise of specialists in first principles, forcefields and molecular simulation related to nanoscale lubrication in a simple web-based interface. The iMoDELS SDE is being developed using model-integrated computing (MIC), a state-of-the-art powerful, well-established, extensible, community-supported, and application-hardened software engineering framework that supports scientific and engineering workflows. Making iMoDELS broadly accessible is motivated by the high cost (over $800B/yr in the US) of friction and wear, which, along with the methodology to overcome them, lubrication, are collectively known as tribology. Tribology involves molecular mechanisms occurring on a nanometer scale, and hence understanding tribological behavior on this scale is critical to developing new technologies for reducing wear due to friction. Deployment of iMoDELS will enable non-computational specialists to be able to evaluate, design and optimize nanoscale lubrication systems, such as hard disk drives, NEMS (nanoelectromechanical systems) and MEMS (microelectromechanical systems), and experiments involving rheological measurements via atomic force microscopes (AFMs) and surface force apparatuses (SFAs).&lt;br/&gt;&lt;br/&gt;The iMoDELS SDE brings together a unique combination of materials and computer scientists who will combine their skills to abstract the deep human expertise currently required for the development of simulation-based experiments, thus making broadly available easy-to-use tools to an empirically driven area of science and engineering (nanotribology) of rapidly growing technological importance. iMoDELS includes the creation and open dissemination of forcefield and simulation results databases that will benefit the simulation community worldwide and catalyze broad-based activity in this area. The proposed research also includes the interdisciplinary training of undergraduate and graduate students, as well as postdoctoral researchers at the interface of tribology, computational materials sciences, and computer science. The PIs will use the iMoDELS SDE, and results from it, in presentations used in outreach to local area high school and in classes given to undergraduate students. The iMoDELS SDE will be vigorously promoted through workshops and presentations at national conferences, and via the dedicated website for development and dissemination.", "ProgramReferenceCode": ["9150", "1674", "8004"], "OrganizationZip": "372122809"}, {"OrganizationCity": "La Jolla", "OrganizationState": "CA", "Title": "Collaborative Research: SI2-SSI: Open Gateway Computing Environments Science Gateways Platform as a Service (OGCE SciGaP)", "ProgramManager": "Daniel Katz", "State": "CA", "Program": ["CROSS-EF ACTIVITIES", "Software Institutes"], "ProgramElementCode": ["7275", "8004"], "StartDate": "10/01/2013", "OrganizationStreet": "Office of Contract & Grant Admin", "EndDate": "09/30/2018", "PrincipalInvestigator": "Mark Miller", "ARRAAmount": null, "OrganizationPhone": "8585344896", "AwardNumber": "1339856", "PIEmailAddress": "mmiller@sdsc.edu", "NSFDirectorate": "CSE", "Co-PIName": "Amitava Majumdar", "LastAmendmentDate": "08/29/2013", "Organization": "University of California-San Diego", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1742099", "Abstract": "Science Gateways are virtual environments that dramatically accelerate scientific discovery by enabling scientific communities to utilize distributed computational and data resources (that is, cyberinfrastructure). Successful Science Gateways provide access to sophisticated and powerful resources, while shielding their users from the resources&apos; complexities. Given Science Gateways&apos; demonstrated impact on progress in many scientific fields, it is important to remove barriers to the creation of new gateways and make it easier to sustain them. The Science Gateway Platform (SciGaP) project will create a set of hosted infrastructure services that can be easily adopted by gateway providers to build new gateways based on robust and reliable open source tools. The proposed work will transform the way Science Gateways are constructed by significantly lowering the development overhead for communities requiring access to cyberinfrastructure, and support the efficient utilization of shared resources.&lt;br/&gt;&lt;br/&gt;SciGaP will transform access to large scale computing and data resources by reducing development time of new gateways and by accelerating scientific research for communities in need of access to large-scale resources. SciGaP&apos;s adherence to open community and open governance principles of the Apache Software Foundation will assure open source software access and open operation of its services. This will give all project stakeholders a voice in the software and will clear the proprietary fog that surrounds cyberinfrastructure services. The benefits of SciGaP services are not restricted to scientific fields, but can be used to accelerate progress in any field of endeavor that is limited by access to computational resources. SciGaP services will be usable by a community of any size, whether it is an individual, a lab group, a department, an institution, or an international community. SciGaP will help train a new generation of cyberinfrastructure developers in open source development, providing these early career developers with the ability to make publicly documented contributions to gateway software and to bridge the gap between academic and non-academic development.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "920930934"}, {"OrganizationCity": "Princeton", "OrganizationState": "NJ", "Title": "SI2-SSI: Accelerating the Pace of Research through Implicitly Parallel Programming", "ProgramManager": "Sol J. Greenspan", "State": "NJ", "Program": ["INFORMATION TECHNOLOGY RESEARC", "SPECIAL PROJECTS - CCF", "Software Institutes"], "ProgramElementCode": ["1640", "2878", "8004"], "StartDate": "10/01/2010", "OrganizationStreet": "Off. of Research & Proj. Admin.", "EndDate": "03/31/2016", "PrincipalInvestigator": "David August", "ARRAAmount": null, "OrganizationPhone": "6092583090", "AwardNumber": "1047879", "PIEmailAddress": "august@cs.princeton.edu", "NSFDirectorate": "CSE", "Co-PIName": "David Walker", "LastAmendmentDate": "09/13/2010", "Organization": "Princeton University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1740214", "Abstract": "Today, two trends conspire to slow down the pace of science, engineering, and academic research progress in general. First, researchers increasingly rely on computation to process ever larger data sets and to perform ever more computationally-intensive simulations. Second, individual processor speeds are no longer increasing with every computer chip generation as they once were. To compensate, processor manufacturers have moved to including more processors, or cores, on a chip with each generation. To obtain peak performance on these multicore chips, software must be implemented so that it can execute in parallel and thereby use the additional processor cores. Unfortunately, writing efficient, explicitly parallel software programs using today&apos;s software-development tools takes advanced training in computer science, and even with such training, the task remains extremely difficult, error-prone, and time consuming. This project will create a new high-level programming platform, called Implicit Parallel Programming (IPP), designed to bring the performance promises of modern multicore machines to scientists and engineers without the costs associated with having to teach these users how to write explicitly parallel programs. In the short term, this research will provide direct and immediate benefit to researchers in several areas of science as the PIs will pair computer science graduate students with non-computer science graduate students to study, analyze, and develop high-value scientific applications. In the long term, this research has the potential to fundamentally change the way scientists obtain performance from parallel machines, improve their productivity, and accelerate the overall pace of science. This work will also have major educational impact by developing courseware and tutorial materials, useable by all scientists and engineers, on the topics of explicit and implicit parallel computing.&lt;br/&gt;&lt;br/&gt;IPP will operate by allowing users to write ordinary sequential programs and then to augment them with logical specifications that expand (or abstract) the set of sequential program behaviors. This capacity for abstraction will provide parallelizing compilers with the flexibility to more aggressively optimize programs than would otherwise be possible. In fact, it will enable effective parallelization techniques where they were impossible before. The language design and compiler implementation will be accompanied by formal semantic analysis that will be used to judge the correctness of compiler transformations, provide a foundation for about reasoning programs, and guide the creation of static analysis and program defect detection algorithms. Moreover since existing programs and languages can be viewed as (degenerately) implicitly parallel, decades of investment in human expertise, languages, compilers, methods, tools, and applications is preserved. In particular, it will be possible to upgrade old legacy programs or libraries from slow sequential versions without overhauling the entire system architecture, but merely by adding a few auxiliary specifications. Compiler technology will help guide scientists and engineers through this process, further simplifying the task. Conceptually, IPP restores an important layer of abstraction, freeing programmers to write high-level code, designed to be easy to understand, rather than low-level code, architected according to the specific demands of a particular parallel machine.", "ProgramReferenceCode": ["1640", "2878"], "OrganizationZip": "085442020"}, {"OrganizationCity": "MADISON", "OrganizationState": "WI", "Title": "SI2-SSI: Collaborative Research: A Computational Materials Data and Design Environment", "ProgramManager": "Daniel Katz", "State": "WI", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "DMR SHORT TERM SUPPORT", "CHEMISTRY PROJECTS", "Software Institutes"], "ProgramElementCode": ["1253", "1712", "1991", "8004"], "StartDate": "10/01/2012", "OrganizationStreet": "21 North Park Street", "EndDate": "09/30/2017", "PrincipalInvestigator": "Dane Morgan", "ARRAAmount": null, "OrganizationPhone": "6082623822", "AwardNumber": "1148011", "PIEmailAddress": "ddmorgan@wisc.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "09/20/2012", "Organization": "University of Wisconsin-Madison", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1050000", "Abstract": "TECHNICAL SUMMARY&lt;br/&gt;The Office of Cyberinfrastructure, Division of Materials Research, and Chemistry Division contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation. This award supports development of new theory and tools to enable rapid and efficient calculation of atomic level material properties. The incredible advances in computing power and tools of atomic scale simulation have now made it possible to predict critical properties for existing and new materials without experimental input. However, present simulation approaches typically require researchers to perform many steps by hand, which is both slow and error prone compared to what a computer can do. Through computer codes that automate the tasks in first principles modeling human bottlenecks can be removed and predictive capabilities of first principles simulation techniques can be accelerated by orders of magnitude. Such a high-throughput computing approach will enable generation of critical materials data on an unprecedented scale and open new doors for material science.&lt;br/&gt;&lt;br/&gt;The team will develop tools for the specific challenges of predicting point defect properties, atomic diffusion, and surface stability, with a focus on automating steps to enable computations on a massive scale. The PIs will use state-of-the-art first principles quantum mechanical methods. Best practices for treating the multiple issues of charged defect calculations, for example convergence with cell size and band gap errors, will be refined and automated for rapid execution. Similarly, tools to identify diffusion pathways and determine their barriers will be streamlined to allow users to quickly identify transport properties of new systems. New theoretical approaches to modeling charged surfaces will be developed to enable simulation of surfaces in more realistic environments. This award will support prediction of properties that play a critical role in advancing a wide range of technologies, from improving semiconductors for next generation computers to better fuel cells for more efficient energy conversion. Software tools and data produced by this effort will enable researchers to predict properties for thousands of materials with almost no human effort, accelerating the pace at which researchers can develop new materials technologies.&lt;br/&gt;&lt;br/&gt;Software and data developed from this award will be shared with academic and industrial researchers through modules on the web, scientific journals and presentations at national and international conferences. This award supports two workshops to educate researchers about the latest opportunities to use high-throughput computing of atomic scale properties for materials development. Students will be trained to work at the critical interface of the computer and physical sciences, supporting a generation of scientists who use modern computers to their fullest potential to develop new understanding and technology.&lt;br/&gt;&lt;br/&gt;NON-TECHNICAL SUMMARY&lt;br/&gt;The Office of Cyberinfrastructure, Division of Materials Research, and Chemistry Division contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation. This award supports development of new theory and tools to enable rapid and efficient calculation of atomic level material properties. The incredible advances in computing power and tools of atomic scale simulation have now made it possible to predict critical properties for existing and new materials without experimental input. However, present simulation approaches typically require researchers to perform many steps by hand, which is both slow and error prone compared to what a computer can do. Through computer codes that automate the tasks in first-principles modeling human bottlenecks can be removed and predictive capabilities of first principles simulation techniques can be accelerated by orders of magnitude. Such a high-throughput computing approach will enable generation of critical materials data on an unprecedented scale and open new doors for material science.&lt;br/&gt;&lt;br/&gt;The team will develop tools for the specific challenges of predicting point defect properties, atomic diffusion, and surface stability, with a focus on automating steps to enable computations on a massive scale. These properties play a critical role in advancing a wide range of technologies, from improving semiconductors for next generation computers to better fuel cells for more efficient energy conversion. Software tools and data produced by this effort will enable researchers to predict properties for thousands of materials with almost no human effort, accelerating the pace at which researchers can develop new materials technologies.&lt;br/&gt;&lt;br/&gt;Software and data developed from this award will be shared with academic and industrial researchers through modules on the web, scientific journals and presentations at national and international conferences. In particular, this award will support two workshops to educate researchers about the latest opportunities to use high-throughput computing of atomic scale properties for materials development. This award will train students to work at the critical interface of the computer and physical sciences, supporting a generation of scientists who use modern computers to their fullest potential to develop new understanding and technology.", "ProgramReferenceCode": ["1982", "7237", "7433", "7569", "7644", "8009", "9216", "9263", "1253", "1712", "1991", "8004"], "OrganizationZip": "537151218"}, {"OrganizationCity": "Austin", "OrganizationState": "TX", "Title": "Collaborative Research: SI2-SSI: A Comprehensive Ray Tracing Framework for Visualization in Distributed-Memory Parallel Environments", "ProgramManager": "Daniel Katz", "State": "TX", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "10/01/2013", "OrganizationStreet": "101 E. 27th Street, Suite 5.300", "EndDate": "09/30/2016", "PrincipalInvestigator": "Paul Navratil", "ARRAAmount": null, "OrganizationPhone": "5124716424", "AwardNumber": "1339863", "PIEmailAddress": "pnav@tacc.utexas.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/06/2014", "Organization": "University of Texas at Austin", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1198122", "Abstract": "Scientific visualization plays a large role in exploring the scientific simulations that run on supercomputers; new discoveries are often made by studying renderings generated through visualization of simulation results. The standard technique for rendering geometry is rasterization and the most commonly used library for performing this is OpenGL. Many visualization programs (VisIt, Ensight, VAPOR, ParaView, VTK) use OpenGL for rendering. However, recent architectural changes on supercomputers create significant opportunities for alternate rendering techniques. The computational power available on emerging many-core architectures, such as the Intel Xeon Phi processors on TACC?s Stampede system, enable ray-tracing, a higher quality technique. Further, as the amount of geometry per node rises, ray-tracing becomes increasingly cost effective, since its computational costs are proportional to the screen size, not the geometry size. Finally, the software implementation for OpenGL can not be easily mapped to non-GPU multi-core and many-core systems, creating a significant gap; if not closed, visualization will not be possible directly on large supercomputers. This confluence of new, more capable architectures, the increase in geometry per node, and concerns about the durability of the established rendering path all motivate this work. &lt;br/&gt;&lt;br/&gt;To address these trends, this research uses a two-pronged approach. First, the research will replace the OpenGL pathways that are commonly used for visualization with a high-performance, open-source ray tracing engine that can interactively render on both a CPU and on accelerator architectures. This new library will support the OpenGL API and will be usable immediately by any OpenGL-based visualization package without additional code modi&amp;#64257;cation. Second, this research will provide a direct interface to a high-performance distributed ray tracing engine so that applications can take advantage of ray tracing capabilities not easily exposed through the standard OpenGL interface, such as participating media and global illumination simulation. These features will enable the open science community to easily create photo-realistic imagery with natural lighting cues to aid in analysis and discovery. It will further expand the capacity of existing cyberinfrastructure to provide interactive visualization on standard HPC resources. &lt;br/&gt;&lt;br/&gt;This work has the potential to revolutionize in situ visualization capabilities by unifying the (potentially hybrid) architecture that efficiently run both simulation and visualization. Communicating with underrepresented groups will be a major component of outreach efforts through the PCARP, MITE and Women in Engineering programs. In addition, the project team will disseminate this work to the general public through NSF XD program, the VisIt visualization toolkit and by exhibiting at forums such as IEEE Visualization, IEEE High Performance Graphics and ACM Supercomputing.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "787121532"}, {"OrganizationCity": "Austin", "OrganizationState": "TX", "Title": "Collaborative Research: SI2-SSE: UT Wrangler: Understanding the Software Needs of High End Computer Users", "ProgramManager": "Daniel Katz", "State": "TX", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "10/01/2013", "OrganizationStreet": "101 E. 27th Street, Suite 5.300", "EndDate": "09/30/2016", "PrincipalInvestigator": "Robert McLay", "ARRAAmount": null, "OrganizationPhone": "5124716424", "AwardNumber": "1339708", "PIEmailAddress": "mclay@cfdlab.ae.utexas.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "09/13/2013", "Organization": "University of Texas at Austin", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "233046", "Abstract": "This research addresses two important questions: what software do researchers actually use on high-end computers, and how successful are they in their efforts to use it? It is a plan to improve our understanding of individual users&apos; software needs, then leverage that understanding to help stakeholders conduct business in a more efficient, effective, and systematic way. The signature product, UTWrangler, builds on work that is already improving the user experience and enhancing support programs for thousands of users on twelve supercomputers across the United States and Europe. For the first time, complete, accurate, detailed, and continuous ground truth information about software needs, trends, and issues at the level of the individual job are being delivered.&lt;br/&gt;&lt;br/&gt;UTWrangler will instrument, monitor, and analyze individual jobs on high-end computers to generate a picture of the compilers, libraries, and other software that users need to run their jobs successfully. It will highlight the products our researchers need and do not need, and alert users and support staff to the root causes of software configuration issues as soon as the problems occur. UTWrangler&apos;s prototypes prove its value and future impact: simplifying end users&apos; workflows; improving support, training and documentation; saving money; and helping administrators prioritize maintenance of their large base of installed software. UTWrangler will build on the capabilities of its prototypes, providing a robust, sustainable, second generation mechanism that will help the computational research community make the most effective use of limited computing cycles and labor hours. And UTWrangler will mitigate the difficulties new users encounter, reporting configuration problems as soon as jobs begin, and identifying opportunities to improve documentation, education and outreach programs.", "ProgramReferenceCode": ["7433", "8005"], "OrganizationZip": "787121532"}, {"OrganizationCity": "Austin", "OrganizationState": "TX", "Title": "Collaborative Research: SI2-SSI: A Comprehensive Performance Tuning Framework for the MPI Stack", "ProgramManager": "Daniel Katz", "State": "TX", "Program": ["INFORMATION TECHNOLOGY RESEARC", "Software Institutes"], "ProgramElementCode": ["1640", "8004"], "StartDate": "06/01/2012", "OrganizationStreet": "101 E. 27th Street, Suite 5.300", "EndDate": "05/31/2016", "PrincipalInvestigator": "William Barth", "ARRAAmount": null, "OrganizationPhone": "5124716424", "AwardNumber": "1148424", "PIEmailAddress": "bbarth@tacc.utexas.edu", "NSFDirectorate": "CSE", "Co-PIName": "Tommy Minyard", "LastAmendmentDate": "06/04/2012", "Organization": "University of Texas at Austin", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "449995", "Abstract": "The Message Passing Interface (MPI) is a very widely used parallel programming model on modern High-End Computing (HEC) systems. Many performance aspects of MPI libraries, such as latency, bandwidth, scalability, memory footprint, cache pollution, overlap of computation and communication etc. are highly dependent on system configuration and application requirements. Additionally, modern clusters are changing rapidly with the growth of multi-core processors and commodity networking technologies such as InfiniBand and 10GigE/iWARP. They are becoming diverse and heterogeneous with varying number of processor cores, processor speed, memory speed, multi-generation network adapters/switches, I/O interface technologies, and accelerators (GPGPUs), etc. Typically, any MPI library deals with the above kind of diversity in platforms and sensitivity of applications by employing various runtime parameters. These parameters are tuned during its release, or by&lt;br/&gt;system administrators, or by end-users. These default parameters may or may not be optimal for all system configurations and applications.&lt;br/&gt;&lt;br/&gt;The MPI library of a typical proprietary system goes through heavy performance tuning for a range of applications. Since commodity clusters provide greater flexibility in their configurations (processor, memory and network), it is very hard to achieve optimal tuning using released version of any MPI library, with its default settings. This leads to the following broad challenge: &quot;Can a comprehensive performance tuning framework be designed for MPI library so that the next generation InfiniBand, 10GigE/iWARP and RoCE clusters and applications will be able to extract `bare-metal&apos; performance and maximum scalability?&quot; The investigators, involving computer&lt;br/&gt;scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) as well as computational scientists from the Texas Advanced Computing Center (TACC) and San Diego Supercomputer Center (SDSC), University of California San Diego (UCSD), will be addressing the above challenge with innovative solutions.&lt;br/&gt;&lt;br/&gt;The investigators will specifically address the following challenges: 1) Can a set of static tools be designed to optimize performance of an MPI library during installation time? 2) Can a set of dynamic tools with low overhead be designed to optimize performance on a per-user and per-application basis during production runs? 3) How to incorporate the proposed performance tuning framework with the upcoming MPIT interface? 4) How to configure MPI libraries on a given system to deliver different optimizations to a set of driving applications? and 5) What kind of benefits (in terms of performance, scalability, memory efficiency and reduction in cache pollution) can be achieved by the proposed tuning framework? The research will be driven by a set of applications from established NSF computational science researchers running large scale simulations on the TACC Ranger and other systems at OSC, SDSC and OSU. The proposed designs will be integrated into the open-source MVAPICH2 library.", "ProgramReferenceCode": ["7433", "8009", "1640", "8004"], "OrganizationZip": "787121532"}, {"OrganizationCity": "Austin", "OrganizationState": "TX", "Title": "Collaborative Research: SI2-SSI: A Linear Algebra Software Infrastructure for Sustained Innovation in Computational Chemistry and other Sciences", "ProgramManager": "Evelyn M. Goldfield", "State": "TX", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "INFORMATION TECHNOLOGY RESEARC", "DMR SHORT TERM SUPPORT", "CHEMISTRY PROJECTS", "Software Institutes"], "ProgramElementCode": ["1253", "1640", "1712", "1991", "8004"], "StartDate": "06/01/2012", "OrganizationStreet": "101 E. 27th Street, Suite 5.300", "EndDate": "05/31/2016", "PrincipalInvestigator": "Robert van de Geijn", "ARRAAmount": null, "OrganizationPhone": "5124716424", "AwardNumber": "1148125", "PIEmailAddress": "rvdg@cs.utexas.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Don Batory", "Margaret Myers", "Victor Eijkhout", "John Stanton"], "LastAmendmentDate": "05/17/2013", "Organization": "University of Texas at Austin", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1701189", "Abstract": "Linear algebra is a branch of mathematics that provides the foundation for a significant fraction of computations in science and engineering. Historically, the importance of linear algebra is such that highly specialized codes written by computer scientists have been used by the community of scientific programmers as a vital part of their application programs. With the rapid changes in computer architecture during the last several years, it would seem that corresponding modifications in linear algebra routines would be warranted. However, such progress is not in evidence; the development of such routines has been just incremental, involving successive rewrites of routines that had their genesis in the last quarter of the last century. Correspondingly, there is something&lt;br/&gt;of a disconnect between the current `state-of-the-art&apos; linear algebra libraries, modern computer architectures, and applications that utilize the libraries.&lt;br/&gt;&lt;br/&gt;The new project will create a new, vertically integrated framework and implementation that revisits every layer of software, from low-level kernels to higher level functionality. The vertical integration is completed with a new generation of software for computational&lt;br/&gt;chemistry applications, guaranteeing that the developed software, to be freely available to the public, supports sustained innovation in that domain and other sciences. The development builds on the FLAME project, which has been funded by NSF and industry for more than a decade.", "ProgramReferenceCode": ["7433", "7569", "8009", "9216", "9263", "1253", "1640", "1712", "1991", "8004"], "OrganizationZip": "787121532"}, {"OrganizationCity": "SAN ANTONIO", "OrganizationState": "TX", "Title": "Collaborative Research: SI2-SSI: Open Gateway Computing Environments Science Gateways Platform as a Service (OGCE SciGaP)", "ProgramManager": "Daniel Katz", "State": "TX", "Program": ["CROSS-EF ACTIVITIES", "Software Institutes"], "ProgramElementCode": ["7275", "8004"], "StartDate": "10/01/2013", "OrganizationStreet": "7703 FLOYD CURL DR", "EndDate": "09/30/2018", "PrincipalInvestigator": "Borries Demeler", "ARRAAmount": null, "OrganizationPhone": "2105672340", "AwardNumber": "1339649", "PIEmailAddress": "demeler@biochem.uthscsa.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/29/2013", "Organization": "University of Texas Health Science Center San Antonio", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "600065", "Abstract": "Science Gateways are virtual environments that dramatically accelerate scientific discovery by enabling scientific communities to utilize distributed computational and data resources (that is, cyberinfrastructure). Successful Science Gateways provide access to sophisticated and powerful resources, while shielding their users from the resources&apos; complexities. Given Science Gateways&apos; demonstrated impact on progress in many scientific fields, it is important to remove barriers to the creation of new gateways and make it easier to sustain them. The Science Gateway Platform (SciGaP) project will create a set of hosted infrastructure services that can be easily adopted by gateway providers to build new gateways based on robust and reliable open source tools. The proposed work will transform the way Science Gateways are constructed by significantly lowering the development overhead for communities requiring access to cyberinfrastructure, and support the efficient utilization of shared resources.&lt;br/&gt;&lt;br/&gt;SciGaP will transform access to large scale computing and data resources by reducing development time of new gateways and by accelerating scientific research for communities in need of access to large-scale resources. SciGaP&apos;s adherence to open community and open governance principles of the Apache Software Foundation will assure open source software access and open operation of its services. This will give all project stakeholders a voice in the software and will clear the proprietary fog that surrounds cyberinfrastructure services. The benefits of SciGaP services are not restricted to scientific fields, but can be used to accelerate progress in any field of endeavor that is limited by access to computational resources. SciGaP services will be usable by a community of any size, whether it is an individual, a lab group, a department, an institution, or an international community. SciGaP will help train a new generation of cyberinfrastructure developers in open source development, providing these early career developers with the ability to make publicly documented contributions to gateway software and to bridge the gap between academic and non-academic development.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "782293901"}, {"OrganizationCity": "Bloomington", "OrganizationState": "IN", "Title": "Collaborative Research: SI2-SSI: Open Gateway Computing Environments Science Gateways Platform as a Service (OGCE SciGaP)", "ProgramManager": "Daniel Katz", "State": "IN", "Program": ["CROSS-EF ACTIVITIES", "Software Institutes"], "ProgramElementCode": ["7275", "8004"], "StartDate": "10/01/2013", "OrganizationStreet": "509 E 3RD ST", "EndDate": "09/30/2018", "PrincipalInvestigator": "Marlon Pierce", "ARRAAmount": null, "OrganizationPhone": "8128550516", "AwardNumber": "1339774", "PIEmailAddress": "marpierc@indiana.edu", "NSFDirectorate": "CSE", "Co-PIName": "Suresh Marru", "LastAmendmentDate": "08/29/2013", "Organization": "Indiana University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "2500000", "Abstract": "Science Gateways are virtual environments that dramatically accelerate scientific discovery by enabling scientific communities to utilize distributed computational and data resources (that is, cyberinfrastructure). Successful Science Gateways provide access to sophisticated and powerful resources, while shielding their users from the resources&apos; complexities. Given Science Gateways&apos; demonstrated impact on progress in many scientific fields, it is important to remove barriers to the creation of new gateways and make it easier to sustain them. The Science Gateway Platform (SciGaP) project will create a set of hosted infrastructure services that can be easily adopted by gateway providers to build new gateways based on robust and reliable open source tools. The proposed work will transform the way Science Gateways are constructed by significantly lowering the development overhead for communities requiring access to cyberinfrastructure, and support the efficient utilization of shared resources.&lt;br/&gt;&lt;br/&gt;SciGaP will transform access to large scale computing and data resources by reducing development time of new gateways and by accelerating scientific research for communities in need of access to large-scale resources. SciGaP&apos;s adherence to open community and open governance principles of the Apache Software Foundation will assure open source software access and open operation of its services. This will give all project stakeholders a voice in the software and will clear the proprietary fog that surrounds cyberinfrastructure services. The benefits of SciGaP services are not restricted to scientific fields, but can be used to accelerate progress in any field of endeavor that is limited by access to computational resources. SciGaP services will be usable by a community of any size, whether it is an individual, a lab group, a department, an institution, or an international community. SciGaP will help train a new generation of cyberinfrastructure developers in open source development, providing these early career developers with the ability to make publicly documented contributions to gateway software and to bridge the gap between academic and non-academic development.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "474013654"}, {"OrganizationCity": "SALT LAKE CITY", "OrganizationState": "UT", "Title": "Collaborative Research: SI2-SSI: A Comprehensive Ray Tracing Framework for Visualization in Distributed-Memory Parallel Environments", "ProgramManager": "Daniel Katz", "State": "UT", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "10/01/2013", "OrganizationStreet": "75 S 2000 E", "EndDate": "09/30/2016", "PrincipalInvestigator": "Charles Hansen", "ARRAAmount": null, "OrganizationPhone": "8015816903", "AwardNumber": "1339881", "PIEmailAddress": "hansen@cs.utah.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "09/13/2013", "Organization": "University of Utah", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "282225", "Abstract": "Scientific visualization plays a large role in exploring the scientific simulations that run on supercomputers; new discoveries are often made by studying renderings generated through visualization of simulation results. The standard technique for rendering geometry is rasterization and the most commonly used library for performing this is OpenGL. Many visualization programs (VisIt, Ensight, VAPOR, ParaView, VTK) use OpenGL for rendering. However, recent architectural changes on supercomputers create significant opportunities for alternate rendering techniques. The computational power available on emerging many-core architectures, such as the Intel Xeon Phi processors on TACC?s Stampede system, enable ray-tracing, a higher quality technique. Further, as the amount of geometry per node rises, ray-tracing becomes increasingly cost effective, since its computational costs are proportional to the screen size, not the geometry size. Finally, the software implementation for OpenGL can not be easily mapped to non-GPU multi-core and many-core systems, creating a significant gap; if not closed, visualization will not be possible directly on large supercomputers. This confluence of new, more capable architectures, the increase in geometry per node, and concerns about the durability of the established rendering path all motivate this work. &lt;br/&gt;&lt;br/&gt;To address these trends, this research uses a two-pronged approach. First, the research will replace the OpenGL pathways that are commonly used for visualization with a high-performance, open-source ray tracing engine that can interactively render on both a CPU and on accelerator architectures. This new library will support the OpenGL API and will be usable immediately by any OpenGL-based visualization package without additional code modi&amp;#64257;cation. Second, this research will provide a direct interface to a high-performance distributed ray tracing engine so that applications can take advantage of ray tracing capabilities not easily exposed through the standard OpenGL interface, such as participating media and global illumination simulation. These features will enable the open science community to easily create photo-realistic imagery with natural lighting cues to aid in analysis and discovery. It will further expand the capacity of existing cyberinfrastructure to provide interactive visualization on standard HPC resources. &lt;br/&gt;&lt;br/&gt;This work has the potential to revolutionize in situ visualization capabilities by unifying the (potentially hybrid) architecture that efficiently run both simulation and visualization. Communicating with underrepresented groups will be a major component of outreach efforts through the PCARP, MITE and Women in Engineering programs. In addition, the project team will disseminate this work to the general public through NSF XD program, the VisIt visualization toolkit and by exhibiting at forums such as IEEE Visualization, IEEE High Performance Graphics and ACM Supercomputing.", "ProgramReferenceCode": ["7433", "8009", "9150"], "OrganizationZip": "841128930"}, {"OrganizationCity": "GAINESVILLE", "OrganizationState": "FL", "Title": "SI2-SSE: Human- and Machine-Intelligent Software Elements for Cost-Effective Scientific Data Digitization", "ProgramManager": "Daniel Katz", "State": "FL", "Program": ["ADVANCES IN BIO INFORMATICS", "Software Institutes"], "ProgramElementCode": ["1165", "8004"], "StartDate": "08/01/2015", "OrganizationStreet": "1 UNIVERSITY OF FLORIDA", "EndDate": "07/31/2018", "PrincipalInvestigator": ["Jose Fortes", "Andrea Matsunaga"], "ARRAAmount": null, "OrganizationPhone": "3523923516", "AwardNumber": "1535086", "PIEmailAddress": ["fortes@ufl.edu", "ammatsun@ufl.edu"], "NSFDirectorate": "CSE", "Co-PIName": "Mauricio Tsugawa", "LastAmendmentDate": "09/17/2015", "Organization": "University of Florida", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "488048", "Abstract": "In the era of data-intensive scientific discovery, Big Data scientists in all communities spend the majority of their time and effort collecting, integrating, curating, transforming, and assessing quality before actually performing discovery analysis. Some endeavors may even start from information not being available and accessible in digital form, and when it is available, it is often in non-structured form, not compatible with analytics tools that require structured and uniformly-formatted data. Two main methods to deal with the volume and variety of data as well as to accelerate the rate of digitization have been to apply crowdsourcing or machine-learning solutions. However, very little has been done to simultaneously take advantage of both types of solutions, and to make it easier for different efforts to share and reuse developed software elements. The vision of the Human- and Machine-Intelligent Network (HuMaIN) project is to accelerate scientific data digitization through fundamental advances in the integration and mutual cooperation between human and machine processing in order to handle practical hurdles and bottlenecks present in scientific data digitization. Even though HuMaIN concentrates on digitization tasks faced by the biodiversity community, the software elements being developed are generic in nature, and expected to be applicable to other scientific domains (e.g., exploring the surface of the moon for craters require the same type of crowdsourcing tool as finding words in text, and the same questions of whether machine-learning tools could provide similar results can be tested).&lt;br/&gt;&lt;br/&gt;The HuMaIN project proposes to conduct research and develop the following software elements: (a) configurable Machine-Learning applications for scientific data digitization (e.g., Optical Character Recognition and Natural Language Processing), which will be made automatically available as RESTful services for increasing the ability of HuMaIN software elements to interoperate with other elements while decreasing the software development time via a new application specification language; (b) workflows leading to a cyber-human coordination system that will take advantage of feedback loops (e.g., based on consensus of crowdsourced data and its quality) for self-adaptation to changes and increased sustainability of the overall system, (c) new crowdsourcing micro-tasks with ability of being reusable for a variety of scenarios and containing user activity sensors for studying time-effective user interfaces, and (d) services to support automated creation and configuration of crowdsourcing workflows on demand to fit the needs of individual groups. A cloud-based system will be deployed to provide the necessary execution environment with traceability of service executions involved in cyber-human workflows, and cost-effectiveness analysis of all the software elements developed in this project will provide assessment and evaluation of long standing what-if scenarios pertaining human- and machine-intelligent tasks. Crowdsourcing activities will attract a wide range of users with tasks that require low expertise, and at the same time it will expose volunteers to applied science and engineering, potentially attracting interest of K-12 teachers and students.", "ProgramReferenceCode": ["7433", "8005"], "OrganizationZip": "326112002"}, {"OrganizationCity": "KNOXVILLE", "OrganizationState": "TN", "Title": "Collaborative Research: SI2-SSI:Task-Based Environment for Scientific Simulation at Extreme Scale (TESSE)", "ProgramManager": "Daniel Katz", "State": "TN", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "DMR SHORT TERM SUPPORT", "Software Institutes"], "ProgramElementCode": ["1253", "1712", "8004"], "StartDate": "05/15/2015", "OrganizationStreet": "1 CIRCLE PARK", "EndDate": "04/30/2018", "PrincipalInvestigator": "George Bosilca", "ARRAAmount": null, "OrganizationPhone": "8659743466", "AwardNumber": "1450300", "PIEmailAddress": "bosilca@eecs.utk.edu", "NSFDirectorate": "CSE", "Co-PIName": "Thomas Herault", "LastAmendmentDate": "05/20/2015", "Organization": "University of Tennessee Knoxville", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1178068", "Abstract": "This project, TESSE, is developing a new-generation programming environment that uniquely address the needs of emerging computational models in chemistry and other fields, and allowing these models to reap the benefits of the computer hardware of tomorrow. The project thus is bringing closer the ability to predict properties of matter and design matter transformations of importance to the technological leadership and energy security of the US. TESSE&apos;s impact has scientific, cyberinfrastructure, and interdisciplinary educational aspects. TESSE helps carry widely used scientific applications (NWChem, MADNESS, MPQC, etc.) into a whole new generation of many-core and accelerated architectures, while at the same time dramatically improving programmer productivity. In terms of cyberinfrastructure, TESSE&apos;s PaRSEC runtime system delivers capabilities that many libraries and advanced applications will require for high performance on large scale and hybridized systems.&lt;br/&gt;&lt;br/&gt;The goals of this project, Task-based Environment for Scientific Simulation at Extreme Scale (TESSE), are to design and demonstrate via substantial scientific simulations within chemistry and other disciplines a prototype software framework that provides a groundbreaking response to the twin problems of portable performance and programmer productivity for advanced scientific applications on emerging massively-parallel, hybrid, many-core systems. TESSE will create a viable foundation for a new generation of science codes, one which enables even more rapid exploration of new physical models, provides greatly enhanced performance portability through directed acyclic graph (DAG) scheduling and auto-tuned kernels, and works towards full interoperability between major chemistry packages through compatible runtimes and data structures. TESSE will mature to become a standard, widely available, community-based and broadly-applicable parallel programming environment complementing and rivaling MPI/OpenMP. This is needed due to the widely appreciated shortfalls of existing mainstream programming models and the already huge successes of the existing DAG-based runtimes that are the foundation of the next generation of NSF- and DOE-supported (Sca)LAPACK high-performance linear algebra libraries.", "ProgramReferenceCode": ["7433", "8009", "8084", "9150", "9216"], "OrganizationZip": "379960003"}, {"OrganizationCity": "Columbus", "OrganizationState": "OH", "Title": "Collaborative Research: SI2-SSI: A Comprehensive Performance Tuning Framework for the MPI Stack", "ProgramManager": "Daniel Katz", "State": "OH", "Program": ["INFORMATION TECHNOLOGY RESEARC", "Software Institutes"], "ProgramElementCode": ["1640", "8004"], "StartDate": "06/01/2012", "OrganizationStreet": "Office of Sponsored Programs", "EndDate": "05/31/2016", "PrincipalInvestigator": "Dhabaleswar Panda", "ARRAAmount": null, "OrganizationPhone": "6146888735", "AwardNumber": "1148371", "PIEmailAddress": "panda@cse.ohio-state.edu", "NSFDirectorate": "CSE", "Co-PIName": "Karen Tomko", "LastAmendmentDate": "06/04/2012", "Organization": "Ohio State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1251644", "Abstract": "The Message Passing Interface (MPI) is a very widely used parallel programming model on modern High-End Computing (HEC) systems. Many performance aspects of MPI libraries, such as latency, bandwidth, scalability, memory footprint, cache pollution, overlap of computation and communication etc. are highly dependent on system configuration and application requirements. Additionally, modern clusters are changing rapidly with the growth of multi-core processors and commodity networking technologies such as InfiniBand and 10GigE/iWARP. They are becoming diverse and heterogeneous with varying number of processor cores, processor speed, memory speed, multi-generation network adapters/switches, I/O interface technologies, and accelerators (GPGPUs), etc. Typically, any MPI library deals with the above kind of diversity in platforms and sensitivity of applications by employing various runtime parameters. These parameters are tuned during its release, or by&lt;br/&gt;system administrators, or by end-users. These default parameters may or may not be optimal for all system configurations and applications.&lt;br/&gt;&lt;br/&gt;The MPI library of a typical proprietary system goes through heavy performance tuning for a range of applications. Since commodity clusters provide greater flexibility in their configurations (processor, memory and network), it is very hard to achieve optimal tuning using released version of any MPI library, with its default settings. This leads to the following broad challenge: &quot;Can a comprehensive performance tuning framework be designed for MPI library so that the next generation InfiniBand, 10GigE/iWARP and RoCE clusters and applications will be able to extract `bare-metal&apos; performance and maximum scalability?&quot; The investigators, involving computer&lt;br/&gt;scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) as well as computational scientists from the Texas Advanced Computing Center (TACC) and San Diego Supercomputer Center (SDSC), University of California San Diego (UCSD), will be addressing the above challenge with innovative solutions.&lt;br/&gt;&lt;br/&gt;The investigators will specifically address the following challenges: 1) Can a set of static tools be designed to optimize performance of an MPI library during installation time? 2) Can a set of dynamic tools with low overhead be designed to optimize performance on a per-user and per-application basis during production runs? 3) How to incorporate the proposed performance tuning framework with the upcoming MPIT interface? 4) How to configure MPI libraries on a given system to deliver different optimizations to a set of driving applications? and 5) What kind of benefits (in terms of performance, scalability, memory efficiency and reduction in cache pollution) can be achieved by the proposed tuning framework? The research will be driven by a set of applications from established NSF computational science researchers running large scale simulations on the TACC Ranger and other systems at OSC, SDSC and OSU. The proposed designs will be integrated into the open-source MVAPICH2 library.", "ProgramReferenceCode": ["7433", "8009", "1640", "8004"], "OrganizationZip": "432101016"}, {"OrganizationCity": "Chicago", "OrganizationState": "IL", "Title": "SI2-SSI Integration of Synchrotron X-Ray Analysis Software Methods into the Larch Framework", "ProgramManager": "Rajiv Ramnath", "State": "IL", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "GEOPHYSICS", "DMR SHORT TERM SUPPORT", "Software Institutes", "EarthCube"], "ProgramElementCode": ["1253", "1574", "1712", "8004", "8074"], "StartDate": "10/01/2015", "OrganizationStreet": "5801 South Ellis Avenue", "EndDate": "09/30/2018", "PrincipalInvestigator": "Matthew Newville", "ARRAAmount": null, "OrganizationPhone": "7737028669", "AwardNumber": "1450468", "PIEmailAddress": "newville@cars.uchicago.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "09/16/2015", "Organization": "University of Chicago", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "540969", "Abstract": "The solutions to many of the outstanding problems in geology, environmental science, material science, and biology require understanding the chemical state and detailed atomic structure of the molecules and solids that make up our world. Such problems range from understanding the molecular forms of arsenic in rice, determining the chemical composition of the earths interior, and improving the performance and reducing the environmental impact of batteries that are in our laptops, cellphones, and cars. The nation&apos;s synchrotron facilities provide powerful X-ray facilities that allow researchers to study these questions by investigating the chemical makeup and crystal structure of complex, real-world materials such as plant seeds, contaminated soils, human and animal tissue, minerals and meteorites, and working batteries and catalysts. Synchrotron measurement techniques have developed very rapidly over the past few decades, and are being used by many more researchers. The ability to handle and interpret the large and complex datasets now being routinely generated at these facilities is often a significant challenge, even for experts. The work here will develop the Larch X-ray analysis framework to provide open-source software that is easy to use and specific enough to correctly interpret several categories of synchrotron X-ray data. The approach will provide tools that are flexible enough to enable researchers to explore and interpret new combinations of data easily enough to make new connections and discoveries in a wide variety of scientific areas.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project will integrate visualization and analysis software for multiple synchrotron X-ray techniques into the open source and extensible Larch X-ray Analysis framework. The immediate focus of the work is to support visualization and quantitative analysis of the rich and complex data from X-ray microprobes, including X-ray fluorescence imaging, fluorescence and absorption spectroscopies, and X-ray diffraction. The Larch framework already provides a suite of analysis procedures for X-ray absorption spectroscopy and fluorescence imaging, and has been designed to be readily extensible by adding plug-ins in Python, widely used in scientific computing and being embraced in the synchrotron user communities. Existing state-of-the-art analysis procedures for X-ray fluorescence, X-ray absorption, and X-ray diffraction have been identified to be integrated into the Larch framework, adapting and translating the software as needed to be compatible with the open-source Python framework. With the combination of state-of-the-art analysis methods for multiple data types, Larch will provide a single well-supported and -documented analysis package with robust, easy to use analytic methods for a range of synchrotron X-ray data. By being easily extensible, the Larch package can also accommodate methods for other synchrotron X-ray techniques.", "ProgramReferenceCode": ["7433", "7574", "8009", "9216", "8004"], "OrganizationZip": "606375418"}, {"OrganizationCity": "TEMPE", "OrganizationState": "AZ", "Title": "SI2-SSE: E-SDMS: Energy Simulation Data Management System Software", "ProgramManager": "Daniel Katz", "State": "AZ", "Program": ["Software Institutes", "CDS&E"], "ProgramElementCode": ["8004", "8084"], "StartDate": "10/01/2013", "OrganizationStreet": "ORSPA", "EndDate": "09/30/2016", "PrincipalInvestigator": "K. Selcuk Candan", "ARRAAmount": null, "OrganizationPhone": "4809655479", "AwardNumber": "1339835", "PIEmailAddress": "candan@asu.edu", "NSFDirectorate": "CSE", "Co-PIName": "Maria Luisa Sapino", "LastAmendmentDate": "09/11/2013", "Organization": "Arizona State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "499699", "Abstract": "The building sector was responsible for nearly half of CO2 emissions in US in 2009. According to the US Energy Information Administration, buildings consume more energy than any other sector, with 48.7% of the overall energy consumption, and building energy consumption is projected to grow faster than the consumptions of industry and transportation sectors. As a response to this, by 2030 only 18% of the US building stock is expected to be relying on the current energy management technologies, with the rest either having been retrofitted or designed from the ground up using smart and cleaner energy technologies. These building energy management systems (BEMSs) need to integrate large volumes of data, including (a) continuously collected heating, ventilation, and air conditioning (HVAC) sensor and actuation data, (b) other sensory data, such as occupancy, humidity, lighting levels, air speed and quality, (c) architectural, mechanical, and building automation system configuration data for these buildings, (d) local whether and GIS data that provide contextual information, as well as (e) energy price, consumption, and cost data from electricity (such as smart grid) and gas utilities. In theory, these data can be leveraged from the initial design and/or retrofitting of buildings with data driven building optimization (including the evaluation of the building location, orientation, and alternative energy-saving strategies) to total cost of ownership (TCOs) simulation tools and day-to-day operation decisions. In practice, however, because of the size and complexity of the data, the varying spatial and temporal scales at which the key processes operate, (a) creating models to support such simulations, (b) executing simulations that involve 100s of inter-dependent parameters spanning multiple spatio-temporal frames, affected by complex dynamic processes operating at different resolutions, and (c) analyzing simulation results are extremely costly. The energy simulation data management system (e-SDMS) software will address challenges that arise from the need to model, index, search, visualize, and analyze, in a scalable manner, large volumes of multi-variate series resulting from observations and simulations. e-SDMS will, therefore, fill an important hole in data-driven building design and clean-energy (an area of national priority) and will enable applications and services with significant economic and environmental impact.&lt;br/&gt;&lt;br/&gt;The key observations driving the research is that many data sets of urgent interest to energy simulations include the following: (a) voluminous, (b) heterogeneous, (c) multi-variate, (d) temporal, (e) inter-related (meaning that the parameters of interest are dependent on each other and constrained with the structure of the building), and (f) multi-resolution (meaning that simulations and observations cover days to months of data and may be considered at different granularities of space, time, and parameters). Moreover, generating an appropriate ensemble of simulations for decision making often requires multiple simulations, each with different parameters settings corresponding to slightly different, but plausible, scenarios. Therefore, significant savings in modeling and analysis can be obtained through data management software supporting modular re-use of existing simulation results in new settings, such as re-contextualization and modular recomposition (or &quot;sketching&quot;) of building models and if-then analysis of simulation traces under new parameters, new building floorplans, and new contexts. In developing the energy simulation data management system (e-SDMS), the research addresses the key data challenges that render data-driven energy simulations, today, difficult. This requires (a) a novel building models, simulation traces, and sensor/actuation traces (BSS) data model to accommodate energy simulation data and models, (b) feature analysis and indexing of sensory data and simulation traces along with the corresponding building models, and (c) algorithms for analysis and exploration of simulation traces and re-contextualization of models for new building plans and contextual metadata. This research will therefore, impact computational challenges that arise from the need to model, analyze, index, visualize, search, and recompose, in a scalable manner, large volumes of multi-variate series resulting from energy observations and simulations. E-SDMS consists of an (a) eViz server, which works as a frontend to e-SDMS, an (b) eDMS middleware for feature extraction, indexing, simulation analysis, and sketching, and an (c) eStore backend for data storage. To avoid waste and achieve scalabilities needed for managing large data sets, e-SDMS employs novel multi-resolution data partitioning and resource allocation strategies. The multi-resolution data encoding, partitioning, and analysis algorithms are efficiently computable, leverage massive parallelism, and result in high quality, compact data descriptions.", "ProgramReferenceCode": ["7433", "8005"], "OrganizationZip": "852816011"}, {"OrganizationCity": "La Jolla", "OrganizationState": "CA", "Title": "Collaborative Research: SI2-SSI: Empowering the Scientific Community with Streaming Data Middleware: Software Integration into Complex Science Environments", "ProgramManager": "Daniel Katz", "State": "CA", "Program": ["ADVANCES IN BIO INFORMATICS", "ECOSYSTEM STUDIES", "Software Institutes", "Cyber Secur - Cyberinfrastruc"], "ProgramElementCode": ["1165", "1181", "8004", "8027"], "StartDate": "08/01/2012", "OrganizationStreet": "Office of Contract & Grant Admin", "EndDate": "07/31/2016", "PrincipalInvestigator": "Tony Fountain", "ARRAAmount": null, "OrganizationPhone": "8585344896", "AwardNumber": "1148458", "PIEmailAddress": "fountain@sdsc.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Ilya Zaslavsky", "Sameer Tilak"], "LastAmendmentDate": "09/14/2015", "Organization": "University of California-San Diego", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1455429", "Abstract": "This project will catalyze a new generation of sensor-based streaming data applications by building on the foundation of the Open Source DataTurbine (OSDT) middleware (www.dataturbine.org). This will involve strategic software development, systems integration and testing, science experiments, and education and outreach. The software developments will include (A) tools for configuring, testing, and controlling distributed sensor systems, (B) software interfaces that are compliant with the Open Geospatial Consortium Sensor Web Enablement standards, (C) a new OSDT-Matlab interface, and (D) OSDT cybersecurity enhancements. These software products will be integrated into the production research infrastructures at Purdue, the University of Connecticut, the North Temperate Lake LTER Network site, and the Monterey Bay Aquarium Research Institute. The software will be professionally developed and managed as community resources. The intellectual merit of this project will be advances in software engineering, interoperability standards, and systems management for complex real-time sensor systems as well as advances on important open questions in civil engineering, marine science, and ecology. This project will yield broader impacts by enabling new science applications, enhancing productivity, and accelerating innovation across NSF directorates. To ensure that these broader impacts are realized, the team will provide student research opportunities, develop curriculum materials and training courses, and cultivate a community of software users.", "ProgramReferenceCode": ["7434", "8009", "1165", "1181", "8004", "8027"], "OrganizationZip": "920930934"}, {"OrganizationCity": "Corvallis", "OrganizationState": "OR", "Title": "SI2-SSE: Collaborative Research: An Intelligent and Adaptive Parallel CPU/GPU Co-Processing Software Library for Aaccelerating Reactive-Flow Simulations", "ProgramManager": "Daniel Katz", "State": "OR", "Program": ["COMBUSTION, FIRE, & PLASMA SYS", "Software Institutes"], "ProgramElementCode": ["1407", "8004"], "StartDate": "09/01/2015", "OrganizationStreet": "OREGON STATE UNIVERSITY", "EndDate": "08/31/2018", "PrincipalInvestigator": "Kyle Niemeyer", "ARRAAmount": null, "OrganizationPhone": "5417374933", "AwardNumber": "1535065", "PIEmailAddress": "kyle.niemeyer@oregonstate.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/03/2015", "Organization": "Oregon State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "278287", "Abstract": "In order to develop the next generation of clean and efficient vehicle engines and power-generating combustors, engineers need the next generation of computational modeling tools. Accurately describing the chemistry of conventional and alternative liquid transportation fuels is vital to predict harmful emission levels and other important quantities, but the high computational cost of detailed models for chemistry poses a significant barrier to use by designers. In order to use such accurate models, software is needed that can efficiently handle chemistry in practical simulations. This collaborative project aims to develop such tools, employing the computational power of modern parallelized central processing units (CPUs) and graphics processing units (GPUs). In addition to helping designers create clean and efficient engine technology, the advances made in this project are widely applicable to other computational modeling problems including astrophysics, nuclear reactions, atmospheric chemistry, biochemical networks, and even cardiac electrophysiology.&lt;br/&gt;&lt;br/&gt;The objective of the proposed effort is to develop software elements specifically targeted at co-processing on GPUs, CPUs, and other many-core accelerator devices to reduce the computational cost of using detailed chemistry and enable high-fidelity yet affordable reactive-flow simulations. This will be achieved by (1) developing and comparing chemical kinetics integration algorithms for parallel operation on CPUs and GPUs/accelerators, (2) developing a method for detecting local stiffness due to chemical kinetics and adaptively selecting the most efficient solver based on available hardware, (3) implementing a computational cell clustering strategy to group similar spatial locations, (4) demonstrating the improved performance offered by these software elements using commercial and open-source computational fluid dynamics codes for modeling reactive flows, and (5) designing a portable and sustainable software library based on the above software elements, including building a community of users. The result of this program will be an open source software library that significantly decreases the cost of using detailed, accurate chemistry in realistic combustion simulations; the success of the program will be determined based on achieving order-of-magnitude performance improvement or better.", "ProgramReferenceCode": ["148E", "7433", "8005"], "OrganizationZip": "973318507"}, {"OrganizationCity": "WALTHAM", "OrganizationState": "MA", "Title": "SI2-SSI: The Language Application Grid: A Framework for Rapid Adaptation and Reuse", "ProgramManager": "Daniel Katz", "State": "MA", "Program": ["SPECIAL PROJECTS - CISE", "SPECIAL PROJECTS - CCF", "COLLABORATIVE RESEARCH", "COMPUTER SYSTEMS", "IIS SPECIAL PROJECTS", "ROBUST INTELLIGENCE", "SOFTWARE & HARDWARE FOUNDATION", "Software Institutes", null], "ProgramElementCode": ["1714", "2878", "7298", "7354", "7484", "7495", "7798", "8004", "O422"], "StartDate": "08/01/2012", "OrganizationStreet": "415 SOUTH ST MAILSTOP 116", "EndDate": "07/31/2016", "PrincipalInvestigator": "James Pustejovsky", "ARRAAmount": null, "OrganizationPhone": "7817362121", "AwardNumber": "1147912", "PIEmailAddress": "jamesp@cs.brandeis.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Marc Verhagen", "Eric Nyberg", "Christopher Cieri"], "LastAmendmentDate": "09/11/2015", "Organization": "Brandeis University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1764929", "Abstract": "The need for robust language processing capabilities across academic disciplines, education, and industry is without question of vital importance to national security, infrastructure development, and the competitiveness of American business. However, at this time a robust, interoperable software infrastructure to support natural language processing (NLP) research and development does not exist. To fill this gap, this project establishes an large international collaborative effort involving key international players to develop an open, web-based infrastructure through which massive and distributed language resources can be easily accessed, in whole or in part, and within which tailored language services can be efficiently composed, disseminated and consumed by researchers, developers, and students. &lt;br/&gt;&lt;br/&gt;The goal of this project is to build a comprehensive network of web services and resources within the NLP community. This requires four specific activities: (1) Design, develop and promote a service-oriented architecture for NLP development that defines atomic and composite web services, along with support for service discovery, testing and reuse; (2) Construct a Language Application Grid (LAPPS Grid) based on Service Grid Software developed in Japan; (3) Provide an open advancement (OA) framework for component- and application-based evaluation that enables rapid identification of frequent error categories within modules, thus contributing to more effective investment of resources; (4) Actively promote adoption, use, and community involvement with the LAPPS Grid. &lt;br/&gt;&lt;br/&gt;By providing access to cloud-based services and support for locally-run services, the LAPPS Grid will lead to the development of a massive global network of language data and processing capabilities that can be used by scientists and engineers from diverse disciplines, providing components that require no expertise in language processing to use. Research in sociology, psychology, economics, education, linguistics, digital media, and the humanities will be impacted by the ability to easily manipulate and process diverse language data sources in multiple languages.", "ProgramReferenceCode": ["5983", "7433", "7944", "8009", "1714", "2878", "7484", "8004"], "OrganizationZip": "024532728"}, {"OrganizationCity": "Poughkeepsie", "OrganizationState": "NY", "Title": "SI2-SSI: The Language Application Grid: A Framework for Rapid Adaptation and Reuse", "ProgramManager": "Daniel Katz", "State": "NY", "Program": ["LINGUISTICS", "METHOD, MEASURE & STATS", "SPECIAL PROJECTS - CISE", "COLLABORATIVE RESEARCH", "IIS SPECIAL PROJECTS", "SOFTWARE & HARDWARE FOUNDATION", "Software Institutes", null], "ProgramElementCode": ["1311", "1333", "1714", "7298", "7484", "7798", "8004", "O422"], "StartDate": "08/01/2012", "OrganizationStreet": "124 Raymond Avenue", "EndDate": "07/31/2016", "PrincipalInvestigator": "Nancy Ide", "ARRAAmount": null, "OrganizationPhone": "8454377092", "AwardNumber": "1147944", "PIEmailAddress": "ide@cs.vassar.edu", "NSFDirectorate": "CSE", "Co-PIName": ["James Pustejovsky", "Eric Nyberg", "Christopher Cieri"], "LastAmendmentDate": "09/11/2015", "Organization": "Vassar College", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "978057", "Abstract": "The need for robust language processing capabilities across academic disciplines, education, and industry is without question of vital importance to national security, infrastructure development, and the competitiveness of American business. However, at this time a robust, interoperable software infrastructure to support natural language processing (NLP) research and development does not exist. To fill this gap, this project establishes an large international collaborative effort involving key international players to develop an open, web-based infrastructure through which massive and distributed language resources can be easily accessed, in whole or in part, and within which tailored language services can be efficiently composed, disseminated and consumed by researchers, developers, and students. &lt;br/&gt;&lt;br/&gt;The goal of this project is to build a comprehensive network of web services and resources within the NLP community. This requires four specific activities: (1) Design, develop and promote a service-oriented architecture for NLP development that defines atomic and composite web services, along with support for service discovery, testing and reuse; (2) Construct a Language Application Grid (LAPPS Grid) based on Service Grid Software developed in Japan; (3) Provide an open advancement (OA) framework for component- and application-based evaluation that enables rapid identification of frequent error categories within modules, thus contributing to more effective investment of resources; (4) Actively promote adoption, use, and community involvement with the LAPPS Grid. &lt;br/&gt;&lt;br/&gt;By providing access to cloud-based services and support for locally-run services, the LAPPS Grid will lead to the development of a massive global network of language data and processing capabilities that can be used by scientists and engineers from diverse disciplines, providing components that require no expertise in language processing to use. Research in sociology, psychology, economics, education, linguistics, digital media, and the humanities will be impacted by the ability to easily manipulate and process diverse language data sources in multiple languages.", "ProgramReferenceCode": ["5983", "7433", "7944", "8009", "9251", "1311", "1333", "8004"], "OrganizationZip": "126040657"}, {"OrganizationCity": "Aurora", "OrganizationState": "CO", "Title": "SI2-SSI: Collaborative Research: Sustained Innovation for Linear Algebra Software (SILAS)", "ProgramManager": "Daniel Katz", "State": "CO", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes", "CDS&E-MSS"], "ProgramElementCode": ["1253", "8004", "8069"], "StartDate": "10/01/2013", "OrganizationStreet": "F428, AMC Bldg 500", "EndDate": "09/30/2016", "PrincipalInvestigator": "Julien Langou", "ARRAAmount": null, "OrganizationPhone": "3037240090", "AwardNumber": "1339797", "PIEmailAddress": "julien.langou@ucdenver.edu", "NSFDirectorate": "CSE", "Co-PIName": "Rodney James", "LastAmendmentDate": "09/05/2015", "Organization": "University of Colorado at Denver-Downtown Campus", "AwardInstrument": "Continuing grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "392492", "Abstract": "As the era of computer architectures dominated by serial processors comes to a close, the convergence of several unprecedented changes in processor design has produced a broad consensus that much of the essential software infrastructure of computational science and engineering is utterly obsolete. Math libraries have historically been in the vanguard of software that must be quickly adapted to such design revolutions because they are the common, low-level software workhorses that do all the most basic mathematical calculations for many different types of applications. The Sustained Innovation for Linear Algebra Software (SILAS) project updates two of the most widely used numerical libraries in the history of Computational Science and Engineering---LAPACK and ScaLAPACK, (abbreviated Sca/LAPACK)---enhancing and hardening them for this ongoing revolution in processor architecture and system design. SILAS creates a layered package of software components, capable of running at every level of the platform deployment pyramid, from the desktop to the largest supercomputers in the world. It achieves three complementary objectives: 1) Wherever possible, SILAS delivers seamless access to the most up-to-date algorithms, numerical implementations, and performance, by way of Sca/LAPACK programming interfaces that are familiar to many computational scientists; 2) Wherever necessary, SILAS makes advanced algorithms, numerical implementations and performance capabilities available through new interface extensions; and 3) SILAS provides a well engineered conduit through which new discoveries at the frontiers of research in these areas can be channeled as quickly as possible to all the application communities that depend on high performance linear algebra. The improvements and innovations included in SILAS derive from a variety of sources. They represent the results (including designs and well tested prototypes) of the PIs&apos; own algorithmic and software research agenda, which has targeted multicore, hybrid and extreme scale system architectures. They are an outcome of extensive and on-going interactions with users, vendors, and the management of large NSF and DOE supercomputing facilities. They flow from cross-disciplinary engagement with other areas of computer science and engineering, anticipating the demands and opportunities of new architectures and programming models. And finally, they come from the enthusiastic participation of the research community in developing and offering enhanced versions of existing Sca/LAPACK codes.&lt;br/&gt;&lt;br/&gt;The primary impact of SILAS is a direct function of the importance of the Sca/LAPACK libraries to many branches of computational science. The Sca/LAPACK libraries are the community standard for dense linear algebra and have been adopted and/or supported by a large community of users, computing centers, and HPC vendors. Learning to use them is a basic part of the education of a computational scientist or engineer in many fields and at many academic institutions. Application domains where Sca/LAPACK have historically been heavily used include (among a host of other examples) airplane wing design, radar cross-section studies, flow around ships and other off-shore constructions, diffusion of solid bodies in a liquid, noise reduction, and diffusion of light through small particles. Moreover, the list of application partners working with SILAS to enhance and transform these libraries for next generation platforms expands this traditional list to include quantum chemistry, adaptive mesh refinement schemes, computational materials science, geophysical flows, stochastic simulation and database research for &quot;big data&quot;. No other numerical library can claim this breadth of integration with the community. Thus, there is every reason to believe that enhancing these libraries with state of the art methods and algorithms and adapting them for new and emerging platforms (reaching up to extreme scale), will have a correspondingly large impact on the research and education community, government laboratories, and private industry.", "ProgramReferenceCode": ["7433", "8009", "9251"], "OrganizationZip": "800452571"}, {"OrganizationCity": "Houston", "OrganizationState": "TX", "Title": "Collaborative Research: SI2-CHE: ExTASY Extensible Tools for Advanced Sampling and analYsis", "ProgramManager": "Evelyn M. Goldfield", "State": "TX", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes"], "ProgramElementCode": ["1253", "8004"], "StartDate": "09/01/2013", "OrganizationStreet": "6100 MAIN ST", "EndDate": "08/31/2016", "PrincipalInvestigator": "Cecilia Clementi", "ARRAAmount": null, "OrganizationPhone": "7133484820", "AwardNumber": "1265929", "PIEmailAddress": "cecilia@rice.edu", "NSFDirectorate": "MPS", "LastAmendmentDate": "08/28/2013", "Organization": "William Marsh Rice University", "AwardInstrument": "Standard Grant", "NsfOrganization": "CHE", "AwardedAmountToDate": "585000", "Abstract": "Collaborative Research: SI2-CHE&lt;br/&gt;ExTASY Extensible Tools for Advanced Sampling and analYsis&lt;br/&gt;&lt;br/&gt;An international team consisting of Cecilia Clementi(Rice University), Mauro Maggioni (Duke University) Shantenu Jha (Rutgers University), Glenn Martyna (BM T. J. Watson Laboratory ), Charlie Laughton (University of Nottingham), Ben Leimkuhler ( University of Edinburgh), Iain Bethune (University of Edinburgh) and Panos Parpas(Imperial College) are supported through the SI2-CHE program for the development of ExTASY: Extensible Toolkit for Advanced Sampling and analYsis, a conceptual and software framework that provides a step-change in the sampling of the conformational space of macromolecular systems. Specifically, ExTASY is a lightweight toolkit to enable first-class support for ensemble-based simulations and their seamless integration with dynamic analysis capabilities and ultra-large time step integration methods, whilst being extensible to other community software components via well-designed and standard interfaces. &lt;br/&gt;&lt;br/&gt; The primary impacts of this project are in the biological sciences. This software advances our understanding of biologically important systems, as it can be used to obtain fast and accurate sampling of the conformational dynamics of stable proteins; a prerequisite for the accurate prediction of thermodynamic parameters and biological functions. It also allows tackling systems like intrinsically disordered proteins, which can be beyond the reach of classical structural biology. Along with the research itself, the PIs are involved with outreach programs to attract high school students to science.", "ProgramReferenceCode": ["5918", "5946", "5950", "7433", "8009"], "OrganizationZip": "770051827"}, {"OrganizationCity": "Houston", "OrganizationState": "TX", "Title": "SI2-SSI: Collaborative Research: A Sustainable Infrastructure for Perfomance, Security, and Correctness Tools", "ProgramManager": "Daniel Katz", "State": "TX", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "08/01/2015", "OrganizationStreet": "6100 MAIN ST", "EndDate": "07/31/2020", "PrincipalInvestigator": "John Mellor-Crummey", "ARRAAmount": null, "OrganizationPhone": "7133484820", "AwardNumber": "1450273", "PIEmailAddress": "johnmc@rice.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "07/20/2015", "Organization": "William Marsh Rice University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1500000", "Abstract": "Software has become indispensable to society, used by computational scientists for science and engineering, by analysts mining big data for value, and to connect society over the Internet. However, the properties of software systems for any of these purposes cannot be understood without accounting for code transformations applied by optimizing compilers used to compose algorithm and data structure templates, and libraries available only in binary form. To address this need, this project will overhaul, integrate, and enhance static binary analysis and runtime technologies to produce components that provide a foundation for performance, correctness, and security tools. The project will build upon three successful and widely adopted open source software packages: the DynInst library for analysis and transformation of application binaries, the MRNet infrastructure for control of large-scale parallel executions and data analysis of their results, and the HPCToolkit performance analysis tools. The project team will engage the community to participate in the design and evaluation of the emerging components, as well as to adopt its components. &lt;br/&gt;&lt;br/&gt;This project will have a wide range of impacts. First, software components built by the project will enable the development of sophisticated, high-quality, end-user performance, correctness, and security tools built by the project team, as well as others in academia, government, and industry. Software developed by the project team will help researchers and developers tackle testing, debugging, monitoring, analysis, and tuning of applications for systems at all scales. Second, end-user tools produced by the project have a natural place in the classroom to help students write efficient, correct, and secure programs. Third, components produced by the project will lower the barrier for new researchers to enter the field and build tools that have impact on production applications without years of investment. Fourth, the project will provide training for graduate students and interns in the area of software for performance, correctness, and security. Finally, through workshops and tutorials, the project will disseminate project results, provide training to enable others to leverage project software, and grow a community of tool researchers who depend on project components and thus have a strong motivation to help sustain project software into the future.&lt;br/&gt;&lt;br/&gt;Modernizing open-source software components and tools for binary analysis will enable static analysis of application characteristics at the level of executable machine code, transformation of binaries to inject monitoring code, measurement to capture a detailed record of application?s interactions with all facets of a target platform, analysis of recorded data in parallel, and attribution of analysis results back to application source code in meaningful ways. Providing innovative, software components that support development of robust performance, correctness, and security tools will accelerate innovation by tools researchers and help them grapple with the increasing complexity of modern software. Of particular note, helping tools researchers and computational scientists grapple with the challenges of software for modern parallel systems and producing training materials that help people use this software, addresses several of the needs identified in the NSF Vision for Cyberinfrastructure for the 21st Century.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "770051827"}, {"OrganizationCity": "RALEIGH", "OrganizationState": "NC", "Title": "SI2-SSE: Multiscale Software for Quantum Simulations in Materials Design, Nano Science and Technology", "ProgramManager": "Daniel Katz", "State": "NC", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "DMR SHORT TERM SUPPORT", "Software Institutes"], "ProgramElementCode": ["1253", "1712", "8004"], "StartDate": "09/01/2013", "OrganizationStreet": "CAMPUS BOX 7514", "EndDate": "08/31/2016", "PrincipalInvestigator": "Jerzy Bernholc", "ARRAAmount": null, "OrganizationPhone": "9195152444", "AwardNumber": "1339844", "PIEmailAddress": "bernholc@ncsu.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Carl Kelley", "Wenchang Lu", "Miroslav Hodak"], "LastAmendmentDate": "08/29/2013", "Organization": "North Carolina State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "500000", "Abstract": "The emergence of petascale computing platforms brings unprecedented opportunities for transformational research through simulation. However, future breakthroughs will depend on the availability of high-end simulation software, which will fully utilize these unparalleled resources and provide the long-sought third avenue for scientific progress in key areas of national interest. This award will deliver a set of open source petascale quantum simulation tools in the broad areas of materials design, nano science and nanotechnology. Materials prediction and design are key aspects to the recently created Materials Genome initiative, which seeks to &quot;deploy advanced materials at least twice as fast, at a fraction of the cost.&quot; Computational materials design is the critical aspect of that initiative, which relies on computation guiding experiments. The outcomes of the latter will in turn lead to follow-up computation in an iterative feedback loop. Nanoscience, which studies properties of materials and processes on fundamental scale of nanometers, promises development of materials and systems with radically new properties. However, the nanoscale properties are hard to measure and even harder to predict theoretically. Only simulations that can fully account for the complexity and variability at that fundamental scale stand a chance of predicting and utilizing the macroscopic properties that emerge. This truly requires petascale resources and efficient petascale software tools.&lt;br/&gt;&lt;br/&gt;This award will develop software tools build on the real-space multigrid (RMG) software suite and distribute them to the national user community. The RMG code already scales to 128,000 CPU cores and 18,000 GPU nodes. The award will further enhance RMG through development of new iterative methods with improved convergence, optimization of additional modules for existing and new petascale computing platforms, and creation of ease-to-use interfaces to the main codes. Workshops in RMG usage will be conducted at XSEDE workshops and other meetings of NSF supercomputing centers. RMG will be distributed through a web portal, which will also contain user forums and video tutorials, recorded at live user sessions. A library of representative examples for the main petascale platforms will be maintained. RMG will enable quantum simulations of unprecedented size, enabling studies of the building blocks of functional nano or bio-nano structures, which often involve thousands of atoms and must be described with the requisite fidelity. The development of petascale quantum simulation software and its user community will lead to cross-fertilization of ideas both within and across fields. Students and postdocs trained in this area will have significant opportunities for advancement and making substantial impact on their own.", "ProgramReferenceCode": ["7433", "7569", "8005", "9216", "9263"], "OrganizationZip": "276957514"}, {"OrganizationCity": "Columbus", "OrganizationState": "OH", "Title": "SI2-SSI: Collaborative Research: A Software Infrastructure for MPI Performance Engineering: Integrating MVAPICH and TAU via the MPI Tools Interface", "ProgramManager": "Daniel Katz", "State": "OH", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "09/01/2015", "OrganizationStreet": "Office of Sponsored Programs", "EndDate": "08/31/2019", "PrincipalInvestigator": "Dhabaleswar Panda", "ARRAAmount": null, "OrganizationPhone": "6146888735", "AwardNumber": "1450440", "PIEmailAddress": "panda@cse.ohio-state.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/31/2015", "Organization": "Ohio State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1200000", "Abstract": "Message-Passing Interface (MPI) continues to dominate the supercomputing landscape, being the primary parallel programming model of choice. A large variety of scientific applications in use today are based on MPI. On the current and next-generation High-End Computing (HEC) systems, it is essential to understand the interaction between time-critical applications and the underlying MPI implementations in order to better optimize them for both scalability and performance. Current users of HEC systems develop their applications with high-performance MPI implementations, but analyze and fine tune the behavior using standalone performance tools. Essentially, each software component views the other as a blackbox, with little sharing of information or access to capabilities that might be useful in optimization strategies. Lack of a standardized interface that allows interaction between the profiling tool and the MPI library has been a big impediment. The newly introduced MPI_T interface in the MPI-3 standard provides a simple mechanism that allows MPI implementers to expose variables representing configuration parameters or performance measurements from within the implementation for the benefit of tools, tuning frameworks, and other support libraries. However, few performance analysis and tuning tools take advantage of the MPI_T interface and none do so to dynamically optimize at execution time. This research and development effort aims to build a software infrastructure for MPI performance engineering using the new MPI_T interface.&lt;br/&gt;&lt;br/&gt;With the adoption of MPI_T in the MPI standard, it is now possible to take positive steps to realize close interaction between and integration of MPI libraries and performance tools. This research, undertaken by a team of computer scientists from OSU and UO representing the open source MVAPICH and TAU projects, aims to create an open source integrated software infrastructure built on the MPI_T interface which defines the API for interaction and information interchange to enable fine grained performance optimizations for HPC applications. The challenges addressed by the project include: 1) enhancing existing support for MPI_T in MVAPICH to expose a richer set of performance and control variables; 2) redesigning TAU to take advantage of the new MPI_T variables exposed by MVAPICH; 3) extending and enhancing TAU and MVAPICH with the ability to generate recommendations and performance engineering reports; 4) proposing fundamental design changes to make MPI libraries like MVAPICH ``reconfigurable&apos;&apos; at runtime; and 5) adding support to MVAPICH and TAU for interactive performance engineering sessions. The framework will be validated on a variety of HPC benchmarks and applications. The integrated middleware and tools will be made publicly available to the community. The research will have a significant impact on enabling optimizations of HPC applications that have previously been difficult to provide. As a result, it will contribute to deriving &quot;best practice&quot; guidelines for running on next-generation Multi-Petaflop and Exascale systems. The research directions and their solutions will be used in the curriculum of the PIs to train undergraduate and graduate students.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "432101016"}, {"OrganizationCity": "Los Angeles", "OrganizationState": "CA", "Title": "SI2-SSI: A Sustainable Community Software Framework for Petascale Earthquake Modeling", "ProgramManager": "Rajiv Ramnath", "State": "CA", "Program": ["Geotechnical Engineering and M", "PetaApps", "Software Institutes"], "ProgramElementCode": ["1636", "7691", "8004"], "StartDate": "08/01/2012", "OrganizationStreet": "University Park", "EndDate": "07/31/2016", "PrincipalInvestigator": "Thomas Jordan", "ARRAAmount": null, "OrganizationPhone": "2137407762", "AwardNumber": "1148493", "PIEmailAddress": "tjordan@usc.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Kim Olsen", "Yifeng Cui", "Jacobo Bielak"], "LastAmendmentDate": "08/07/2012", "Organization": "University of Southern California", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "2522784", "Abstract": "Earthquakes have major economic and societal consequences as can be seen from the aftermath of the recent large earthquakes in Japan, Chile, and New Zealand. This multidisciplinary project, which includes both geoscientists, computer scientists, and structural engineers, integrates high-level and middle-level scientific software elements developed by the Southern California Earthquake Center (SCEC) into a software environment for integrated seismic modeling that can be used for seismic hazard analysis. The framework includes integration of community velocity models, codes for dynamic and pseudo-dynamic rupture generation, deterministic and stochastic earthquake engines, and the applications necessary to employ forward simulations in two types of inverse problems: seismic source imaging and full 3D tomography. Modifications to already existing software packages slated to be significantly enhanced in the course of the workflow will allow simulations to be run on petascale machines and allow the better managing of scientific workflows. The work also focuses on software lifecycle issues such as model formation, verification, prediction, and validation and support the use of petascale computers by earthquake scientists. The goal of the project is to facilitate the incorporation of better theory and data into computationally intensive modeling of earthquake processes. Software will be designed to interface smoothly with OpenSHA, as well as OpenSEES, PEER, and NEES. Project partners will also develop and test two computational platforms, one that will have a user-friendly interface for calculating seismographs and the other will generate large suites of simulations for a layered earthquake hazard model. Models will be validated against datasets for 13 well-recorded historic California earthquakes of magnitude 6.0 or higher. The initial API will take advantage of the asynchronous IO features of Fortran 2003 with plans for adding C/C++ and Python interfaces. All codes developed will be open-source and publicly available and software distribution will be accompanied by sample input datasets and example forecast results. Broader impacts include the development of a new generation of time-dependent earthquake forecasts to produce ground-shake hazard maps, partnership with a federal agency and the private sector. It also includes a component of student and postdoctoral training and outreach to user communities. Undergraduate interns, many of whom have historically been from groups under-represented in STEM fields, will be trained in use of the software during an 8-week summer training course.", "ProgramReferenceCode": ["037E", "043E", "1576", "7433", "8009"], "OrganizationZip": "900890001"}, {"OrganizationCity": "boise", "OrganizationState": "ID", "Title": "SI2-SSE: GEM3D: Open-Source Cartesian Adaptive Complex Terrain Atmospheric Flow Solver for GPU Clusters", "ProgramManager": "Daniel Katz", "State": "ID", "Program": ["PHYSICAL & DYNAMIC METEOROLOGY", "Software Institutes", "EarthCube"], "ProgramElementCode": ["1525", "8004", "8074"], "StartDate": "10/01/2014", "OrganizationStreet": "1910 University Drive", "EndDate": "09/30/2017", "PrincipalInvestigator": "Inanc Senocak", "ARRAAmount": null, "OrganizationPhone": "2084261574", "AwardNumber": "1440638", "PIEmailAddress": "senocak@boisestate.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Elena Sherman", "Grady Wright", "Donna Calhoun"], "LastAmendmentDate": "08/13/2014", "Organization": "Boise State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "500000", "Abstract": "The U.S. Government invests in leadership supercomputing facilities through several agencies to advance scientific discovery in many fronts. This project is motivated by this national commitment to supercomputing research and the increasing availability of many-core computing hardware from workstations to supercomputers. Today scientists and engineers have access to extreme-scale computing resources. However, many legacy codes do not take advantage of recent innovations in computing hardware, and there is a lack of open-source simulation science software that can effectively leverage the many-core computing paradigm. Computational fluid dynamics (CFD) solvers have advanced many fields such as aerospace engineering and atmospheric sciences. Many current open-source CFD models and numerical weather prediction models do not take full advantage of the superior compute performance of graphics processing units (GPUs). By creating an open-source community model that can execute on multi-GPU workstations and large GPU clusters, the project team expects to broaden the use of high-performance computing in fluid dynamics applications. The immediate target application is wind modeling over complex terrain, to support research and development in wind resource assessment, power forecasting, atmospheric research, and air pollution. Through this project, the PIs will continue to transfer and expand the knowledge bases in GPU computing, computational mathematics, and software engineering to new students. Skill sets that transcend traditional disciplines are highly prized by national laboratories as there is a critical shortage of workforce who can conduct scientific research using supercomputers. Students and postdoctoral researchers who are involved in this project will contribute toward this critical workforce. &lt;br/&gt;&lt;br/&gt;This project brings together engineers, applied mathematicians, and computer scientists. The entire suite of software elements will be designed for GPU clusters with an MPI-CUDA implementation that overlaps computation with communications using a three-dimensional decomposition for enhanced scalability. The implementation will balance performance and further development and ownership by a broader community of academic researchers. The team will follow modern software engineering practices for concurrent applications. An adaptive mesh refinement strategy that can scale on GPU clusters will be developed. A novel projection method based on radial basis functions will impose the divergence-free constraint on a hierarchy of adaptively refined grids. Software elements will be tested using unit testing and verification techniques for concurrent programs, and against data available from benchmark numerical problems. The flow solver will include modules for the immersed boundary approach for arbitrarily complex terrain and the dynamic large-eddy simulation technique. The software implementation and syntax will be intuitive to allow contributions from a larger community. The project team expects the proposed software to help reduce modeling errors with very high resolution simulations and contribute toward a fundamental understanding of turbulent winds over complex terrain. The PIs of this project will continue their teaching efforts in Parallel Scientific Computing, Computational Mathematics, and Software Engineering. The results will be disseminated through conference presentations and via a wiki site for the open-source project. Software elements will be released under an open-source GNU General Public License.", "ProgramReferenceCode": ["7433", "8005", "9150"], "OrganizationZip": "837251135"}, {"OrganizationCity": "ORONO", "OrganizationState": "ME", "Title": "SI2-SSI: Collaborative Proposal: Performance Application Programming Interface for Extreme-Scale Environments (PAPI-EX)", "ProgramManager": "Daniel Katz", "State": "ME", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "09/01/2015", "OrganizationStreet": "5717 Corbett Hall", "EndDate": "08/31/2019", "PrincipalInvestigator": "Vincent Weaver", "ARRAAmount": null, "OrganizationPhone": "2075811484", "AwardNumber": "1450122", "PIEmailAddress": "vincent.weaver@maine.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/27/2015", "Organization": "University of Maine", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "274828", "Abstract": "Modern High Performance Computing (HPC) systems continue to increase in size and complexity. Tools to measure application performance in these increasingly complex environments must also increase the richness of their measurements to provide insights into the increasingly intricate ways in which software and hardware interact. The PAPI performance-monitoring library has provided a clear, portable interface to the hardware performance counters available on all modern CPUs and some other components of interest (scattered across the chip and system). Widely deployed and widely used, PAPI has established itself as fundamental software infrastructure, not only for the scientific computing community, but for many industry users of HPC as well. But the radical changes in processor and system design that have occurred over the past several years pose new challenges to PAPI and the HPC software infrastructure as a whole. The PAPI-EX project integrates critical PAPI enhancements that flow from both governmental and industry research investments, focusing on processor and system design changes that are expected to be present in every extreme scale platform on the path to exascale computing.&lt;br/&gt;&lt;br/&gt;The primary impact of PAPI-EX is a direct function of the importance of the PAPI library. PAPI has been in predominant use by tool developers, major national HPC centers, system vendors, and application developers for over 15 years. PAPI-EX builds on that foundation. As important research infrastructure, the PAPI-EX project allows PAPI to continue to play its essential role in the face of the revolutionary changes in the design and scale of new systems. In terms of enhancing discovery and education, the list of partners working with PAPI-EX includes NSF computing centers, major tool developers, major system vendors, and individual community leaders, and this diverse group will help facilitate training sessions, targeted workshops, and mini-symposia at national and international meetings. Finally, the active promotion of PAPI by many major system vendors means that PAPI, and therefore PAPI-EX, will continue to deliver major benefits for government and industry in many domains.&lt;br/&gt;&lt;br/&gt;PAPI-EX addresses a hardware environment in which the cores of current and future multicore CPUs share various performance-critical resources (a.k.a., &apos;inter-core&apos; resources), including power management, on-chip networks, the memory hierarchy, and memory controllers between cores. Failure to manage contention for these &apos;inter-core&apos; resources has already become a major drag on overall application performance. Consequently, the lack of ability to reveal the actual behavior of these resources at a low level, has become very problematic for the users of the many performance tools (e.g., TAU, HPCToolkit, Open|SpeedShop, Vampir, Scalasca, CrayPat, Active Harmony, etc.). PAPI-EX enhances and extends PAPI to solve this critical problem and prepare it to play its well-established role in HPC performance optimization. Accordingly, PAPI-EX targets the following objectives: (1) Develop shared hardware counter support that includes system-wide and inter-core measurements; (2) Provide support for data-flow based runtime systems; (3) Create a sampling interface to record streams of performance data with relevant context; (4) Combine an easy-to-use tool for text-based application performance analysis with updates to PAPI?s high-level API to create a basic, ?out of the box? instrumentation API.", "ProgramReferenceCode": ["7433", "8009", "9150"], "OrganizationZip": "044695717"}, {"OrganizationCity": "Ithaca", "OrganizationState": "NY", "Title": "SI2-SSE: Quantum Monte Carlo Software for a Broad Electronic Structure Research Community via Minimal Explicit Dependency (MED) programming", "ProgramManager": "Rajiv Ramnath", "State": "NY", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "DMR SHORT TERM SUPPORT", "Software Institutes", "DMREF"], "ProgramElementCode": ["1253", "1712", "8004", "8292"], "StartDate": "10/01/2015", "OrganizationStreet": "373 Pine Tree Road", "EndDate": "09/30/2018", "PrincipalInvestigator": "Cyrus Umrigar", "ARRAAmount": null, "OrganizationPhone": "6072555014", "AwardNumber": "1534965", "PIEmailAddress": "cyrusumrigar@cornell.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/25/2015", "Organization": "Cornell University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "420798", "Abstract": "This project, jointly supported by the Advanced Cyberinfrastructure (ACI) Division, Division of Materials Research (DMR), and Division of Chemistry (CHE) at NSF, will provide a robust, usable software infrastructure for methods for materials analysis, which, when run on high-performance computers, have the potential to revolutionize and accelerate the discovery of materials for a variety of engineering applications ranging from better catalysts, to solar cells to thermoelectrics used for refrigeration. The big hurdle to doing this is that these methods for solving the fundamental equation of quantum mechanics, known as the Schroedinger equation, are either not sufficiently accurate or are too computationally expensive. The purpose of this proposal is to develop software for a class of stochastic techniques, called quantum Monte Carlo methods, that will allow scientists and engineers to solve the equation more accurately and thereby predict materials properties with greater accuracy. The software uses a new programming paradigm to make the use as well as the extension of the software by developers easier. If researchers find this programming paradigm useful, it may propagate to a much larger research community and may eventually become an integral part of a new programming language. Thus this project will contribute to the basic sciences and potentially contribute to the field of computer science as well.&lt;br/&gt;&lt;br/&gt;The goal of this proposal is to develop a software package consisting of a variety of quantum Monte Carlo (QMC) analysis tools. The software will have the capability to compute static and dynamic response properties, thereby greatly enhancing the range of applicability of QMC methods. Past research on optimizing many-body wave functions will aid in this goal since some of the formalism is the same. Both real space QMC methods, namely variational Monte Carlo (VMC) and diffusion Monte Carlo (DMC), and, determinant space QMC methods, namely semistochastic QMC (SQMC), which is an extension of the full configuration interaction QMC (FCIQMC) method, will be included, since each of these methods is the method of choice for some set of applications. The software uses the Minimal Explicit Dependency (MED) programming paradigm that greatly facilitates the development of complex programs. In particular, it lowers the barrier for researchers, who are not familiar with the inner workings of the program to contribute new functionality, thereby contributing to the long term survival of the software.", "ProgramReferenceCode": ["7237", "7433", "7569", "8005", "8400", "9215", "9216"], "OrganizationZip": "148502820"}, {"OrganizationCity": "La Jolla", "OrganizationState": "CA", "Title": "SI2-SSE: Fingerprinting Scientific Codes to Verify and Create Compatible System Software Environments", "ProgramManager": "Daniel Katz", "State": "CA", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "04/01/2012", "OrganizationStreet": "Office of Contract & Grant Admin", "EndDate": "03/31/2016", "PrincipalInvestigator": "Philip Papadopoulos", "ARRAAmount": null, "OrganizationPhone": "8585344896", "AwardNumber": "1148473", "PIEmailAddress": "phil@sdsc.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "04/11/2012", "Organization": "University of California-San Diego", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "500000", "Abstract": "Every computational science application requires supporting software (code libraries, data files, other programs) to run properly. However, the computer systems that complex scientific applications run on are continuously updated by administrators to add new capabilities, fix bugs, and plug security holes. These are necessary changes, but a user needs to know if &quot;something changed in the system that will break his/her code&quot;. A similar question arises when running a code on a different computer from which it was compiled or developed: &quot;does the new system have all the necessary support software to run my applications?&quot; This proposal describes a plan for developing the software capability to both statically and dynamically &quot;fingerprint&quot; a scientific application to answer those two questions and therefore test for compatibility of new system or determine if a new update could have deleterious effects. Our proposed software system will have five major components: fingerprint format, static analysis of an application to create one type of fingerprint, a system verifier that checks if a system satisfies the needs defined in a fingerprint, dynamic analysis to find applications dependencies only detectable at run time, and a composer capability to automatically define a Rocks cluster managed system that will fulfill the requirements of a specific fingerprint. All software developed will be open-source and freely available.&lt;br/&gt;&lt;br/&gt;Nearly every person who uses a computer has seen messages similar to &quot;new updates have been installed, you must restart your computer.&quot; Sometimes, applications break because of these updates. This proposal&apos;s broader impact is that, for a wide variety of scientific applications, we will be able to determine if a new system or an existing system is compatible by verifying the code&apos;s fingerprint. Since, scientific applications can be very sensitive to underlying software changes, it will be possible to detect incompatibility before wasting time and energy to run a large-scale application on an production systems. When these applications are used as the basis of scientific discovery, it becomes even more imperative that we view the computing environment and application together as an experimental apparatus whose configuration we need to better understand. We believe that the composer capability of this proposal can have the transformational impact by developing fully- descriptive catalogs of what an application needs to function properly. In the era in which simulation output is being used to drive policy, this kind of scientific reproducibility has impact well beyond the notions of academic completeness.", "ProgramReferenceCode": ["8005", "8004"], "OrganizationZip": "920930934"}, {"OrganizationCity": "La Jolla", "OrganizationState": "CA", "Title": "Collaborative Research: SI2-SSI: A Comprehensive Performance Tuning Framework for the MPI Stack", "ProgramManager": "Rajiv Ramnath", "State": "CA", "Program": ["INFORMATION TECHNOLOGY RESEARC", "Software Institutes"], "ProgramElementCode": ["1640", "8004"], "StartDate": "06/01/2012", "OrganizationStreet": "Office of Contract & Grant Admin", "EndDate": "05/31/2016", "PrincipalInvestigator": "Amitava Majumdar", "ARRAAmount": null, "OrganizationPhone": "8585344896", "AwardNumber": "1147926", "PIEmailAddress": "majumdar@sdsc.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "06/04/2012", "Organization": "University of California-San Diego", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "450772", "Abstract": "The Message Passing Interface (MPI) is a very widely used parallel programming model on modern High-End Computing (HEC) systems. Many performance aspects of MPI libraries, such as latency, bandwidth, scalability, memory footprint, cache pollution, overlap of computation and communication etc. are highly dependent on system configuration and application requirements. Additionally, modern clusters are changing rapidly with the growth of multi-core processors and commodity networking technologies such as InfiniBand and 10GigE/iWARP. They are becoming diverse and heterogeneous with varying number of processor cores, processor speed, memory speed, multi-generation network adapters/switches, I/O interface technologies, and accelerators (GPGPUs), etc. Typically, any MPI library deals with the above kind of diversity in platforms and sensitivity of applications by employing various runtime parameters. These parameters are tuned during its release, or by&lt;br/&gt;system administrators, or by end-users. These default parameters may or may not be optimal for all system configurations and applications.&lt;br/&gt;&lt;br/&gt;The MPI library of a typical proprietary system goes through heavy performance tuning for a range of applications. Since commodity clusters provide greater flexibility in their configurations (processor, memory and network), it is very hard to achieve optimal tuning using released version of any MPI library, with its default settings. This leads to the following broad challenge: &quot;Can a comprehensive performance tuning framework be designed for MPI library so that the next generation InfiniBand, 10GigE/iWARP and RoCE clusters and applications will be able to extract `bare-metal&apos; performance and maximum scalability?&quot; The investigators, involving computer&lt;br/&gt;scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) as well as computational scientists from the Texas Advanced Computing Center (TACC) and San Diego Supercomputer Center (SDSC), University of California San Diego (UCSD), will be addressing the above challenge with innovative solutions.&lt;br/&gt;&lt;br/&gt;The investigators will specifically address the following challenges: 1) Can a set of static tools be designed to optimize performance of an MPI library during installation time? 2) Can a set of dynamic tools with low overhead be designed to optimize performance on a per-user and per-application basis during production runs? 3) How to incorporate the proposed performance tuning framework with the upcoming MPIT interface? 4) How to configure MPI libraries on a given system to deliver different optimizations to a set of driving applications? and 5) What kind of benefits (in terms of performance, scalability, memory efficiency and reduction in cache pollution) can be achieved by the proposed tuning framework? The research will be driven by a set of applications from established NSF computational science researchers running large scale simulations on the TACC Ranger and other systems at OSC, SDSC and OSU. The proposed designs will be integrated into the open-source MVAPICH2 library.", "ProgramReferenceCode": ["7433", "8009", "1640", "8004"], "OrganizationZip": "920930934"}, {"OrganizationCity": "Menlo Park", "OrganizationState": "CA", "Title": "SI2-SSE: Synthesizing Self-Contained Scientific Software", "ProgramManager": "Daniel Katz", "State": "CA", "Program": ["SPECIAL PROJECTS - CCF", "Software Institutes"], "ProgramElementCode": ["2878", "8004"], "StartDate": "10/01/2014", "OrganizationStreet": "333 RAVENSWOOD AVE", "EndDate": "09/30/2017", "PrincipalInvestigator": "Ashish Gehani", "ARRAAmount": null, "OrganizationPhone": "6508592651", "AwardNumber": "1440800", "PIEmailAddress": "ashish.gehani@sri.com", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/08/2014", "Organization": "SRI International", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "499919", "Abstract": "Computational aspects of scientific experiments have been growing steadily. This creates an increasing need to be able to reproduce the results. Science is also increasingly performed by exploring diverse sets of data. Unsurprisingly, there is a demand for being able to easily repeat the numerous transformations performed. Software packaged with tools from this project will allow scientists to publish their code in a form that can be utilized by others with minimal effort. By eliminating many of the challenges of building, configuring, and running software, it will allow members of the scientific community to more easily reproduce each others&apos; computational results.&lt;br/&gt;&lt;br/&gt;Increasingly, entire virtual machines are published to ensure that a recipient does not have to replicate the compute environment, retrieve data and code dependencies, or invest effort into configuring the system. However, this approach scales poorly with the growth in size of the included data sets, the extraneous functionality in applications that utilize versatile software libraries, and the irrelevant code in stock operating system distributions. This project will design, develop, and evaluate a toolchain that allows scientists to transform their software into specialized applications with all the necessary environmental conditions and portions of required data sets built directly into the code. The resulting scientific appliances can be distributed for others to explore and verify results without the overhead of shipping extraneous data and code.", "ProgramReferenceCode": ["7433", "8004", "8005", "2878"], "OrganizationZip": "940253493"}, {"OrganizationCity": "La Jolla", "OrganizationState": "CA", "Title": "SI2-SSE: Petascale Enzo: Software Infrastructure Development and Community Engagement", "ProgramManager": "Daniel Katz", "State": "CA", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", null, "Software Institutes"], "ProgramElementCode": ["1253", "1798", "8004"], "StartDate": "09/01/2014", "OrganizationStreet": "Office of Contract & Grant Admin", "EndDate": "08/31/2017", "PrincipalInvestigator": "Michael Norman", "ARRAAmount": null, "OrganizationPhone": "8585344896", "AwardNumber": "1440709", "PIEmailAddress": "mlnorman@ucsd.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/13/2014", "Organization": "University of California-San Diego", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "500000", "Abstract": "The purpose of this project is to develop an astrophysics and cosmology software application &quot;Enzo-P&quot;, built on the highly scalable parallel adaptive mesh refinement (AMR) software framework &quot;Cello&quot; that is being developed concurrently. The Enzo-P application will be capable of running extreme scale numerical simulations to investigate frontier questions in star formation, molecular cloud turbulence, interstellar medium dynamics, galaxy formation, intergalctic medium, formation of the first stars and galaxies, galaxy clusters, and cosmic reionization. This new software will empower the current large and diverse Enzo user/developer community to take full advantage of current and future high performance computer (HPC) systems. The Cello AMR framework can be used independently of Enzo-P, thus enabling researchers in other diverse scientific fields to develop AMR applications capable of running on &quot;Petascale-and-beyond&quot; HPC platforms. &lt;br/&gt;&lt;br/&gt;The novel approach used for Cello is to implement a &quot;forest-of-octree&quot; AMR scheme using the Charm++ parallel programming system. Octree-based AMR has been shown to be among the highest scaling AMR approaches, with demonstrated scaling to over 200K CPU cores. The Charm++ object-oriented parallel programming language supports data-driven asynchronous execution, is inherently latency-tolerant and automatically overlaps computation with communication, and provides support for developing Exascale applications, including in-memory distributed checkpointing and sophisticated dynamic load balancing schemes. Enzo-P development will be directed by the vibrant Enzo open development community, who will migrate Enzo&apos;s self-gravity, cosmology, chemistry and cooling, MHD, and radiation hydrodynamics capabilities to use the Cello scalable AMR framework.", "ProgramReferenceCode": ["1206", "7433", "8005"], "OrganizationZip": "920930934"}, {"OrganizationCity": "CHARLOTTE", "OrganizationState": "NC", "Title": "SI2-SSE: Collaborative Research: TrajAnalytics: A Cloud-Based Visual Analytics Software System to Advance Transportation Studies Using Emerging Urban Trajectory Data", "ProgramManager": "Daniel Katz", "State": "NC", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "09/01/2015", "OrganizationStreet": "9201 University City Boulevard", "EndDate": "08/31/2018", "PrincipalInvestigator": "Jing Yang", "ARRAAmount": null, "OrganizationPhone": "7046871888", "AwardNumber": "1535081", "PIEmailAddress": "Jing.Yang@uncc.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/05/2015", "Organization": "University of North Carolina at Charlotte", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "200950", "Abstract": "Advanced technologies in sensing and computing have created urban trajectory datasets of humans and vehicles travelling over urban networks. Understanding and analyzing the large-scale, complex data reflecting city dynamics is of great importance to enhance both human lives and urban environments. Domain practitioners, researchers, and decision-makers need to manage, query and visualize such big and dynamics data to conduct both exploratory and analytical tasks. To support these tasks, this project develops a open source software tool, named TrajAnalytics, which integrates computational techniques of scalable trajectory database, intuitive and interactive visualization, and high-end computers, to explicitly facilitate interactive data analytics of the emerging trajectory data.&lt;br/&gt;&lt;br/&gt;The software provides a new platform for researchers in transportation assessment and planning to directly study moving populations in urban spaces. Researchers in social, economic and behavior sciences can use the software to understand the complicated mechanisms of urban security, economic activities, behavior trends, and so on. Specifically, this software advances the research activities in the community of transportation studies. The software also acts as an outreach platform allowing government agencies to communicate more effectively with the public with the real-world dynamics of city traffic, vehicles, and networks. The integration of research and education prepares the next generation workforce, where students across multi-disciplines can benefit through their participation in the development and use of the software.&lt;br/&gt;&lt;br/&gt;In order to lay the foundations for effective analysis of urban trajectory datasets, this software (1) facilitates easy access and unprecedented capability for researchers and analysts with a cloud-based storage and computing infrastructure; (2) develops a parallel graph based data model, named TrajGraph, where massive trajectories are managed on a large-scale graph created from urban networks which is naturally amenable for studying urban dynamics; and (3) provides a visualization interface, named TrajVis, which supports interactive visual analytics tasks with a set of visualization tools and interaction functions. A variety of transportation-related researchers from geography, civil engineering, computer science participate in the design, evaluation, and use of the software. This robust and easy-to-use software enables the users to conduct iterative, evolving information foraging and sense making.", "ProgramReferenceCode": ["7433", "8005"], "OrganizationZip": "282230001"}, {"OrganizationCity": "Columbus", "OrganizationState": "OH", "Title": "SI2-SSE: Open OnDemand: Transforming Computational Science through Omnidisciplinary Software Cyberinfrastructure", "ProgramManager": "Daniel Katz", "State": "OH", "Program": ["SPECIAL PROJECTS - CCF", "Software Institutes"], "ProgramElementCode": ["2878", "8004"], "StartDate": "07/01/2015", "OrganizationStreet": "Office of Sponsored Programs", "EndDate": "06/30/2018", "PrincipalInvestigator": "David Hudak", "ARRAAmount": null, "OrganizationPhone": "6146888735", "AwardNumber": "1534949", "PIEmailAddress": "dhudak@osc.edu", "NSFDirectorate": "CSE", "Co-PIName": "Douglas Johnson", "LastAmendmentDate": "08/25/2015", "Organization": "Ohio State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "500000", "Abstract": "Supercomputing, or High-Performance Computing (HPC), has the power to advance work in all fields of science and engineering. Unfortunately, the impact of HPC is often limited simply because the computers themselves are difficult to use and scientists and engineers would rather spend their time advancing their disciplines than learn HPC. Learning esoteric commands is a hurdle to many students and researchers when they first begin to work with traditional HPC systems, which has contributed to the relatively small proportions of women, minorities, and persons with disabilities within related STEM fields. The Open OnDemand project addresses this problem through innovative software that makes HPC no more difficult than using a desktop computer or a web site, hence reducing that initial learning curve. Open OnDemand will provide an enhanced infrastructure for research and education, in that students and educators will be exposed to the same tools and techniques on smaller departmental and classroom systems that they&apos;ll eventually utilize at larger HPC centers. This streamlines the pipeline for developing a more globally competitive STEM workforce that is prepared to dive right into computational problems and resources once they graduate from academia. Open OnDemand also allows scientists and engineers to make specialized domain-specific workflows and expertise available to more collaborators and users, both from academia and industry. This should result in increased partnerships, and the transfer of more technologies from the public to private sectors.&lt;br/&gt;&lt;br/&gt;The web has become the dominant access mechanism for remote compute services in every computing area except high-performance computing (HPC). Accessing HPC cyberinfrastructure (CI) resources today, either at the campus or national level, typically requires advanced knowledge of UNIX, familiarity with command-line interfaces and installation and configuration of custom client software. Web applications in HPC today do exist in the form of science gateways. However, gateways have not proliferated in part due to the development and administrative overheads required for each individual gateway. These factors demonstrate an accessibility gap for HPC. Open OnDemand is an open source platform for HPC and remote computing access that addresses the accessibility gap. Open OnDemand will be a public release based on the successful OSC OnDemand platform. OSC OnDemand is a web platform providing Ohio Supercomputer Center (OSC) users integrated access to HPC systems, web applications, and VNC services. OSC OnDemand has been in production since January 2013, has over 800 distinct users from 27 different NSF fields of science, and its apps have been launched over 70,000 times. In addition to easing access to HPC services, Open OnDemand centralizes web app overheads, easing support for custom visualization and science gateway apps. Open OnDemand has the ability to transform cyberinfrastructure by providing a platform to enable a new delivery method for scientific web tools like HubZero apps, XSEDE Science Gateways, iPython notebooks and workflow tools like Pegasus. &lt;br/&gt;&lt;br/&gt;Under this project, the Open OnDemand platform will be created by (a) transitioning the existing OSC OnDemand software to a community developed project hosted on GitHub, (b) extending the per-user web server to serve Rails apps, (c) replacing the custom-developed Proxy with an existing open source project like NGINX and (d) replacing the existing Java-based VNC client with an HTML5 solution. File usage will be improved by updating or replacing AjaXplorer and integrating high performance file transfer functions (sftp and Globus Online). The accessibly apps will be updated by by (a) upgrading or replacing AnyTerm, (b) updating Job Constructor and (c) integrating Open XDMoD and SUPREMM for job and cluster performance metrics. In addition, community infrastructure will be created including (a) system administrator documentation and discussion forums and (b) training materials based on existing OSC OnDemand materials. A beta program will be conducted including (a) assisting beta sites with installs, (b) updating training materials to include new functionality, (c) beginning metric reporting and (d) documenting a Galaxy case study for app integration. HPC center staff will be engaged by through the &quot;boot camps&quot; and &quot;train the trainer&quot; sessions. Finally, Open OnDemand will be proposed as a project to the XSEDE Campus Bridging Technology Insertion Service. These activities will help meet the following objectives: (1) Transition OSC OnDemand to a community-developed open source software package called Open OnDemand, (2) Improve the interface capabilities of Open OnDemand by updating and expanding the accessibility apps, including integration of Globus Online and Open XDMoD projects. (3) Conduct a program to engage departmental, campus and national HPC users and administrators on enhancing HPC inclusivity through Open OnDemand and (4) Leverage Open OnDemand as a platform to support existing web-based applications such XSEDE Science Gateways and HubZero applications based on our experience supporting a data-intensive biomedical package (the Galaxy Project).", "ProgramReferenceCode": ["7433", "8005"], "OrganizationZip": "432101016"}, {"OrganizationCity": "La Jolla", "OrganizationState": "CA", "Title": "SI2-SSE: AMASS - An Automated Monitoring AnalySis Service for Cyberinfrastructure", "ProgramManager": "Daniel Katz", "State": "CA", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "09/01/2014", "OrganizationStreet": "Office of Contract & Grant Admin", "EndDate": "08/31/2017", "PrincipalInvestigator": "Shava Smallen", "ARRAAmount": null, "OrganizationPhone": "8585344896", "AwardNumber": "1440769", "PIEmailAddress": "ssmallen@sdsc.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Lawrence Saul", "Sameer Tilak"], "LastAmendmentDate": "12/03/2014", "Organization": "University of California-San Diego", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "515205", "Abstract": "A science gateway is a community-developed set of tools, applications, and data collections that are integrated via a portal or a suite of applications. It provides easy, typically browser-based, access to supercomputers, software tools, and data repositories to allow researchers to focus on their scientific goals and less on the cyberinfrastructure. These gateways are fostering collaboration and exchange of ideas among thousands of researchers from multiple communities ranging from atmospheric science, astrophysics, chemistry, biophysics, biochemistry, earthquake engineering, geophysics, to neuroscience, and biology. However due to limited development and administrative personnel resources, science gateways often leverage only a small subset of the NSF-funded CI to mitigate the complexities involved with using multiple resource and services at scale in part due to software and hardware failures. Since many successful science gateways have had unprecedented growth in their user base and ever increasing datasets, increasing their usage of CI resources without introducing additional complexity would help them meet this demand.&lt;br/&gt;&lt;br/&gt;In response to this need, an Automated Monitoring AnalySis Service (AMASS) will be built to provide a flexible and extensible service for automated analysis of monitoring data initially focused on science gateways. AMASS will be based on data mining and machine learning techniques and emerging big data technologies to analyze monitoring data for improving the reliability and operational efficiency of CI as well as progress on fundamental questions in systematic and population biology, computational neuroscience, and biophysics communities. Along with AMASS, a simulation framework will be built for testing automated analysis algorithms and adaptive execution techniques. An intuitive query API will be provided for science gateway software to use and will be integrated into the following three target science gateways that will drive the project&apos;s research and development: the Cyberinfrastructure for Phylogenetic Research (CIPRES), the Neuroscience Gateway (NSG), and UltraScan. The proposed approach does not require any changes to the end user applications, and the software developments will significantly enhance the science productivity and user satisfaction of science gateways by integrating monitoring data into their infrastructure to enable adaptive execution of their applications, allowing scientists to answer more sophisticated questions without having to understand the complexities of a large-scale distributed environment. The developed software products will be available as open source products under an Apache License and will be integrated into the NSF-funded SciGap project in order to impact a broader range of science gateways.", "ProgramReferenceCode": ["7433", "8005", "9251"], "OrganizationZip": "920930934"}, {"OrganizationCity": "CHAPEL HILL", "OrganizationState": "NC", "Title": "Collaborative Research: SI2-SSI: An Interactive Software Infrastructure for Sustaining Collaborative Community Innovation in the Hydrologic Sciences", "ProgramManager": "Daniel Katz", "State": "NC", "Program": ["ADVANCES IN BIO INFORMATICS", "Software Institutes"], "ProgramElementCode": ["1165", "8004"], "StartDate": "07/01/2012", "OrganizationStreet": "104 AIRPORT DR STE 2200", "EndDate": "06/30/2017", "PrincipalInvestigator": "Ray Idaszak", "ARRAAmount": null, "OrganizationPhone": "9199663411", "AwardNumber": "1148090", "PIEmailAddress": "rayi@email.unc.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Lawrence Band", "Xiaohui Carol Song", "Venkatesh Merwade", "David Valentine"], "LastAmendmentDate": "07/16/2012", "Organization": "University of North Carolina at Chapel Hill", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "2128078", "Abstract": "Water, its quality, quantity, accessibility, and management, is crucial to society. However, our ability to model and quantitatively understand the complex interwoven environmental processes that control water and its availability is severely hampered by inadequate tools related to hydrologic data discovery, systems integration, modeling/ simulation, and education. This project develops sustainable cyberinfrastructure for better access to water-related data and models in the hydrologic sciences, enabling hydrologists and other associated communities to collaborate and combine data and models from multiple sources. It will provide new ways in which hydrologic knowledge is created and applied to better understand water availability, quality, and dynamics. It will also help to provide a more comprehensive understanding of the interactions between natural and engineered aspects of the water cycle. These goals will be achieved through the development of interoperable cyberinfrastructure tools and the creation of an online collaborative environment, called HydroShare, which enables scientists to easily discover and access hydrologic and related data and models, retrieve them to their desktop, and perform analyses in a high performance computing environment. The software to be developed will take advantage of existing NSF cyberinfrastructure (iRODS, HUBzero, CSDMS, CUAHSI HIS) and be created as open source code. Its development will be end user-driven. In terms of broader impacts, the project builds essential infrastructure for science by developing software tools and computing environments to allow better understanding of the impacts of climate change (i.e., floods, droughts, biofuels, etc.) and to allow improved water resource development and the management of freshwater resources both above and below ground. Resulting software will be made publicly available and provides a strong student and workforce training/education component. In addition, the project supports an institution in an EPSCoR state and engages, as a PI, a person who is from a group under-represented in the sciences and engineering.", "ProgramReferenceCode": ["7433", "8009", "1165", "8004"], "OrganizationZip": "275991350"}, {"OrganizationCity": "La Jolla", "OrganizationState": "CA", "Title": "SI2-SSE: Wavelet Enabled Progressive Data Access and Storage Protocol (WASP)", "ProgramManager": "Daniel Katz", "State": "CA", "Program": ["ADVANCES IN BIO INFORMATICS", "PHYSICAL & DYNAMIC METEOROLOGY", "Software Institutes", "EarthCube"], "ProgramElementCode": ["1165", "1525", "8004", "8074"], "StartDate": "10/01/2014", "OrganizationStreet": "Office of Contract & Grant Admin", "EndDate": "09/30/2017", "PrincipalInvestigator": "Lawrence Frank", "ARRAAmount": null, "OrganizationPhone": "8585344896", "AwardNumber": "1440412", "PIEmailAddress": "lfrank@ucsd.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "07/25/2014", "Organization": "University of California-San Diego", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "500000", "Abstract": "Advances in digital imaging methods are revolutionizing a wide range of scientific disciplines by facilitating the acquisition of huge amounts of data that allow the visualization and analysis of complex, multidimensional images. Concurrently, modern computing technologies enable numerical modeling of a broad gamut of scientific phenomena, resulting in vast quantities of numerical data, which are just the starting point for the scientific exploration that modern computational and visualization methods enable. This is particularly true in the biological and geosciences, two seemingly very different disciplines. These capabilities come with a cost: increasing data size and complexity require more sophisticated methods for data analysis and visualization. This project will conduct research that will lead to a common software framework for supporting a multi scale progressive data refinement method based upon the representation of the data as a wavelet expansion, and enabling interactive exploration of large data sets for the bio and geoscience communities. The development of a general toolkit for wavelet based representations of data will have broad impact, allowing the multi scale analysis, storage, and visualization for data collected in a wide range of fields and on a multitude of platforms, from high end computing facilities to laptop computers used by students, field biologists, and others.&lt;br/&gt;&lt;br/&gt;Analysis and visualization of large data sets play an important role in scientific discovery. Efficient, and broadly available tools to accomplish these tasks are crucial for a wide range of scientific and educational fields.&#160; However, efficient analysis and visualization is a non trivial problem as the size and complexity of data increases. This research addresses this challenge through a general progressive access, multi scale data representation for efficient handling of structured data sets across a range of science domains. The development is based upon a wavelet enabled data representation developed by NCAR for geoscience applications.&#160; The tools will utilize the very flexible and open source standard NetCDF format, and the methods will be documented as a set of conventions and a toolkit developed that incorporates and integrates these components for dissemination. In addition to an open source toolkit, these tools will be integrated into the VAPOR (NCAR) and STK (CSCI) platforms, thus expanding the capabilities and efficiencies of these platforms for the geo and bio sciences communities, respectively. &#160; Advancements generated by this project will be openly disseminated to the user community through an open source toolkit.", "ProgramReferenceCode": ["4444", "7433", "8005"], "OrganizationZip": "920930934"}, {"OrganizationCity": "CHAPEL HILL", "OrganizationState": "NC", "Title": "SI2-SSI: Collaborative Research: Scalable Infrastructure for Enabling Multiscale and Multiphysics Applications in Fluid Dynamics, Solid Mechanics, and Fluid-Structure Interaction", "ProgramManager": "Rajiv Ramnath", "State": "NC", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes", "CDS&E-MSS", "CDS&E"], "ProgramElementCode": ["1253", "8004", "8069", "8084"], "StartDate": "08/01/2015", "OrganizationStreet": "104 AIRPORT DR STE 2200", "EndDate": "07/31/2020", "PrincipalInvestigator": "Boyce Griffith", "ARRAAmount": null, "OrganizationPhone": "9199663411", "AwardNumber": "1450327", "PIEmailAddress": "boyceg@email.unc.edu", "NSFDirectorate": "CSE", "Co-PIName": "Robert O'Bara", "LastAmendmentDate": "08/11/2015", "Organization": "University of North Carolina at Chapel Hill", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "924376", "Abstract": "Many biological and biomedical systems involve the interaction of a flexible structure and a fluid. These systems range from the writhing and coiling of DNA, to the beating and pumping of cilia and flagella, to the flow of blood in the body, to the locomotion of fish, insects, and birds. This project aims to develop advanced software infrastructure for performing dynamic computer simulations of such biological and biomedical systems. To facilitate the deployment of this software in a range of scientific and engineering applications, this project will develop new software capabilities in concert with new computer models that use the software. Specific application domains to be advanced in this project include models of aquatic locomotion that can be used to understand the neural control of movement and ultimately to develop new treatments for neurological pathologies such as spinal cord injuries, and models that simulate the interaction between the electrophysiology of the heart and the contractions of the heart that pump blood throughout the body, which could lead to improved approaches to treating heart disease. The software to be developed within the project is freely available online and is used by a number of independent research groups in a variety of scientific and engineering domains. It is being actively used in projects that model different aspects of cardiovascular dynamics, such as platelet aggregation and the dynamics of natural and prosthetic heart valves, and in projects that study other biological problems, including cancer dynamics, insect flight, aquatic locomotion, and the dynamics of phytoplankton. The software is also being applied to non-biological problems, including nanoscale models of colloidal suspensions and models of active particles. The improved methods and software to be developed in this project will thereby have a broad and sustained impact on a large number of ongoing research efforts in the biological and biomedical sciences and other scientific and engineering disciplines.&lt;br/&gt;&lt;br/&gt;The immersed boundary (IB) method is a broadly applicable framework for modeling and simulating fluid-structure interaction (FSI). The IB method was introduced to model the fluid dynamics of heart valves, and subsequent development initially focused on simulating cardiac fluid dynamics. This methodology is broadly useful, however, and has been applied to a variety of problems in which a fluid flow interacts with immersed structures, including elastic bodies, bodies with known or prescribed deformational kinematics, and rigid bodies. Extensions of the IB method have also been developed to model electrophysiological systems and systems with chemically active structures. To improve the efficiency of the IB method, the PI has developed adaptive versions of the IB method that employ structured adaptive mesh refinement (AMR) to deploy high spatial resolution only where needed. These methods have been implemented within the IBAMR software framework, which provides parallel implementations of the IB method and its extensions that leverage high-quality computational libraries including SAMRAI, PETSc, and libMesh. This project will further extend the IBAMR software by implementing modeling and discretization technologies required by the research applications of current and prospective users of the software, by developing improved solver infrastructure facilitated by the implementation of native support for structured AMR discretizations in the PETSc library, and by integrating with existing high-quality software tools for model development, deployment, and analysis. IBAMR is freely distributed online and is used within a number of independent research groups both to the further development of the IB method and also to its application to simulate diverse problems in fluid dynamics and FSI. By enhancing IBAMR, this project will also enhance the ability of these and other researchers to construct detailed models without requiring those researchers to develop the significant software infrastructure needed to perform such simulations. This project will also develop general-purpose support for AMR discretizations in PETSc, a software library with thousands of active users, ~400 downloads per month, and numerous applications. The work of this project will help to grow the IBAMR user community of students and researchers by developing UI tools for building models, running simulations, and analyzing results. Students will be actively engaged in all aspects of the project, including code, method, and model development.", "ProgramReferenceCode": ["7433", "8004", "8009", "8084"], "OrganizationZip": "275991350"}, {"OrganizationCity": "KNOXVILLE", "OrganizationState": "TN", "Title": "SI2-SSI: Collaborative Research: Sustained Innovation for Linear Algebra Software (SILAS)", "ProgramManager": "Daniel Katz", "State": "TN", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "SPECIAL PROJECTS - CCF", "Software Institutes", "CDS&E-MSS"], "ProgramElementCode": ["1253", "2878", "8004", "8069"], "StartDate": "10/01/2013", "OrganizationStreet": "1 CIRCLE PARK", "EndDate": "09/30/2016", "PrincipalInvestigator": "Jack Dongarra", "ARRAAmount": null, "OrganizationPhone": "8659743466", "AwardNumber": "1339822", "PIEmailAddress": "dongarra@icl.utk.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/25/2015", "Organization": "University of Tennessee Knoxville", "AwardInstrument": "Continuing grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1427955", "Abstract": "As the era of computer architectures dominated by serial processors comes to a close, the convergence of several unprecedented changes in processor design has produced a broad consensus that much of the essential software infrastructure of computational science and engineering is utterly obsolete. Math libraries have historically been in the vanguard of software that must be quickly adapted to such design revolutions because they are the common, low-level software workhorses that do all the most basic mathematical calculations for many different types of applications. The Sustained Innovation for Linear Algebra Software (SILAS) project updates two of the most widely used numerical libraries in the history of Computational Science and Engineering---LAPACK and ScaLAPACK, (abbreviated Sca/LAPACK)---enhancing and hardening them for this ongoing revolution in processor architecture and system design. SILAS creates a layered package of software components, capable of running at every level of the platform deployment pyramid, from the desktop to the largest supercomputers in the world. It achieves three complementary objectives: 1) Wherever possible, SILAS delivers seamless access to the most up-to-date algorithms, numerical implementations, and performance, by way of Sca/LAPACK programming interfaces that are familiar to many computational scientists; 2) Wherever necessary, SILAS makes advanced algorithms, numerical implementations and performance capabilities available through new interface extensions; and 3) SILAS provides a well engineered conduit through which new discoveries at the frontiers of research in these areas can be channeled as quickly as possible to all the application communities that depend on high performance linear algebra. The improvements and innovations included in SILAS derive from a variety of sources. They represent the results (including designs and well tested prototypes) of the PIs&apos; own algorithmic and software research agenda, which has targeted multicore, hybrid and extreme scale system architectures. They are an outcome of extensive and on-going interactions with users, vendors, and the management of large NSF and DOE supercomputing facilities. They flow from cross-disciplinary engagement with other areas of computer science and engineering, anticipating the demands and opportunities of new architectures and programming models. And finally, they come from the enthusiastic participation of the research community in developing and offering enhanced versions of existing Sca/LAPACK codes.&lt;br/&gt;&lt;br/&gt;The primary impact of SILAS is a direct function of the importance of the Sca/LAPACK libraries to many branches of computational science. The Sca/LAPACK libraries are the community standard for dense linear algebra and have been adopted and/or supported by a large community of users, computing centers, and HPC vendors. Learning to use them is a basic part of the education of a computational scientist or engineer in many fields and at many academic institutions. Application domains where Sca/LAPACK have historically been heavily used include (among a host of other examples) airplane wing design, radar cross-section studies, flow around ships and other off-shore constructions, diffusion of solid bodies in a liquid, noise reduction, and diffusion of light through small particles. Moreover, the list of application partners working with SILAS to enhance and transform these libraries for next generation platforms expands this traditional list to include quantum chemistry, adaptive mesh refinement schemes, computational materials science, geophysical flows, stochastic simulation and database research for &quot;big data&quot;. No other numerical library can claim this breadth of integration with the community. Thus, there is every reason to believe that enhancing these libraries with state of the art methods and algorithms and adapting them for new and emerging platforms (reaching up to extreme scale), will have a correspondingly large impact on the research and education community, government laboratories, and private industry.", "ProgramReferenceCode": ["7433", "8009", "9150"], "OrganizationZip": "379960003"}, {"OrganizationCity": "RALEIGH", "OrganizationState": "NC", "Title": "SI2-SSE: Enhanced Software Tools for Biomolecular Free Energy Calculations", "ProgramManager": "Daniel Katz", "State": "NC", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "DMR SHORT TERM SUPPORT", "Software Institutes", "DMREF"], "ProgramElementCode": ["1253", "1712", "8004", "8292"], "StartDate": "09/01/2015", "OrganizationStreet": "CAMPUS BOX 7514", "EndDate": "08/31/2018", "PrincipalInvestigator": "Celeste Sagui", "ARRAAmount": null, "OrganizationPhone": "9195152444", "AwardNumber": "1534941", "PIEmailAddress": "sagui@ncsu.edu", "NSFDirectorate": "CSE", "Co-PIName": "Christopher Roland", "LastAmendmentDate": "08/24/2015", "Organization": "North Carolina State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "500000", "Abstract": "The central role of atomistic simulation-based free energy calculations for basic chemical and biological research is now firmly established. The free energy is the central quantity that guides the behavior of a system at, or near, equilibrium, determining such characteristics as molecular conformations, molecular binding, chemical reactions, etc. Unfortunately, accurate and reliable free energies are very difficult to calculate, particularly for many biomolecular systems characterized by rugged free energy landscapes. Hence, special techniques are required for calculating such free energy landscapes. Having previously developed the so-called Adaptively Biased Molecular Dynamics method, enhanced with other methods (either developed or adapted by our group) for phase space sampling, this project will further develop the capabilities of this software thereby enlarging the kinds of simulation problems that can be tackled. The software will be released to the public as open source software and as parts of the AMBER software package. In terms of scientific applications, this project also will investigate the conformation and properties of proteins with Intrinsically Disordered Regions, and the binding of DNA with a special class of transcription factors.&lt;br/&gt;&lt;br/&gt;The single most important quantity for describing biomolecular and chemical systems in equilibrium is the free energy. However, calculating free energies is notoriously difficult and computationally expensive. This problem is particularly pressing for many biomolecular systems, which are characterized by complicated free energy landscapes that are hard to explore with regular molecular dynamics simulations. The PIs previously developed the Adaptively Biased Molecular Dynamics method (ABMD) with Multiple Walkers, and Replica Exchange Molecular Dynamics (REMD) extensions. ABMD is an umbrella sampling method with a time-dependent biasing potential for calculating free energy landscapes and conformational sampling. The software suite, along with Steered Molecular Dynamics (SMD) extensions, has been released to the public as part of the AMBER software package. This project will take this set of software tools to the next level by developing the capability to handle quaternion-based collective variables, the so-called Milestoning technique, and self-directed, interacting multiple walkers. Envisioned applications relate to proteins with Intrinsically Disordered Regions (IDRs) and mechanisms of DNA binding by basic Helix-Loop-Helix (bHLH) domains in transcription factors.", "ProgramReferenceCode": ["1982", "7237", "7433", "7569", "7573", "8005", "8400", "9215", "9216", "9263"], "OrganizationZip": "276957514"}, {"OrganizationCity": "Nashville", "OrganizationState": "TN", "Title": "SI2-SSE: Development of a Software Framework for Formalizing Forcefield Atom-Typing for Molecular Simulation", "ProgramManager": "Daniel Katz", "State": "TN", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "DMR SHORT TERM SUPPORT", "Software Institutes", "DMREF"], "ProgramElementCode": ["1253", "1712", "8004", "8292"], "StartDate": "09/01/2015", "OrganizationStreet": "Office of Sponsored Programs", "EndDate": "08/31/2018", "PrincipalInvestigator": "Christopher Iacovella", "ARRAAmount": null, "OrganizationPhone": "6158756070", "AwardNumber": "1535150", "PIEmailAddress": "christopher.r.iacovella@vanderbilt.edu", "NSFDirectorate": "CSE", "Co-PIName": "Janos Sallai", "LastAmendmentDate": "08/24/2015", "Organization": "Vanderbilt University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "501836", "Abstract": "Molecular simulation plays a key role in understanding the atomistic and molecular level interactions that underlie many natural and man-made materials and processes. Classical molecular simulations rely upon forcefields to describe the various interactions that exist between atoms and/or groups of atoms. The availability of forcefields for molecular simulation has reduced the effort researchers must devote to the difficult and costly task of determining the interactions between species, allowing them to instead focus on the motivating scientific questions. However, determining which parameters in a forcefield to use is still often a tedious and error prone task. This difficulty is related to the strong dependence of the parameters on the chemical context of the atoms; the chemical context may depend on the local bonded environment of an atom in a molecule, the local environment of neighboring atoms, the type of molecule(s) being considered, the phase of the molecule(s), etc. Forcefields can contain tens or hundreds of different types of the same element, where each type represents the element in a different chemical context. Atom typing can be challenging, often requiring the user to consult textual comments scattered in parameter files or the scientific literature where the parameters were published. Unfortunately, as of today, the documentation of a typical forcefield tends to be scarce and unstructured, commonly expressed in plain English or in an ad-hoc shorthand notation, leading to ambiguities and increasing the likelihood of incorrect usage. While there are freely available tools to aid in atom-typing, these are typically specific to a particular forcefield or simulator and capture the atom-typing and parameterization rules in ways that are hard to maintain, debug, and evolve. The central tenet of this project is that there is an imminent need in the research community for a forcefield agnostic formalism to express atom-typing and parameterization rules in a way that is expressive enough for human consumption, while being machine readable to enable automation in complex scientific workflows.&lt;br/&gt;&lt;br/&gt;This work proposes to establish a formalism to express the chemical context for which a particular forcefield parameter is applicable (i.e., forcefield usage semantics) and an atom-typing tool that interprets this formalism to generate forcefield parameterizations that are provably correct. Annotating forcefields with this formalism will serve as clear, unambiguous documentation of the atom-types and parameter usage, and also allows ambiguities or inconsistencies in forcefield specifications to be programmatically pinpointed during development. Successfully developing this framework will simplify the rules needed for atom-typing, which is crucial as forcefields continue to grow, specialize, and become more complex. The machine-readable annotations of forcefield usage semantics will enable automating tedious and error prone tasks and have the potential to enable new application areas, ranging from automated forcefield comparison and cross-validation, to complex simulation workflows integrating multiple forcefields and simulator tools. &#160;&#160;An open online forcefield repository containing the annotated forcefields, associated open source software, and documentation on how to use, annotate, and develop forcefields within the proposed framework will be developed to disseminate results and foster community involvement. &#160;", "ProgramReferenceCode": ["7433", "8005", "8400", "9150"], "OrganizationZip": "372122809"}, {"OrganizationCity": "Atlanta", "OrganizationState": "GA", "Title": "SI2-SSE: ShareSafe: A Framework for Researchers and Data Owners to Help Facilitate Secure Graph Data Sharing", "ProgramManager": "Daniel Katz", "State": "GA", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "09/01/2015", "OrganizationStreet": "Office of Sponsored Programs", "EndDate": "08/31/2018", "PrincipalInvestigator": "Raheem Beyah", "ARRAAmount": null, "OrganizationPhone": "4048944819", "AwardNumber": "1534872", "PIEmailAddress": "rbeyah@ece.gatech.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/19/2015", "Organization": "Georgia Tech Research Corporation", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "500000", "Abstract": "There is a critical need for information/data sharing to solve some of our most significant academic and societal problems. These data are increasing in size and are becoming much more complex; in many cases, they can be considered structured. An example of structured data is data describing disease propagation in a specific population. Widespread sharing of data can, among many other things, help corporations increase their revenues, help reduce the spread of communicable diseases, accelerate the cure of some of the most significant diseases, and enable reproducible experiments amongst researchers. Although there is little disagreement that sharing data has tremendous benefits, it is still not as widespread as it should be. This is, in part, due to privacy concerns with sharing datasets. This project will develop an open source system (ShareSafe) that allows data owners to evaluate the security (such as resistance to de-anonymization attacks) and utility of their anonymized datasets before release, which will help facilitate the data sharing process.&lt;br/&gt;&lt;br/&gt;The overarching goals of this project are to develop a software framework, ShareSafe, that (1) helps structured data owners (e.g., social network researchers, epidemiologists) evaluate the security (against modern de-anonymization attacks) and utility of their datasets when using simple and state-of-the-art anonymization techniques; and (2) to provide structured data security/privacy researchers a uniform platform to comprehensively study, evaluate, and compare existing/newly developed techniques for structured data utility and privacy. ShareSafe is a comprehensive, user-friendly framework with the following capabilities: ShareSafe will enable data owners to: (1) anonymize their datasets with all of the state-of-the art anonymization techniques; (2) measure the utility of anonymized datasets using state-of-the-art utility measurement techniques; (3) evaluate the practical security of their datasets by subjecting them to state-of-the-art de-anonymization attacks; and (4) evaluate the theoretical security of their datasets by subjecting them to state-of-the-art de-anonymization quantification (de-anonymizability analysis) techniques. Understanding the results from (2)-(4) allows data owners to determine which anonymization algorithm suits their needs when sharing datasets. Finally, the aforementioned techniques will be implemented in a uniform manner as open source software, allowing graph data security/privacy researchers the ability to comprehensively study, evaluate, and compare existing/newly developed techniques for graph data utility and privacy.", "ProgramReferenceCode": ["7433", "8005"], "OrganizationZip": "303320420"}, {"OrganizationCity": "KENT", "OrganizationState": "OH", "Title": "SI2-SSE: Collaborative Research: TrajAnalytics: A Cloud-Based Visual Analytics Software System to Advance Transportation Studies Using Emerging Urban Trajectory Data", "ProgramManager": "Daniel Katz", "State": "OH", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "09/01/2015", "OrganizationStreet": "OFFICE OF THE COMPTROLLER", "EndDate": "08/31/2018", "PrincipalInvestigator": "Ye Zhao", "ARRAAmount": null, "OrganizationPhone": "3306722070", "AwardNumber": "1535031", "PIEmailAddress": "zhao@cs.kent.edu", "NSFDirectorate": "CSE", "Co-PIName": "Xinyue Ye", "LastAmendmentDate": "08/05/2015", "Organization": "Kent State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "300000", "Abstract": "Advanced technologies in sensing and computing have created urban trajectory datasets of humans and vehicles travelling over urban networks. Understanding and analyzing the large-scale, complex data reflecting city dynamics is of great importance to enhance both human lives and urban environments. Domain practitioners, researchers, and decision-makers need to manage, query and visualize such big and dynamics data to conduct both exploratory and analytical tasks. To support these tasks, this project develops a open source software tool, named TrajAnalytics, which integrates computational techniques of scalable trajectory database, intuitive and interactive visualization, and high-end computers, to explicitly facilitate interactive data analytics of the emerging trajectory data.&lt;br/&gt;&lt;br/&gt;The software provides a new platform for researchers in transportation assessment and planning to directly study moving populations in urban spaces. Researchers in social, economic and behavior sciences can use the software to understand the complicated mechanisms of urban security, economic activities, behavior trends, and so on. Specifically, this software advances the research activities in the community of transportation studies. The software also acts as an outreach platform allowing government agencies to communicate more effectively with the public with the real-world dynamics of city traffic, vehicles, and networks. The integration of research and education prepares the next generation workforce, where students across multi-disciplines can benefit through their participation in the development and use of the software.&lt;br/&gt;&lt;br/&gt;In order to lay the foundations for effective analysis of urban trajectory datasets, this software (1) facilitates easy access and unprecedented capability for researchers and analysts with a cloud-based storage and computing infrastructure; (2) develops a parallel graph based data model, named TrajGraph, where massive trajectories are managed on a large-scale graph created from urban networks which is naturally amenable for studying urban dynamics; and (3) provides a visualization interface, named TrajVis, which supports interactive visual analytics tasks with a set of visualization tools and interaction functions. A variety of transportation-related researchers from geography, civil engineering, computer science participate in the design, evaluation, and use of the software. This robust and easy-to-use software enables the users to conduct iterative, evolving information foraging and sense making.", "ProgramReferenceCode": ["7433", "8005"], "OrganizationZip": "442420001"}, {"OrganizationCity": "Los Angeles", "OrganizationState": "CA", "Title": "SI2-SSI: Community Software for Extreme-Scale Computing in Earthquake System Science", "ProgramManager": "Rajiv Ramnath", "State": "CA", "Program": ["GEOPHYSICS", "GEOINFORMATICS", "Software Institutes", "EarthCube", "CDS&E"], "ProgramElementCode": ["1574", "7255", "8004", "8074", "8084"], "StartDate": "09/01/2015", "OrganizationStreet": "University Park", "EndDate": "08/31/2019", "PrincipalInvestigator": "Thomas Jordan", "ARRAAmount": null, "OrganizationPhone": "2137407762", "AwardNumber": "1450451", "PIEmailAddress": "tjordan@usc.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Kim Olsen", "Yifeng Cui", "Ricardo Taborda"], "LastAmendmentDate": "08/17/2015", "Organization": "University of Southern California", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "2200000", "Abstract": "The Software Environment for Integrated Seismic Modeling (SEISM) Project of the Southern California Earthquake Center (SCEC) will develop advanced earthquake simulation software capable of using high-performance computing to produce new information about earthquakes and the hazards they present. SCEC&apos;s SEISM project is developing an integrated, sustainable community software framework for earthquake system science to serve diverse communities of earthquake scientists and engineers, computer scientists, and at-risk stakeholders. The SEISM project is a collaboration among several diverse user communities with shared interests in reducing seismic risk and enhancing seismic resilience. SCEC SEISM researchers are addressing scientific problems that limit the accuracy and scale in current numerical representations of earthquake processes. SEISM computational improvements in seismic hazard calculations will benefit earthquake system science worldwide. The SCEC SEISM project will educate a diverse STEM workforce from the undergraduate to early-career level, and it will cross-train scientists and engineers in a challenging high-performance environment. As one application of SEISM, the researchers will develop new simulations for the Great California ShakeOut, which is engaging millions of people in earthquake preparedness exercises.&lt;br/&gt;&lt;br/&gt;Earthquake simulations at the spatiotemporal scales required for probabilistic seismic hazard analysis present some of the toughest computational challenges in geoscience, requiring extreme-scale computing. The Southern California Earthquake Center is creating a Software Environment for Integrated Seismic Modeling (SEISM) that will provide the extreme-scale simulation capability needed to transform probabilistic seismic hazard analysis into a physics-based science. This project will advance SEISM through a user-driven research and development agenda that will push validated SEISM capabilities to higher seismic frequencies and towards extreme-scale computing. It will develop an integrated, sustainable community software framework for earthquake system science to serve diverse communities of earthquake scientists and engineers, computer scientists and at-risk stakeholders. A new SEISM-T framework will support both in-situ and post-hoc data processing to make efficient use of available heterogeneous architectures. The main goal of the project is to increase the 4D outer-scale/inner-scale ratio of simulations at constant time-to-solution by two orders of magnitude above current capabilities. The software development plan will use an agile process of test-driven development, continuous software integration, automated acceptance test suites for each application, frequent software releases, and attention to user feedback. The researchers will take advantage of the SCEC Implementation Interface to develop a dialog among user communities regarding the application of SEISM to the reduction of seismic risk and enhancement of seismic resilience. This research will address fundamental scientific problems that limit the accuracy and scale range in current numerical representations of earthquake processes, which will benefit earthquake system science worldwide. This project will educate a diverse STEM workforce from the undergraduate to early-career level, and it will cross-train scientists and engineers in a challenging high-performance environment. As one application of SEISM, the project team will develop new simulations for the Great California ShakeOut, which is engaging millions of people in earthquake preparedness exercises.", "ProgramReferenceCode": ["7433", "8009", "8004"], "OrganizationZip": "900890001"}, {"OrganizationCity": "KNOXVILLE", "OrganizationState": "TN", "Title": "SI2-SSE: A Software Element for Neutrino Radiation Hydrodynamics in GenASiS", "ProgramManager": "Daniel Katz", "State": "TN", "Program": [null, "COMPUTATIONAL PHYSICS", "Software Institutes"], "ProgramElementCode": ["1798", "7244", "8004"], "StartDate": "09/01/2015", "OrganizationStreet": "1 CIRCLE PARK", "EndDate": "08/31/2018", "PrincipalInvestigator": "Reuben Budiardja", "ARRAAmount": null, "OrganizationPhone": "8659743466", "AwardNumber": "1535130", "PIEmailAddress": "reubendb@utk.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Eirik Endeve", "Christian Cardall", "Anthony Mezzacappa"], "LastAmendmentDate": "08/12/2015", "Organization": "University of Tennessee Knoxville", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "432488", "Abstract": "Multi-physics computational modeling is an integral part of the scientific study of many complex natural phenomena. These phenomena often involve the physics of radiation transport. For example, neutrino radiation hydrodynamics is a key element of the physics governing environments with hot and dense nuclear matter. Such extreme environments include the Early Universe, during primordial nucleosynthesis of light nuclei such as hydrogen through lithium just after the Big Bang; the merger through inspiraling of neutron star-neutron star or neutron star-black hole binaries; and the death throes of massive stars, more than ten times the mass of the Sun, in stellar explosions known as core-collapse supernovae, which are responsible for elements such as oxygen and calcium without which life as we know it would not exist. Radiation transport and kinetic theory of particles besides neutrinos---photons, electrons, or neutrons---are also relevant to many areas of astrophysics, as well as a broad range of other science applications, including materials science, plasma physics, neutron transport, multiphase flows, and high-energy-density physics. As such, the availability of a software element to solve radiation transport problems is highly valuable to researchers.&lt;br/&gt;&lt;br/&gt;This project will create and deploy a software element to solve radiation hydrodynamics problems on modern supercomputers featuring &quot;hybrid&quot; architectures that include traditional CPUs plus &quot;accelerators&quot; or &quot;coprocessors,&quot; such as GPUs or Intel Many Integrated Core processors, respectively. This radiation hydrodynamics functionality will be developed within GenASiS (General Astrophysical Simulation System), a new framework being developed to facilitate the simulation of astrophysical phenomena on the world&apos;s leading capability supercomputers. In particular, the radiation transport solver will utilize the extant capabilities of GenASiS for adaptive computational &quot;mesh refinement,&quot; whereby the representation of the natural continuum is captured adaptively on a mesh of points foundational to any computational model in order to maximize the fidelity of the computational model for a given computational cost. We will use the so-called M1 approach, solving directly for the zeroth and first angular moments (energy density and momentum) of the radiation field, with higher-order moments given by &quot;closure relations,&quot; expressing them in terms of the zeroth and first moments. The energy dependence of the radiation field will be retained, with the zeroth and first angular moments discretized into &quot;energy bins.&quot; Our computational approach to neutrino radiation transport will be an &quot;implicit-explicit&quot; (IMEX) scheme. Interactions between radiation and matter will be handled with a time-implicit subsolver, which will involve the inversion of dense matrices local to each node of the machine to exploit all available hardware in the node, including accelerators and coprocessors when available. Algorithms and software resulting from this project will be made available to the community. GenASiS, as an extensible, object-oriented simulation framework, will be valuable to researchers seeking to experiment with and implement different kinds of solvers for multi-physics problems. In particular, the neutrino hydrodynamics solver developed in this project is of high interest to astrophysics modelers.", "ProgramReferenceCode": ["1206", "7433", "8005", "8084", "9150"], "OrganizationZip": "379960003"}, {"OrganizationCity": "Evanston", "OrganizationState": "IL", "Title": "SI2-SSI: Collaborative Research: Scalable Infrastructure for Enabling Multiscale and Multiphysics Applications in Fluid Dynamics, Solid Mechanics, and Fluid-Structure Interaction", "ProgramManager": "Rajiv Ramnath", "State": "IL", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes", "CDS&E-MSS", "CDS&E"], "ProgramElementCode": ["1253", "8004", "8069", "8084"], "StartDate": "08/01/2015", "OrganizationStreet": "1801 Maple Ave.", "EndDate": "07/31/2020", "PrincipalInvestigator": "Neelesh Patankar", "ARRAAmount": null, "OrganizationPhone": "8474913003", "AwardNumber": "1450374", "PIEmailAddress": "n-patankar@northwestern.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/11/2015", "Organization": "Northwestern University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "512966", "Abstract": "Many biological and biomedical systems involve the interaction of a flexible structure and a fluid. These systems range from the writhing and coiling of DNA, to the beating and pumping of cilia and flagella, to the flow of blood in the body, to the locomotion of fish, insects, and birds. This project aims to develop advanced software infrastructure for performing dynamic computer simulations of such biological and biomedical systems. To facilitate the deployment of this software in a range of scientific and engineering applications, this project will develop new software capabilities in concert with new computer models that use the software. Specific application domains to be advanced in this project include models of aquatic locomotion that can be used to understand the neural control of movement and ultimately to develop new treatments for neurological pathologies such as spinal cord injuries, and models that simulate the interaction between the electrophysiology of the heart and the contractions of the heart that pump blood throughout the body, which could lead to improved approaches to treating heart disease. The software to be developed within the project is freely available online and is used by a number of independent research groups in a variety of scientific and engineering domains. It is being actively used in projects that model different aspects of cardiovascular dynamics, such as platelet aggregation and the dynamics of natural and prosthetic heart valves, and in projects that study other biological problems, including cancer dynamics, insect flight, aquatic locomotion, and the dynamics of phytoplankton. The software is also being applied to non-biological problems, including nanoscale models of colloidal suspensions and models of active particles. The improved methods and software to be developed in this project will thereby have a broad and sustained impact on a large number of ongoing research efforts in the biological and biomedical sciences and other scientific and engineering disciplines.&lt;br/&gt;&lt;br/&gt;The immersed boundary (IB) method is a broadly applicable framework for modeling and simulating fluid-structure interaction (FSI). The IB method was introduced to model the fluid dynamics of heart valves, and subsequent development initially focused on simulating cardiac fluid dynamics. This methodology is broadly useful, however, and has been applied to a variety of problems in which a fluid flow interacts with immersed structures, including elastic bodies, bodies with known or prescribed deformational kinematics, and rigid bodies. Extensions of the IB method have also been developed to model electrophysiological systems and systems with chemically active structures. To improve the efficiency of the IB method, the PI has developed adaptive versions of the IB method that employ structured adaptive mesh refinement (AMR) to deploy high spatial resolution only where needed. These methods have been implemented within the IBAMR software framework, which provides parallel implementations of the IB method and its extensions that leverage high-quality computational libraries including SAMRAI, PETSc, and libMesh. This project will further extend the IBAMR software by implementing modeling and discretization technologies required by the research applications of current and prospective users of the software, by developing improved solver infrastructure facilitated by the implementation of native support for structured AMR discretizations in the PETSc library, and by integrating with existing high-quality software tools for model development, deployment, and analysis. IBAMR is freely distributed online and is used within a number of independent research groups both to the further development of the IB method and also to its application to simulate diverse problems in fluid dynamics and FSI. By enhancing IBAMR, this project will also enhance the ability of these and other researchers to construct detailed models without requiring those researchers to develop the significant software infrastructure needed to perform such simulations. This project will also develop general-purpose support for AMR discretizations in PETSc, a software library with thousands of active users, ~400 downloads per month, and numerous applications. The work of this project will help to grow the IBAMR user community of students and researchers by developing UI tools for building models, running simulations, and analyzing results. Students will be actively engaged in all aspects of the project, including code, method, and model development.", "ProgramReferenceCode": ["7433", "8004", "8009", "8084"], "OrganizationZip": "602013149"}, {"OrganizationCity": "West Lafayette", "OrganizationState": "IN", "Title": "SI2-SSE Collaborative Research: SPIKE-An Implementation of a Recursive Divide-and-Conquer Parallel Strategy for Solving Large Systems of Liner Equations", "ProgramManager": "Almadena Y. Chtchelkanova", "State": "IN", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "DYNAMICAL SYSTEMS", "Software Institutes", "CDS&E-MSS"], "ProgramElementCode": ["1253", "7478", "8004", "8069"], "StartDate": "06/01/2012", "OrganizationStreet": "Young Hall", "EndDate": "05/31/2016", "PrincipalInvestigator": "Ahmed Sameh", "ARRAAmount": null, "OrganizationPhone": "7654941055", "AwardNumber": "1147422", "PIEmailAddress": "sameh@cs.purdue.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "06/20/2012", "Organization": "Purdue University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "240000", "Abstract": "Drs. Negrut, Sameh, and Knepley will investigate, produce, and maintain a methodology and its software implementation that leverage emerging heterogeneous hardware architectures to solve billion-unknowns linear systems in a robust, scalable, and efficient fashion. The two classes of problems targeted under this project are banded dense and sparse general linear systems.&lt;br/&gt;&lt;br/&gt;This project is motivated by the observation that the task of solving a linear system is one of the most ubiquitous ingredients in the numerical solution of Applied Mathematics problems. It is relied upon for the implicit integration of Ordinary Differential Equation (ODE) and Differential Algebraic Equation (DAE) problems, in the numerical solution of Partial Differential Equation (PDE) problems, in interior point optimization methods, in least squares approximations, in solving eigenvalue problems, and in data analysis. In fact, the vast majority of nonlinear problems in Scientific Computing are solved iteratively by drawing on local linearizations of nonlinear operators and the solution of linear systems. Recent advances in (a) hardware architecture; i.e., the emergence of General Purpose Graphics Processing Unit (GP-GPU) cards, and (b) scalable solution algorithms, provide an opportunity to develop a new class of parallel algorithms, called SPIKE, which can robustly and efficiently solve very large linear systems of equations.&lt;br/&gt;&lt;br/&gt;Drawing on its divide-and-conquer paradigm, SPIKE builds on several algorithmic primitives: matrix reordering strategies, dense linear algebra operations, sparse direct solvers, and Krylov subspace methods. It provides a scalable solution that can be deployed in a heterogeneous hardware ecosystem and has the potential to solve billion-unknown linear systems in the cloud or on tomorrow?s exascale supercomputers. Its high degree of scalability and improved efficiency stem from (i) optimized memory access pattern owing to an aggressive pre-processing stage that reduces a generic sparse matrix to a banded one through a novel reordering strategy; (ii) good exposure of coarse and fine grain parallelism owing to a recursive, divide-and-conquer solution strategy; (iii) efficient vectorization in evaluating the coupling terms in the divide-and-conquer stage owing to a CPU+GPU heterogeneous computing approach; and (iv) algorithmic polymorphism, given that SPIKE can serve both as a direct solver or an effective preconditioner in an iterative Krylov-type method.&lt;br/&gt;&lt;br/&gt;In Engineering, SPIKE will provide the Computer Aided Engineering (CAE) community with a key component; i.e., fast solution of linear systems, required by the analysis of complex problems through computer simulation. Examples of applications that would benefit from this technology are Structural Mechanics problems (Finite Element Analysis in car crash simulation), Computational Fluid Dynamics problems (solving Navier-Stokes equations in the simulation of turbulent flow around a wing profile), and Computational Multibody Dynamics problems (solving Newton-Euler equations in large granular dynamics problems).&lt;br/&gt;&lt;br/&gt;SPIKE will also be interfaced to the Portable, Extensible Toolkit for Scientific Computation (PETSc), a two decades old flexible and scalable framework for solving Science and Engineering problems on supercomputers. Through PETSc, SPIKE will be made available to a High Performance Computing user community with more than 20,000 members worldwide. PETSc users will be able to run SPIKE without any modifications on vastly different supercomputer architectures such as the IBM BlueGene/P and BlueGene/Q, or the Cray XT5. SPIKE will thus run scalably on the largest machines in the world and will be tuned for very different network and hardware topologies while maintaining a simple code base.&lt;br/&gt;&lt;br/&gt;The experience collected and lessons learned in this project will augment a graduate level class, ?High Performance Computing for Engineering Applications? taught at the University of Wisconsin-Madison. A SPIKE tutorial and research outcomes will be presented each year at the International Conference for High Performance Computing, Networking, Storage and Analysis. A one day High Performance Computing Boot Camp will be organized each year in conjunction with the American Society of Mechanical Engineers (ASME) conference and used to disseminate the software outcomes of this effort. Finally, this project will shape the research agendas of two graduate students working on advanced degrees in Computational Science.", "ProgramReferenceCode": ["7433", "8005", "1253", "7478"], "OrganizationZip": "479072114"}, {"OrganizationCity": "West Lafayette", "OrganizationState": "IN", "Title": "Collaborative Research: SI2-SSI: Empowering the Scientific Community with Streaming Data Middleware: Software Integration into Complex Science Environments", "ProgramManager": "Daniel Katz", "State": "IN", "Program": ["ADVANCES IN BIO INFORMATICS", "ECOSYSTEM STUDIES", "Software Institutes", "Cyber Secur - Cyberinfrastruc"], "ProgramElementCode": ["1165", "1181", "8004", "8027"], "StartDate": "08/01/2012", "OrganizationStreet": "Young Hall", "EndDate": "07/31/2016", "PrincipalInvestigator": "Shirley Dyke", "ARRAAmount": null, "OrganizationPhone": "7654941055", "AwardNumber": "1148255", "PIEmailAddress": "sdyke@purdue.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "03/12/2013", "Organization": "Purdue University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "42000", "Abstract": "This project will catalyze a new generation of sensor-based streaming data applications by building on the foundation of the Open Source DataTurbine (OSDT) middleware (www.dataturbine.org). This will involve strategic software development, systems integration and testing, science experiments, and education and outreach. The software developments will include (A) tools for configuring, testing, and controlling distributed sensor systems, (B) software interfaces that are compliant with the Open Geospatial Consortium Sensor Web Enablement standards, (C) a new OSDT-Matlab interface, and (D) OSDT cybersecurity enhancements. These software products will be integrated into the production research infrastructures at Purdue, the University of Connecticut, the North Temperate Lake LTER Network site, and the Monterey Bay Aquarium Research Institute. The software will be professionally developed and managed as community resources. The intellectual merit of this project will be advances in software engineering, interoperability standards, and systems management for complex real-time sensor systems as well as advances on important open questions in civil engineering, marine science, and ecology. This project will yield broader impacts by enabling new science applications, enhancing productivity, and accelerating innovation across NSF directorates. To ensure that these broader impacts are realized, the team will provide student research opportunities, develop curriculum materials and training courses, and cultivate a community of software users.", "ProgramReferenceCode": ["7434", "8009", "9251", "1165", "1181", "8004", "8027"], "OrganizationZip": "479072114"}, {"OrganizationCity": "KNOXVILLE", "OrganizationState": "TN", "Title": "SI2-CHE: CCP-SAS - Collaborative Computing consortium for advanced analyses of structural data in chemical biology and soft condensed matter", "ProgramManager": "Evelyn M. Goldfield", "State": "TN", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes"], "ProgramElementCode": ["1253", "8004"], "StartDate": "06/01/2013", "OrganizationStreet": "1 CIRCLE PARK", "EndDate": "05/31/2017", "PrincipalInvestigator": "Paul Butler", "ARRAAmount": null, "OrganizationPhone": "8659743466", "AwardNumber": "1265821", "PIEmailAddress": "pbutler@utk.edu", "NSFDirectorate": "MPS", "LastAmendmentDate": "05/31/2013", "Organization": "University of Tennessee Knoxville", "AwardInstrument": "Standard Grant", "NsfOrganization": "CHE", "AwardedAmountToDate": "611582", "Abstract": "An international team consisting of Paul Butler (University of Tennessee, Knoxville and NIST), Jianhan Chen (Kansas State University), Emre Brooks (University of Texas Health Science Center, San Antonio) and their UK collaborators led by Stephen J. Perkins (University College, London) are supported through the SI2-CHE program. The object of this project is to provide a web-based GUI front-end with a high-performance back-end to increase the accessibility of advanced atomistic modeling of scattering data by novice users. Advanced analysis modules and new simulation methods including implicit solvent models are being developed to increase the accuracy of scattering calculations and simulation protocols. Software is being developed and disseminated with continual feedback from the research teams&apos; large international user-base. These efforts leverage an international user and developer community that use high-performance computing resources on a wide-range of cutting-edge chemical problems. &lt;br/&gt;&lt;br/&gt;A typical bench scientist purifies and characterizes samples, collects the small angle x-ray scattering (SAXS), small angle neutron scattering (SANS), or analytical ultracentrifugation (AUC) data, and interprets the results using simplistic models. It is rare that the same individual also has the skills to use advanced atomistic simulation software. The CCP-SAS project is focused on developing an easy-to-use modeling package that enables users to generate physically accurate atomistic models, calculate scattering profiles and compare results to experimental scattering data sets in a single web-based software suite. This enables a broad range of scattering scientists to access often complicated simulation and scattering analysis methods seamlessly thus providing a significant acceleration to the discovery process. &lt;br/&gt;&lt;br/&gt;The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC.", "ProgramReferenceCode": ["5918", "5946", "5950", "7433", "8009", "9150"], "OrganizationZip": "379960003"}, {"OrganizationCity": "West Lafayette", "OrganizationState": "IN", "Title": "SI2-SSE Collaborative Research: Molecular Simulations of Polymer Nanostructures in the Cloud", "ProgramManager": "Daniel Katz", "State": "IN", "Program": ["DMR SHORT TERM SUPPORT", "Software Institutes", "CDS&E"], "ProgramElementCode": ["1712", "8004", "8084"], "StartDate": "10/01/2014", "OrganizationStreet": "Young Hall", "EndDate": "09/30/2017", "PrincipalInvestigator": "Alejandro Strachan", "ARRAAmount": null, "OrganizationPhone": "7654941055", "AwardNumber": "1440727", "PIEmailAddress": "strachan@purdue.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Chunyu Li", "Benjamin Haley"], "LastAmendmentDate": "07/31/2014", "Organization": "Purdue University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "349804", "Abstract": "The use of polymers, their composites, and nanostructures is growing at a fast pace, both by displacing traditional materials and by enabling emerging technologies. Examples range from the all-composite airframe of the Boeing 787 and the new Airbus 350 to wearable electronics. This project aims to develop a software infrastructure to simulate these materials with atomic resolution and make these tools universally accessible and useful via on-line computing. These simulations have the potential to accelerate the development of optimized material formulations that can benefit society and reduce the associated costs by combining physical with computational experiments. Making these advanced tools available for free online simulations and complementing them with tutorials and educational material will encourage their use in the classroom and will impact the education of new generations of engineers and scientists familiar with these powerful tools that will be required to address tomorrow&apos;s challenges.&lt;br/&gt;&lt;br/&gt;The objective of this effort is to enable pervasive, reproducible molecular simulations of polymeric materials using state-of-the-art tools and with quantified uncertainties building on recent breakthroughs in molecular simulations, cyber-infrastructure and uncertainty quantification. The framework will consist of three main components: i) powerful simulation tools for polymer nano structures including: state-of-the-art molecular builders, a parallel MD engine with stencils that enable efficient structure relaxation and property characterization and post-processing codes; ii) a UQ framework to orchestrate the molecular simulations and propagate uncertainties in input parameters to predictions and compare the predictions to experimental values; iii) databases of force fields and molecular structures as well as predicted and experimental properties. The simulation framework will be deployed via NSF&apos;s nanoHUB where users will be able to run online simulations without downloading or installing any software while expert users will have the option to download, modify and contribute to the infrastructure. Usage of and contributions to the software framework will be facilitated and encouraged via online and in-person user guides, learning modules and research tutorials.", "ProgramReferenceCode": ["024E", "085E", "7237", "7433", "8005", "8400", "9216"], "OrganizationZip": "479072114"}, {"OrganizationCity": "Fort Collins", "OrganizationState": "CO", "Title": "Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities", "ProgramManager": "Daniel Katz", "State": "CO", "Program": ["PHYSICAL & DYNAMIC METEOROLOGY", "Software Institutes", "EarthCube"], "ProgramElementCode": ["1525", "8004", "8074"], "StartDate": "08/01/2015", "OrganizationStreet": "601 S Howes St", "EndDate": "07/31/2018", "PrincipalInvestigator": "Russ Schumacher", "ARRAAmount": null, "OrganizationPhone": "9704916355", "AwardNumber": "1450089", "PIEmailAddress": "russ.schumacher@colostate.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/04/2015", "Organization": "Colorado State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "177173", "Abstract": "Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.&lt;br/&gt;&lt;br/&gt;The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of &quot;nuclei,&quot; which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities.", "ProgramReferenceCode": ["4444", "7433", "8009"], "OrganizationZip": "805232002"}, {"OrganizationCity": "KNOXVILLE", "OrganizationState": "TN", "Title": "SI2-SSE: Collaborative Research: ADAPT: Next Generation Message Passing Interface (MPI) Library - Open MPI", "ProgramManager": "Daniel Katz", "State": "TN", "Program": ["SPECIAL PROJECTS - CCF", "Software Institutes"], "ProgramElementCode": ["2878", "8004"], "StartDate": "09/01/2013", "OrganizationStreet": "1 CIRCLE PARK", "EndDate": "08/31/2016", "PrincipalInvestigator": "George Bosilca", "ARRAAmount": null, "OrganizationPhone": "8659743466", "AwardNumber": "1339820", "PIEmailAddress": "bosilca@eecs.utk.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/29/2013", "Organization": "University of Tennessee Knoxville", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "347216", "Abstract": "High-performance computing has reshaped science and industry in many areas. However, the rapid evolution at the hardware level over the last few years have been unmatched by corresponding changes at the programming paradigm level. According to the consensus of several major studies, the degree of parallelism on large systems is expected to increase by several orders of magnitude. As a result, the Message Passing Interface (MPI), which has been the de-facto standard message passing paradigm, lacks an efficient and portable way of handling today&apos;s architectures. To efficiently handle such systems, MPI implementations must adopt more asynchronous and thread-friendly behaviors to perform better than they do on today?s systems. Maintaining and further enhancing MPI, one of the most widely-used communication libraries in high-performance computing, will have a far-reaching impact beyond the scientific community, and represents a critical building block for continued advances in all areas of science and engineering.&lt;br/&gt;The ADAPT project enhances, hardens and modernizes the Open MPI library in the context of this ongoing revolution in processor architecture and system design. It creates a viable foundation for a new generation of Open MPI components, enabling the rapid exploration of new physical capabilities, providing greatly improved performance portability, and working toward full interoperability between classes of components. More specifically, ADAPT implements fundamental software techniques that can be used in many-core systems to efficiently execute MPI-based applications and to tolerate fail-stop process failures, at scales ranging from current large systems to the extreme scale systems that are coming soon. To improve the efficiency of Open MPI, ADAPT integrates, as a core component, knowledge about the hardware architecture, and allows all layers of the software stack full access to this information. Process placement, distributed topologies, file accesses, point-to-point and collective communications can then adapt to such topological information, providing more portability. The ADAPT team is also updating the current collective communication layer to allow for a task-based collective description contained at a group-level, which in turn adjusts to the intra and inter-node topology. Planned expansion of the current code with resilient capabilities allows Open MPI to efficiently survive hard and soft error types of failures. These capabilities can be used as building blocks for all currently active fault tolerance proposals in the MPI standard body.&lt;br/&gt;MPI is already one of the most relevant parallel programming models, the most important brick of most parallel applications, and one of the most critical communication pieces of most other programing models. Thus, the experience of the research team and emerging capabilities can benefit all future users of these programming standards, tools, and libraries--regardless of discipline. Any improvement in the performance and capabilities of a major MPI library such as Open MPI, has tremendous potential for an immediate and dramatic impact on the application communities. In addition to improving the time to solution for their applications, it has the potential to decrease the energy usage and maximize the performance delivered by the existing execution platforms. The scale at which the Open MPI library is used in government research institutions (including universities and national laboratories), as well as in the private sector, is a major vector for a quick impact on all scientific and engineering communities.", "ProgramReferenceCode": ["7433", "8005", "9150"], "OrganizationZip": "379960003"}, {"OrganizationCity": "Storrs", "OrganizationState": "CT", "Title": "SI2-SSE: Collaborative Research: An Intelligent and Adaptive Parallel CPU/GPU Co-Processing Software Library for Accelerating Reactive-Flow Simulations", "ProgramManager": "Daniel Katz", "State": "CT", "Program": ["COMBUSTION, FIRE, & PLASMA SYS", "Software Institutes"], "ProgramElementCode": ["1407", "8004"], "StartDate": "09/01/2015", "OrganizationStreet": "438 Whitney Road Ext.", "EndDate": "08/31/2018", "PrincipalInvestigator": "Chih-Jen Sung", "ARRAAmount": null, "OrganizationPhone": "8604863622", "AwardNumber": "1534688", "PIEmailAddress": "cjsung@engr.uconn.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/03/2015", "Organization": "University of Connecticut", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "214357", "Abstract": "In order to develop the next generation of clean and efficient vehicle engines and power-generating combustors, engineers need the next generation of computational modeling tools. Accurately describing the chemistry of conventional and alternative liquid transportation fuels is vital to predict harmful emission levels and other important quantities, but the high computational cost of detailed models for chemistry poses a significant barrier to use by designers. In order to use such accurate models, software is needed that can efficiently handle chemistry in practical simulations. This collaborative project aims to develop such tools, employing the computational power of modern parallelized central processing units (CPUs) and graphics processing units (GPUs). In addition to helping designers create clean and efficient engine technology, the advances made in this project are widely applicable to other computational modeling problems including astrophysics, nuclear reactions, atmospheric chemistry, biochemical networks, and even cardiac electrophysiology.&lt;br/&gt;&lt;br/&gt;The objective of the proposed effort is to develop software elements specifically targeted at co-processing on GPUs, CPUs, and other many-core accelerator devices to reduce the computational cost of using detailed chemistry and enable high-fidelity yet affordable reactive-flow simulations. This will be achieved by (1) developing and comparing chemical kinetics integration algorithms for parallel operation on CPUs and GPUs/accelerators, (2) developing a method for detecting local stiffness due to chemical kinetics and adaptively selecting the most efficient solver based on available hardware, (3) implementing a computational cell clustering strategy to group similar spatial locations, (4) demonstrating the improved performance offered by these software elements using commercial and open-source computational fluid dynamics codes for modeling reactive flows, and (5) designing a portable and sustainable software library based on the above software elements, including building a community of users. The result of this program will be an open source software library that significantly decreases the cost of using detailed, accurate chemistry in realistic combustion simulations; the success of the program will be determined based on achieving order-of-magnitude performance improvement or better.", "ProgramReferenceCode": ["148E", "7433", "8005"], "OrganizationZip": "062691133"}, {"OrganizationCity": "Santa Cruz", "OrganizationState": "CA", "Title": "Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities", "ProgramManager": "Daniel Katz", "State": "CA", "Program": ["PHYSICAL & DYNAMIC METEOROLOGY", "Software Institutes", "EarthCube"], "ProgramElementCode": ["1525", "8004", "8074"], "StartDate": "08/01/2015", "OrganizationStreet": "1156 High Street", "EndDate": "07/31/2018", "PrincipalInvestigator": "Carl Maltzahn", "ARRAAmount": null, "OrganizationPhone": "8314595278", "AwardNumber": "1450488", "PIEmailAddress": "carlosm@soe.ucsc.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/04/2015", "Organization": "University of California-Santa Cruz", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "695525", "Abstract": "Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.&lt;br/&gt;&lt;br/&gt;The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of &quot;nuclei,&quot; which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "950641077"}, {"OrganizationCity": "Rapid City", "OrganizationState": "SD", "Title": "Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities", "ProgramManager": "Daniel Katz", "State": "SD", "Program": ["PHYSICAL & DYNAMIC METEOROLOGY", "Software Institutes", "EarthCube"], "ProgramElementCode": ["1525", "8004", "8074"], "StartDate": "08/01/2015", "OrganizationStreet": "501 East Saint Joseph Street", "EndDate": "07/31/2018", "PrincipalInvestigator": "William Capehart", "ARRAAmount": null, "OrganizationPhone": "6053941218", "AwardNumber": "1450170", "PIEmailAddress": "William.Capehart@sdsmt.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/04/2015", "Organization": "South Dakota School of Mines and Technology", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "183956", "Abstract": "Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.&lt;br/&gt;&lt;br/&gt;The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of &quot;nuclei,&quot; which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities.", "ProgramReferenceCode": ["4444", "7433", "8009", "9150"], "OrganizationZip": "577013995"}, {"OrganizationCity": "Milwaukee", "OrganizationState": "WI", "Title": "Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities", "ProgramManager": "Daniel Katz", "State": "WI", "Program": ["PHYSICAL & DYNAMIC METEOROLOGY", "Software Institutes", "EarthCube"], "ProgramElementCode": ["1525", "8004", "8074"], "StartDate": "08/01/2015", "OrganizationStreet": "P O BOX 340", "EndDate": "07/31/2018", "PrincipalInvestigator": "Allen Evans", "ARRAAmount": null, "OrganizationPhone": "4142294853", "AwardNumber": "1450439", "PIEmailAddress": "evans36@uwm.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/04/2015", "Organization": "University of Wisconsin-Milwaukee", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "164381", "Abstract": "Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.&lt;br/&gt;&lt;br/&gt;The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of &quot;nuclei,&quot; which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities.", "ProgramReferenceCode": ["4444", "7433", "8009"], "OrganizationZip": "532010340"}, {"OrganizationCity": "UNIVERSITY PARK", "OrganizationState": "PA", "Title": "Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities", "ProgramManager": "Daniel Katz", "State": "PA", "Program": ["PHYSICAL & DYNAMIC METEOROLOGY", "Software Institutes", "EarthCube"], "ProgramElementCode": ["1525", "8004", "8074"], "StartDate": "08/01/2015", "OrganizationStreet": "110 Technology Center Building", "EndDate": "07/31/2018", "PrincipalInvestigator": "Steven Greybush", "ARRAAmount": null, "OrganizationPhone": "8148651372", "AwardNumber": "1450405", "PIEmailAddress": "sjg213@psu.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/04/2015", "Organization": "Pennsylvania State Univ University Park", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "99953", "Abstract": "Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.&lt;br/&gt;&lt;br/&gt;The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of &quot;nuclei,&quot; which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities.", "ProgramReferenceCode": ["4444", "7433", "8009"], "OrganizationZip": "168027000"}, {"OrganizationCity": "Albany", "OrganizationState": "NY", "Title": "Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities", "ProgramManager": "Daniel Katz", "State": "NY", "Program": ["PHYSICAL & DYNAMIC METEOROLOGY", "Software Institutes", "EarthCube"], "ProgramElementCode": ["1525", "8004", "8074"], "StartDate": "08/01/2015", "OrganizationStreet": "1400 WASHINGTON AVE", "EndDate": "07/31/2018", "PrincipalInvestigator": "Robert Fovell", "ARRAAmount": null, "OrganizationPhone": "5184374550", "AwardNumber": "1450195", "PIEmailAddress": "rfovell@ucla.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/04/2015", "Organization": "SUNY at Albany", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "246111", "Abstract": "Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.&lt;br/&gt;&lt;br/&gt;The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of &quot;nuclei,&quot; which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities.", "ProgramReferenceCode": ["4444", "7433", "8009"], "OrganizationZip": "122220100"}, {"OrganizationCity": "Grand Forks", "OrganizationState": "ND", "Title": "Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities", "ProgramManager": "Daniel Katz", "State": "ND", "Program": ["PHYSICAL & DYNAMIC METEOROLOGY", "Software Institutes", "EarthCube"], "ProgramElementCode": ["1525", "8004", "8074"], "StartDate": "08/01/2015", "OrganizationStreet": "University Station", "EndDate": "07/31/2018", "PrincipalInvestigator": "Gretchen Mullendore", "ARRAAmount": null, "OrganizationPhone": "7017774278", "AwardNumber": "1450168", "PIEmailAddress": "gretchen@atmos.und.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/04/2015", "Organization": "University of North Dakota Main Campus", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "168182", "Abstract": "Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.&lt;br/&gt;&lt;br/&gt;The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of &quot;nuclei,&quot; which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities.", "ProgramReferenceCode": ["4444", "7433", "8009", "9150"], "OrganizationZip": "582026059"}, {"OrganizationCity": "GAINESVILLE", "OrganizationState": "FL", "Title": "SI2-SSE: Peer-to-Peer Overlay Virtual Network for Cloud Computing Research", "ProgramManager": "Daniel Katz", "State": "FL", "Program": ["SPECIAL PROJECTS - CCF", "Software Institutes"], "ProgramElementCode": ["2878", "8004"], "StartDate": "09/01/2013", "OrganizationStreet": "1 UNIVERSITY OF FLORIDA", "EndDate": "08/31/2016", "PrincipalInvestigator": "Renato Figueiredo", "ARRAAmount": null, "OrganizationPhone": "3523923516", "AwardNumber": "1339737", "PIEmailAddress": "renato@acis.ufl.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "05/26/2015", "Organization": "University of Florida", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "494107", "Abstract": "Modern cloud computing systems use virtual machine technologies to deliver unprecedented flexibility to users, enabling businesses and individuals to cost-effectively deploy computing and storage capacity on-demand, at scale, across multiple infrastructures distributed geographically. While the ability to deploy virtual machines for cloud computing is widely supported, researchers face increasing challenges in prototyping and deploying experimental research systems that span across multiple cloud providers. In particular, providing end-to-end network connectivity among distributed virtual machines in today&apos;s Internet environment (where nodes are often constrained by firewalls and network address translators) requires significant investment of time in development, testing and maintenance of code needed solely to provide connectivity. This project addresses these connectivity challenges in cloud computing by developing an open-source scientific software element that allows researchers and users of clouds to seamlessly create virtual networks on demand for distributed virtual machines. To this end, the project creates software-defined virtual networks that support the standard Internet Protocol (IP) and use tunneling of virtual network packets over Peer-to-Peer (P2P) links among virtual machines for scalable and resilient messaging. In addition to the core IP-over-P2P virtual networking, the software provides a framework for configuration, management and monitoring that enables easy deployment of user-defined overlays for inter-cloud research experiments. &lt;br/&gt;The open-source software developed in this project enables advances in the state-of-the-art of research of cloud computing systems and applications. Complementary to research and development activities, this project delivers educational modules, tutorials, software packages, and pre-configured virtual machine images that allow non-expert users to deploy their own virtual networks over private, commercial and public clouds. Because cloud computing technologies are increasingly pervasive and of growing importance to the economy and society, the broader impacts of this project can reach Internet users at large who benefit from the ability to seamlessly interconnect cloud virtual machines across multiple providers. In particular, leveraging online social networking technologies, the virtual network software software enables individuals and small groups to easily create social virtual private networks connecting personal computers and multiple cloud resources.", "ProgramReferenceCode": ["7433", "8005", "9251"], "OrganizationZip": "326112002"}, {"OrganizationCity": "GAINESVILLE", "OrganizationState": "FL", "Title": "Collaborative: SI2-SSE - A Plug-and-Play Software Platform of Robotics-Inspired Algorithms for Modeling Biomolecular Structures and Motions", "ProgramManager": "Daniel Katz", "State": "FL", "Program": ["SPECIAL PROJECTS - CCF", "Software Institutes", "CDS&E"], "ProgramElementCode": ["2878", "8004", "8084"], "StartDate": "02/01/2015", "OrganizationStreet": "1 UNIVERSITY OF FLORIDA", "EndDate": "01/31/2018", "PrincipalInvestigator": "Adrian Roitberg", "ARRAAmount": null, "OrganizationPhone": "3523923516", "AwardNumber": "1440523", "PIEmailAddress": "roitberg@ufl.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/08/2014", "Organization": "University of Florida", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "67235", "Abstract": "This project aims to develop a novel plug-and-play platform of open-source software elements to advance algorithmic research in molecular biology. The focus is on addressing the algorithmic impasse faced by computational chemists and biophysicists in structure-function related problems involving dynamic biomolecules central to our biology. The software platform resulting from this project provides the critical software infrastructure to support transformative research in molecular biology and computer science that benefits society at large by advancing our modeling capabilities and in turn our understanding of the role of biomolecules in critical mechanisms in a living and diseased cell.&lt;br/&gt;&lt;br/&gt;The project addresses the current impasse on the length and time scales that can be afforded in biomolecular modeling and simulation. It does so by integrating cutting-edge knowledge from two different research communities, computational chemists and biophysicists focused on detailed physics-based simulations, and AI researchers focused on efficient search and optimization algorithms. The software elements integrate sophisticated energetic models and molecular representations with powerful search and optimization algorithms for complex modular systems inspired from robot motion planning. The plug-and-play feature of the software platform supports putting together novel algorithms, such as wrapping Molecular Dynamics or Monte Carlo as local search operators within larger robotics-inspired exploration frameworks, and adding emerging biomolecular representations, models, and search techniques even beyond the timeline of this project.", "ProgramReferenceCode": ["7433", "8004", "8005", "8084", "9216", "2878"], "OrganizationZip": "326112002"}, {"OrganizationCity": "MADISON", "OrganizationState": "WI", "Title": "SI2-SSI: Collaborative Research: A Sustainable Infrastructure for Performance, Security, and Correctness Tools", "ProgramManager": "Daniel Katz", "State": "WI", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "08/01/2015", "OrganizationStreet": "21 North Park Street", "EndDate": "07/31/2020", "PrincipalInvestigator": "Barton Miller", "ARRAAmount": null, "OrganizationPhone": "6082623822", "AwardNumber": "1449918", "PIEmailAddress": "bart@cs.wisc.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "07/20/2015", "Organization": "University of Wisconsin-Madison", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1500000", "Abstract": "Software has become indispensable to society, used by computational scientists for science and engineering, by analysts mining big data for value, and to connect society over the Internet. However, the properties of software systems for any of these purposes cannot be understood without accounting for code transformations applied by optimizing compilers used to compose algorithm and data structure templates, and libraries available only in binary form. To address this need, this project will overhaul, integrate, and enhance static binary analysis and runtime technologies to produce components that provide a foundation for performance, correctness, and security tools. The project will build upon three successful and widely adopted open source software packages: the DynInst library for analysis and transformation of application binaries, the MRNet infrastructure for control of large-scale parallel executions and data analysis of their results, and the HPCToolkit performance analysis tools. The project team will engage the community to participate in the design and evaluation of the emerging components, as well as to adopt its components. &lt;br/&gt;&lt;br/&gt;This project will have a wide range of impacts. First, software components built by the project will enable the development of sophisticated, high-quality, end-user performance, correctness, and security tools built by the project team, as well as others in academia, government, and industry. Software developed by the project team will help researchers and developers tackle testing, debugging, monitoring, analysis, and tuning of applications for systems at all scales. Second, end-user tools produced by the project have a natural place in the classroom to help students write efficient, correct, and secure programs. Third, components produced by the project will lower the barrier for new researchers to enter the field and build tools that have impact on production applications without years of investment. Fourth, the project will provide training for graduate students and interns in the area of software for performance, correctness, and security. Finally, through workshops and tutorials, the project will disseminate project results, provide training to enable others to leverage project software, and grow a community of tool researchers who depend on project components and thus have a strong motivation to help sustain project software into the future.&lt;br/&gt;&lt;br/&gt;Modernizing open-source software components and tools for binary analysis will enable static analysis of application characteristics at the level of executable machine code, transformation of binaries to inject monitoring code, measurement to capture a detailed record of application?s interactions with all facets of a target platform, analysis of recorded data in parallel, and attribution of analysis results back to application source code in meaningful ways. Providing innovative, software components that support development of robust performance, correctness, and security tools will accelerate innovation by tools researchers and help them grapple with the increasing complexity of modern software. Of particular note, helping tools researchers and computational scientists grapple with the challenges of software for modern parallel systems and producing training materials that help people use this software, addresses several of the needs identified in the NSF Vision for Cyberinfrastructure for the 21st Century.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "537151218"}, {"OrganizationCity": "NEW ORLEANS", "OrganizationState": "LA", "Title": "Collaborative Research: SI2-SSI: Landlab: A Flexible, Open-Source Modeling Framework for Earth-Surface Dynamics", "ProgramManager": "Daniel Katz", "State": "LA", "Program": ["GEOMORPHOLOGY & LAND USE DYNAM", "Software Institutes", "EarthCube"], "ProgramElementCode": ["7458", "8004", "8074"], "StartDate": "08/01/2015", "OrganizationStreet": "6823 ST CHARLES AVENUE", "EndDate": "07/31/2020", "PrincipalInvestigator": "Nicole Gasparini", "ARRAAmount": null, "OrganizationPhone": "5048654000", "AwardNumber": "1450338", "PIEmailAddress": "ngaspari@tulane.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "07/14/2015", "Organization": "Tulane University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "532320", "Abstract": "Earth scientists discover and predict the behavior of the natural world in part through the use of computer models. Models are used to study a wide range of phenomena, such as soil erosion, flooding, plant growth, landslide occurrence, and many other processes. Models can be used to develop scientific understanding by comparing model calculations with the real world. They can also be used to make predictions about how nature will behave under certain conditions. In the areas of geosciences that deal with the earth&apos;s surface, one bottleneck in the development and use of models is the effort required to build, test, and debug the necessary software. This project contributes to scientific research and discovery by creating open source software tools that scientists can use to more efficiently create, modify, or combine computer models that represent portions of earth&apos;s surface and the processes occurring thereon. The project combines software development with user training and web-based resources, so that the products will be openly accessible, well documented, and widely disseminated within the relevant scientific communities. The project also contributes to K-12, undergraduate, and graduate-level education in computational modeling and earth-surface processes.&lt;br/&gt;&lt;br/&gt;This project catalyzes research in earth-surface dynamics by developing a software framework that enables rapid creation, refinement, and reuse of two-dimensional (2D) numerical models. The phrase earth-surface dynamics refers to a remarkably diverse group of science and engineering fields that deal with our planet&apos;s surface and near-surface environment: its processes, its management, and its responses to natural and human-made perturbations. Scientists who want to use an earth-surface model often build their own unique model from the ground up, re-coding the basic building blocks of their model rather than taking advantage of codes that have already been written. Whereas the end result may be novel software programs, many person-hours are lost rewriting existing code, and the resulting software is often idiosyncratic, poorly documented, and unable to interact with other software programs in the same scientific community and beyond, leading to lost opportunities for exploring an even wider array of scientific questions than those that can be addressed using a single model. The Landlab model framework seeks to eliminate these redundancies and lost opportunities, and simultaneously lower the bar for entry into numerical modeling, by creating a user- and developer-friendly software library that provides scientists with the fundamental building blocks needed for modeling earth-surface dynamics. The framework takes advantage of the fact that nearly all surface-dynamics models share a set of common software elements, despite the wide range of processes and scales that they encompass. Providing these elements in the context of a popular scientific programming environment, with strong user support and community engagement, contributes to accelerating progress in the diverse sciences of the earth&apos;s surface.&lt;br/&gt;&lt;br/&gt;The Landlab modeling framework is designed so that grid creation, data storage, and sharing of data among process components is done for the user. The framework is generic enough so that the coupling of two process components, whether they are squarely within a geoscience subdiscipline, or they cross subdisciplines, is the same. This architecture makes it easy to explore a wide range of questions in the geosciences without the need for much coding. Further, the model code is primarily written in Python, a language that is relatively easy for casual programmers to learn. Because of these attributes, the Landlab modeling framework has the potential to add computational modeling to the toolbox of a wide array of geoscientists, and to clear a path for trans-disciplinary earth-surface modeling that explores societally relevant topics, such as land-cover changes in response to climate change, as well as modeling of topics that currently require the coupling of sophisticated but harder-to-use models, such as sediment source-to-sink dynamics. The project will generate several proof-of-concept studies that are interesting in their own right, and demonstrate the capabilities of the modeling framework. Community engagement is fostered by presenting clinics and demonstrations at professional venues aimed at hydrologists, sedimentologists, critical zone scientists, and the broader earth and environmental sciences community. The team also supports visits by individual scientists for on-site training beyond these clinics. Input/output tools allow compatibility with data from relevant NSF-supported research. The project supports four graduate students and three postdoctoral researchers, and also includes modeling workshops for fifth to seventh grade girls in a community that has a greater than 50% minority population.", "ProgramReferenceCode": ["7433", "8009", "9150"], "OrganizationZip": "701185698"}, {"OrganizationCity": "Columbus", "OrganizationState": "OH", "Title": "SI2-SSE: A Unified Software Environment to Best Utilize Cache and Memory Systems on Multicores", "ProgramManager": "Daniel Katz", "State": "OH", "Program": ["INFORMATION TECHNOLOGY RESEARC", "Software Institutes"], "ProgramElementCode": ["1640", "8004"], "StartDate": "06/01/2012", "OrganizationStreet": "Office of Sponsored Programs", "EndDate": "05/31/2016", "PrincipalInvestigator": "Xiaodong Zhang", "ARRAAmount": null, "OrganizationPhone": "6146888735", "AwardNumber": "1147522", "PIEmailAddress": "zhang@cse.ohio-state.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "06/22/2012", "Organization": "Ohio State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "500000", "Abstract": "This project further develops and maintains a set of system and application software to benefit application users of multicores by providing a unified software environment where multicore system utilities can be easily used as common functions in various applications. The project consists of the following three tasks. First, we further improve our cache-partitioning OS utility, and make efforts to add two other critical system utilities: (1) Multicore buffer cache management to prevent the shared cache from thrashing and pollution; (2) multicore-aware synchronization lock management to effectively make process assignments such that the co-running processes would minimize bandwidth consumption within a multicore chip and cross multiple multicore chips. Second, we continue our efforts to develop a software runtime library that enables programmers to explicitly manage and optimize the shared cache usage and memory accesses by allocating proper cache space and memory modules for different data sets of different processes. Finally, we provide a unified software environment for application users. With a set of easy interface functions, the users can access both middleware runtime library and the system utilities without a requirement of knowing architectural and system details. &lt;br/&gt;&lt;br/&gt;The broader and transformative impact of the project can be significant: (1) Our software will provide effective and accessible solutions for significant performance improvement in multicores for a large scope of application community. (2) Gaining the insights into system interactions among applications, OS, and multicore architecture, we will provide valuable guidance for designs and implementations of application software. (3) The software is online with a maintenance for a public, wide, and sustained usage, which will directly impact open source software, and contribute to application users. (4) The research and software development of the project will train both undergraduate and graduate students for their future technical innovations in academia and industries.", "ProgramReferenceCode": ["7942", "8005", "1640", "8004"], "OrganizationZip": "432101016"}, {"OrganizationCity": "Storrs", "OrganizationState": "CT", "Title": "Collaborative Research: SI2-SSI: Empowering the Scientific Community with Streaming Data Middleware: Software Integration into Complex Science Environments", "ProgramManager": "Daniel Katz", "State": "CT", "Program": ["ADVANCES IN BIO INFORMATICS", "ECOSYSTEM STUDIES", "Software Institutes", "Cyber Secur - Cyberinfrastruc"], "ProgramElementCode": ["1165", "1181", "8004", "8027"], "StartDate": "08/01/2012", "OrganizationStreet": "438 Whitney Road Ext.", "EndDate": "07/31/2016", "PrincipalInvestigator": "Richard Christenson", "ARRAAmount": null, "OrganizationPhone": "8604863622", "AwardNumber": "1148215", "PIEmailAddress": "rchriste@engr.uconn.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/07/2012", "Organization": "University of Connecticut", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "30000", "Abstract": "This project will catalyze a new generation of sensor-based streaming data applications by building on the foundation of the Open Source DataTurbine (OSDT) middleware (www.dataturbine.org). This will involve strategic software development, systems integration and testing, science experiments, and education and outreach. The software developments will include (A) tools for configuring, testing, and controlling distributed sensor systems, (B) software interfaces that are compliant with the Open Geospatial Consortium Sensor Web Enablement standards, (C) a new OSDT-Matlab interface, and (D) OSDT cybersecurity enhancements. These software products will be integrated into the production research infrastructures at Purdue, the University of Connecticut, the North Temperate Lake LTER Network site, and the Monterey Bay Aquarium Research Institute. The software will be professionally developed and managed as community resources. The intellectual merit of this project will be advances in software engineering, interoperability standards, and systems management for complex real-time sensor systems as well as advances on important open questions in civil engineering, marine science, and ecology. This project will yield broader impacts by enabling new science applications, enhancing productivity, and accelerating innovation across NSF directorates. To ensure that these broader impacts are realized, the team will provide student research opportunities, develop curriculum materials and training courses, and cultivate a community of software users.", "ProgramReferenceCode": ["7434", "8009", "1165", "1181", "8004", "8027"], "OrganizationZip": "062691133"}, {"OrganizationCity": "Columbus", "OrganizationState": "OH", "Title": "SI2-SSE: Collaborative Research: Software Elements for Transfer and Analysis of Large-Scale Scientific Data", "ProgramManager": "Rudolf Eigenmann", "State": "OH", "Program": ["SPECIAL PROJECTS - CCF", "Software Institutes"], "ProgramElementCode": ["2878", "8004"], "StartDate": "09/01/2013", "OrganizationStreet": "Office of Sponsored Programs", "EndDate": "08/31/2016", "PrincipalInvestigator": "Gagan Agrawal", "ARRAAmount": null, "OrganizationPhone": "6146888735", "AwardNumber": "1339757", "PIEmailAddress": "agrawal@cse.ohio-state.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/29/2013", "Organization": "Ohio State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "400000", "Abstract": "As science has become increasingly data-driven, and as data volumes and velocity are increasing, scientific advance in many areas will only be feasible if critical `big-data&apos; problems are addressed - and even more importantly, software tools embedding these solutions are readily available to the scientists. Particularly, the major challenge being faced by current data-intensive scientific research efforts is that while the dataset sizes continue to grow rapidly, neither among network bandwidths, memory capacity of parallel machines, memory access speeds, and disk bandwidths are increasing at the same rate.&lt;br/&gt;&lt;br/&gt;Building on top of recent research at Ohio State University, which includes work on automatic data virtualization, indexing methods for scientific data, and a novel bit-vectors based sampling method, the goal of this project is to fully develop, disseminate, deploy, and support robust software elements addressing challenges in data transfers and analysis. The prototypes that have been already developed at Ohio State are being extended into two robust software elements: &lt;br/&gt;&lt;br/&gt;Software Element 1: GridPFTP (Grid Partial-File Transport Protocol): An Extention to GridFTP: an extention of GridFTP that allows users to specify a subset of the file to be transferred, avoiding unnecessary transfer of the entire file.&lt;br/&gt;Software Element 2: Parallel Readers for NetCDF and HDF5 for Paraview and VTK: Data subsetting and sampling tools for NetCDF and HDF5 that perform data selection and sampling at the I/O level, and in parallel.&lt;br/&gt;&lt;br/&gt;This project impacts a number of scientific areas, i.e., any area that involves big (and growing) dataset sizes and need for data transfers and/or visualization. This project also contributes to computer science research in `big data&apos;, including scientific (array-based) databases, and visualization. Another contribution will be towards preparation of the broad science and engineering research community for big data handling and analytics.", "ProgramReferenceCode": ["7433", "8005"], "OrganizationZip": "432101016"}, {"OrganizationCity": "Columbus", "OrganizationState": "OH", "Title": "SI2-SSE: Improving Vectorization", "ProgramManager": "Daniel Katz", "State": "OH", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "09/01/2014", "OrganizationStreet": "Office of Sponsored Programs", "EndDate": "08/31/2017", "PrincipalInvestigator": "Ponnuswamy Sadayappan", "ARRAAmount": null, "OrganizationPhone": "6146888735", "AwardNumber": "1440749", "PIEmailAddress": "sadayappan.1@osu.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "07/01/2014", "Organization": "Ohio State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "500000", "Abstract": "The increasing width of vector instruction sets in processors and accelerators raises the importance of effective vectorization. Although the topic of automatic vectorization by compilers has received significant attention over the last few decades, current vectorizing compilers can typically realize only a small fraction of a processor&apos;s peak performance. This project will explore several compiler optimization approaches for generating high-performance vectorized code. Advanced vectorization techniques will be incorporated in the open-source LLVM/Clang compiler through the Polly/LLVM module. A benchmark suite will also be developed, aimed at testing the effectiveness of vectorizing compilers.&lt;br/&gt;&lt;br/&gt;Production compilers limit their optimization search space in order to control the time taken to compile programs. This is because the majority of users expect rapid compile times. However, the developers of high-performance applications are generally very willing to tolerate a much longer wait for program compilation, in return for a boost in the performance of the compiled code. A significant focus of this project will be the development of vectorization approaches for such users who prioritize high application performance over short compile times. The project will investigate semantically-driven pattern-based approaches to vector optimization, a vectorization-friendly approach to tiling, and aggressive vector instruction scheduling approaches that promise higher performance at the expense of possibly high compile times.", "ProgramReferenceCode": ["7433", "8005"], "OrganizationZip": "432101016"}, {"OrganizationCity": "Detroit", "OrganizationState": "MI", "Title": "SI2-SSE: Development of a GPU Accelerated Gibbs Ensemble Monte Carlo Simulation Engine", "ProgramManager": "Daniel Katz", "State": "MI", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "MATERIALS AND SURFACE ENG", "DMR SHORT TERM SUPPORT", "CHEMISTRY PROJECTS", "Software Institutes"], "ProgramElementCode": ["1253", "1633", "1712", "1991", "8004"], "StartDate": "09/01/2012", "OrganizationStreet": "5057 Woodward", "EndDate": "08/31/2016", "PrincipalInvestigator": "Jeffrey Potoff", "ARRAAmount": null, "OrganizationPhone": "3135772424", "AwardNumber": "1148168", "PIEmailAddress": "jpotoff@wayne.edu", "NSFDirectorate": "CSE", "Co-PIName": "Loren Schwiebert", "LastAmendmentDate": "06/07/2013", "Organization": "Wayne State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "336000", "Abstract": "This award supports the development of a general purpose Gibbs Ensemble Monte Carlo (GEMC) simulation engine that uses low-cost graphics processing units (GPU) for acceleration. The primary objectives of this work are to develop and implement: 1) GPU accelerated configurational-bias methods, 2) efficient algorithms for the computation of Ewald sums on the GPU and 3) automated tuning of the code for different GPUs. This work builds on the PIs? existing particle-based GPU-GEMC engine and will introduce functionality that enables the simulation of biological processes and adsorption in porous materials. The code will be written to maintain compatibility with the file formats used by the software packages NAMD and VMD, simplifying simulation setup and data analysis. The resulting simulation engine will be released under the GNU General Public License v3 (GPLv3) and made available to users via the Internet. &lt;br/&gt;&lt;br/&gt;The software tools developed with support from this award will enable high throughput computational screening of materials for CO2 sequestration, improved materials for the stabilization of drug dispersions, and provide molecular level insight to fundamental biological processes such as membrane fusion. The use of graphics processors for the bulk of the computational effort is expected to provide one to two orders of magnitude reduction in computational time compared to traditional serial, CPU bound code, which will allow for the simulation of systems of greater size and complexity than with existing tools. The development of the proposed GPU-accelerated Monte Carlo simulation engine will enhance the cyber-infrastructure of the biology chemistry, chemical engineering, materials science and physics communities. The GPU-GEMC simulation engine will be promoted through conference presentations at national and international meetings, via a dedicated website, and through publication in peer-reviewed literature.&lt;br/&gt;&lt;br/&gt;This award will enhance education at the graduate and undergraduate levels. Research topics from this work will be integrated into existing courses on GPU computing and molecular simulation. Graduate and undergraduate students will have the opportunity to work as part of a multidisciplinary team composed of engineers and computer scientists. Students will be recruited from groups traditionally underrepresented in STEM fields.", "ProgramReferenceCode": ["1982", "7237", "7433", "7569", "7573", "8005", "9215", "9216", "9251", "9263", "1253", "1633", "1712", "1991", "8004"], "OrganizationZip": "482023622"}, {"OrganizationCity": "Troy", "OrganizationState": "NY", "Title": "SI2-SSE: Fast Dynamic Load Balancing Tools for Extreme Scale Systems", "ProgramManager": "Daniel Katz", "State": "NY", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "10/01/2015", "OrganizationStreet": "110 8TH ST", "EndDate": "09/30/2018", "PrincipalInvestigator": "Mark Shephard", "ARRAAmount": null, "OrganizationPhone": "5182766000", "AwardNumber": "1533581", "PIEmailAddress": "shephard@rpi.edu", "NSFDirectorate": "CSE", "Co-PIName": "Cameron Smith", "LastAmendmentDate": "07/08/2015", "Organization": "Rensselaer Polytechnic Institute", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "500000", "Abstract": "Massively parallel computing combined with scalable simulation workflows that can reliably model systems of interest are central to the continued quest of scientists, engineers, and other practitioners to address advances in scientific discovery, engineering design, and medical treatment. However, to meet their potential, these methods must be able to operate efficiently and scale on massively parallel computers executing millions of processes. Reaching the goal of millions of parallel processes requires new methods in which the computational workload is extremely well balanced and interprocessor communications overheads are minimized. Attaining such parallel performance is greatly complicated in realistic simulation workflows where the models and their discrete computer representation must evolve to ensure simulation reliability, or to account for changing input streams. To address the need to obtain workload balance with controlled communications, various algorithms and associated software, referred to as load balancing procedures, have been, and continue to be, developed. To be effective in the execution of simulation workflows in which the workload evolves, the load balancing procedures must be applied dynamically at multiple points in the simulation. Current load balancing techniques demonstrate two deficiencies when applied as dynamic load balancing procedures at very large numbers of compute cores (e.g., greater than 100,000 cores): They become a major fraction of the total parallel computation (in some cases never finishing within an allocation) and they do not maintain good load balance for simulation steps that must balance based on multiple criteria. Building on initial efforts to improve dynamic load balancing methods for adaptive unstructured mesh applications, the goal of the proposed research is to develop fast multicriteria dynamic load balancing methods that are capable of quickly producing well balanced computations, with well controlled communications, for a wide variety of applications. &lt;br/&gt;&lt;br/&gt;An important characteristic of the dynamic load balancing procedures to be developed is generalizing the graph to account for multiple types of computational entities and interactions. The initial ideas for supporting multiple entity types came from consideration balancing finite element calculations that must consider multiple orders of mesh entities. These concepts will be refined and generalized to support multiple applications areas. An additional development will be fast hybrid dynamic load balancing methods that are combinations of &quot;geometric&quot;, standard graph, and multicriteria graph methods in which the individual methods can be executed globally of at a more local level (such as at the node level). The dynamic load balancing method to be developed will be demonstrated on three applications in which the workload, and its distribution, is changing as the simulation proceeds. The applications will be adaptive mesh simulations, adaptive multiscale modeling, and massive scale free graphs. These applications will be carried out on available massively parallel computers where examples on &gt;1 million cores will be demonstrated. A goal of the dynamic load balancing methods to be developed will be to attain scalability, and do so with controlled data movement such that the wall clock time and energy used is substantially less than that required for an equivalent accuracy non-adaptive calculation.&lt;br/&gt;&lt;br/&gt;The software produced by this project will be made available as open source components. These developments coupled with efforts to support users in applying them in the development of new simulation tools will impact many research communities. Based on past and present efforts, the PIs fully expect that technologies developed in this project will also be integrated into future industrial software systems.", "ProgramReferenceCode": ["7433", "8005"], "OrganizationZip": "121803522"}, {"OrganizationCity": "PITTSBURGH", "OrganizationState": "PA", "Title": "SI2-SSE: TestRig 2.0", "ProgramManager": "Daniel Katz", "State": "PA", "Program": ["SPECIAL PROJECTS - CCF", "Software Institutes", "Campus Cyberinfrastrc (CC-NIE)"], "ProgramElementCode": ["2878", "8004", "8080"], "StartDate": "09/01/2015", "OrganizationStreet": "5000 Forbes Avenue", "EndDate": "02/28/2017", "PrincipalInvestigator": "Christopher Rapier", "ARRAAmount": null, "OrganizationPhone": "4122689527", "AwardNumber": "1534836", "PIEmailAddress": "rapier@psc.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "06/29/2015", "Organization": "Carnegie-Mellon University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "287308", "Abstract": "Advances in computation, modeling, and digital collection have led to an exponential growth in the size of scientific data sets. This &apos;Big Data&apos; is transforming the process of scientific discovery. However, many scientific workflows require that the data be transferred, via networks like the Internet, to an optimal location for analysis and collaboration. Unfortunately, scientists often encounter performance problems on these networks. To resolve these problems, researchers depend on the expertise of engineers at Network Operations Centers (NOCs) for diagnosis and resolution. Resolving these problems often requires a cycle of interactions between the user and engineer that can last for days if not weeks, seriously hindering the scientific workflow. This project, known as TestRig 2.0, will short circuit this cycle by deploying a simple, easy to use system that will automatically collect a wide variety of important network diagnostic information. TestRig 2.0 will distribute dynamically generated test environments that are tuned to the specific needs of NOC engineers. TestRig 2.0 will automatically runs a series of tests against the infrastructure, collect the results, and transfer them back to the NOC engineer. TestRig 2.0 will also conducted a coarse analysis of the collected data so as to give the engineers some initial insight into the collected metrics.&lt;br/&gt;&lt;br/&gt;In this project, researchers at Pittsburgh Supercomputing Center (PSC) propose to build a system, TestRig 2.0, that will quickly and easily gather a wide range of important network diagnostic information. This system includes a centralized management server that dynamically creates and configures client optical disk image files (as defined by ISO 9660). This server will be at PSC; it will provide TestRig 2.0 services to multiple geographically diverse NOCs via a free subscription model. These bootable images will provide a known good TCP stack configuration and system environment. TestRig 2.0 will use this baseline environment to perform tests against the perfSONAR monitoring infrastructure and also collect other pertinent data. The resulting data will undergo an initial automated coarse analysis by the client in order to provide engineers initial insight into the data. The resulting analysis and raw data will then be packaged and transferred back to the appropriate NOC for analysis. The client ISO creation process will include a unique dynamically generated public/private key pair. This key pair will be used for the authentication of the client ISO, distribution management, and user/test management. The rapid collection of pertinent data will enhance the process of scientific discovery by allowing researchers to focus on their research, instead of running tests for network engineers. By providing a framework for the use and development of post-collection data analysis, the system will lower the bar required for effective network diagnostics across a wide range of support personnel.", "ProgramReferenceCode": ["7433", "8005"], "OrganizationZip": "152133815"}, {"OrganizationCity": "MISSISSIPPI STATE", "OrganizationState": "MS", "Title": "SI2-CHE: Collaborative Research: Developing First Principles Monte Carlo Methods for Reactive Phase and Sorption Equilibria in CP2K Software Suite", "ProgramManager": "Evelyn M. Goldfield", "State": "MS", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes"], "ProgramElementCode": ["1253", "8004"], "StartDate": "04/15/2013", "OrganizationStreet": "PO Box 6156", "EndDate": "03/31/2016", "PrincipalInvestigator": "Neeraj Rai", "ARRAAmount": null, "OrganizationPhone": "6623257404", "AwardNumber": "1265872", "PIEmailAddress": "neerajrai@che.msstate.edu", "NSFDirectorate": "MPS", "LastAmendmentDate": "07/06/2014", "Organization": "Mississippi State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "CHE", "AwardedAmountToDate": "230558", "Abstract": "An international research team consisting of Ilja Siepmann, Ben Lynch (University of Minnesota), Neeraj Rai (Mississippi State University), Troy Van Voorhis (Massachusetts Institute of Technology), Ben Slater (University College London), Michiel Sprik (University of Cambridge), Adam Carter (Edinburgh Parallel Computing Centre), J&#252;rg Hutter (University of Zurich), I-Feng Kuo (Lawrence Livermore National Laboratory), Christopher Mundy (Pacific Northwest National Laboratory), Joost VandeVondele (ETH Zurich), and Rodolphe Vuilleumier (University Pierre &amp; Marie Curie Paris) is collaborating to develop and implement new theoretical methods in the CP2K computational chemistry software suite. These new methodologies enable the predictive modeling of reactive multi-phase systems, including free energy landscapes and product yields, where the system interactions are described by Kohn-Sham density functional theory with van der Waals and hybrid functionals. Markov chain Monte Carlo approaches utilizing smart moves with asymmetric underlying matrices, such as the aggregation-volume-bias and configurational-bias Monte Carlo methods, and the Gibbs ensemble framework are employed for efficient exploration of the phase space for reactive single- and multi-phase equilibria in bulk and in confinement. The U.S. based research team is supported jointly by the Chemistry Division in MPS and the Office of Cyberinfrastucture. Funds for the UK based research team are provided by the EPSRC.&lt;br/&gt;&lt;br/&gt;The software infrastructure is advanced by the development of efficient and accurate methodologies for reactive phase and sorption equilibria that are applicable to chemical processes in diverse science and engineering applications. The extensively validated methodologies will be incorporated into the open-source CP2K software suite to make them available to a large user base. The new software can be used to identify optimal reaction conditions and separation processes for sustainable chemistry. The collaborative research team plans to use the new software for the investigation of reactive processes that address critical needs of society (fertilizers for food supply, fuels from renewable sources, and environmentally benign chemical processes).", "ProgramReferenceCode": ["5918", "5946", "5950", "7433", "8009", "8650", "9150", "9216", "9251"], "OrganizationZip": "397629662"}, {"OrganizationCity": "AMES", "OrganizationState": "IA", "Title": "Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science", "ProgramManager": "Daniel Katz", "State": "IA", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes"], "ProgramElementCode": ["1253", "8004"], "StartDate": "08/01/2015", "OrganizationStreet": "1138 Pearson", "EndDate": "07/31/2019", "PrincipalInvestigator": "Mark Gordon", "ARRAAmount": null, "OrganizationPhone": "5152945225", "AwardNumber": "1450217", "PIEmailAddress": "mark@si.msg.chem.iastate.edu", "NSFDirectorate": "CSE", "Co-PIName": "Theresa Windus", "LastAmendmentDate": "06/24/2015", "Organization": "Iowa State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1200000", "Abstract": "Computational chemistry has become ubiquitous throughout the fields of science and engineering. Among the many uses of computational chemistry codes are to aid in the design of new materials with specific properties, to understand protein folding dynamics related to the human body, to understand the mechanisms of enzyme catalysis to produce biofuels, and to identify and characterize gaseous species in the atmosphere. Because computational chemistry simulations can require large amounts of computer, memory and disk space, one focus of this research is to reduce the demands on computer resources by exploiting methods that can efficiently fragment large molecules into smaller pieces and at the same time take advantage of computers that have many thousands of computer cores. The principal investigators are developers of several computational chemistry programs, each of which has unique functionalities that have taken multiple person-years to develop and implement. In order to minimize further development efforts and to maximize the utility of the unique features, a second key focus of this research will be to develop the ability of each program to access the unique features and data of the other programs. A major bottleneck in the effort to construct high performance computers with more and more compute cores is the power that is required to drive such systems. One way that the research team will address the power issue is to explore the utility of low power architectures, such as graphical processing units for computational chemistry calculations. All of the newly developed codes will be made available to the user community by web download at no cost. This project is supported by programs in the Division of Chemistry in MPS and the Division of Advanced Cyberinfrastructure in CISE. &lt;br/&gt;&lt;br/&gt;This project presents an integrated computational science approach to very high quality electronic structure and dynamics calculations that will (a) be broadly accessible to both the development and applications communities, (b) have the capability to address problems of great interest, such as the properties of liquids and solvent effects, and photochemical/photobiological dynamics with high accuracy, (c) provide interoperability and sustainability into the foreseeable future, and (d) solve bottlenecks including power consumption using accelerators. A primary goal is to provide to the broad community new, accurate approaches that may be easily used, with a clear path forward to further code improvement and development. As part of the proposed effort, in addition to the usual outlets of journal articles and public presentations, the investigators will organize workshops at prominent national meetings, so that the expertise and software developed will be available to as broad a group of users as possible. All of the developed codes will be available on the web for easy downloads. The proposed research will develop new paradigms for interoperability, with a focus on the highly popular program suites GAMESS, NWChem, PSI4 and the AIMS dynamics code. Also included will be a cloud-based client-server model, a common quantum chemistry driver and novel data management approaches. The integration of the codes will be accomplished by making use of the combined expertise of the PIs in developing interoperable methods and data interfaces in computational science. In addition, several new methods will be developed, including novel explicit (R12) correlation methods that will be integrated with the most accurate levels of theory: multi-reference and the most accurate and novel coupled cluster methods, as well as the derivation and implementation of analytic derivatives. Applicability to large molecular systems will be made feasible by drawing upon novel fragmentation methods that scale nearly perfectly to the petascale.", "ProgramReferenceCode": ["7433", "8009", "9150"], "OrganizationZip": "500112207"}, {"OrganizationCity": "Atlanta", "OrganizationState": "GA", "Title": "Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science", "ProgramManager": "Daniel Katz", "State": "GA", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes"], "ProgramElementCode": ["1253", "8004"], "StartDate": "08/01/2015", "OrganizationStreet": "Office of Sponsored Programs", "EndDate": "07/31/2019", "PrincipalInvestigator": "Charles Sherrill", "ARRAAmount": null, "OrganizationPhone": "4048944819", "AwardNumber": "1449723", "PIEmailAddress": "sherrill@gatech.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "06/24/2015", "Organization": "Georgia Tech Research Corporation", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "600000", "Abstract": "Computational chemistry has become ubiquitous throughout the fields of science and engineering. Among the many uses of computational chemistry codes are to aid in the design of new materials with specific properties, to understand protein folding dynamics related to the human body, to understand the mechanisms of enzyme catalysis to produce biofuels, and to identify and characterize gaseous species in the atmosphere. Because computational chemistry simulations can require large amounts of computer, memory and disk space, one focus of this research is to reduce the demands on computer resources by exploiting methods that can efficiently fragment large molecules into smaller pieces and at the same time take advantage of computers that have many thousands of computer cores. The principal investigators are developers of several computational chemistry programs, each of which has unique functionalities that have taken multiple person-years to develop and implement. In order to minimize further development efforts and to maximize the utility of the unique features, a second key focus of this research will be to develop the ability of each program to access the unique features and data of the other programs. A major bottleneck in the effort to construct high performance computers with more and more compute cores is the power that is required to drive such systems. One way that the research team will address the power issue is to explore the utility of low power architectures, such as graphical processing units for computational chemistry calculations. All of the newly developed codes will be made available to the user community by web download at no cost.&lt;br/&gt;&lt;br/&gt;This project presents an integrated computational science approach to very high quality electronic structure and dynamics calculations that will (a) be broadly accessible to both the development and applications communities, (b) have the capability to address problems of great interest, such as the properties of liquids and solvent effects, and photochemical/photobiological dynamics with high accuracy, (c) provide interoperability and sustainability into the foreseeable future, and (d) solve bottlenecks including power consumption using accelerators. A primary goal is to provide to the broad community new, accurate approaches that may be easily used, with a clear path forward to further code improvement and development. As part of the proposed effort, in addition to the usual outlets of journal articles and public presentations, the investigators will organize workshops at prominent national meetings, so that the expertise and software developed will be available to as broad a group of users as possible. All of the developed codes will be available on the web for easy downloads. The proposed research will develop new paradigms for interoperability, with a focus on the highly popular program suites GAMESS, NWChem, PSI4 and the AIMS dynamics code. Also included will be a cloud-based client-server model, a common quantum chemistry driver and novel data management approaches. The integration of the codes will be accomplished by making use of the combined expertise of the PIs in developing interoperable methods and data interfaces in computational science. In addition, several new methods will be developed, including novel explicit (R12) correlation methods that will be integrated with the most accurate levels of theory: multi-reference and the most accurate and novel coupled cluster methods, as well as the derivation and implementation of analytic derivatives. Applicability to large molecular systems will be made feasible by drawing upon novel fragmentation methods that scale nearly perfectly to the petascale.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "303320420"}, {"OrganizationCity": "AMES", "OrganizationState": "IA", "Title": "SI2-SSE: A Next-Generation Open-Source Computational Fluid Dynamic Code for Polydisperse Multiphase Flows in Science and Engineering", "ProgramManager": "Daniel Katz", "State": "IA", "Program": ["FLUID DYNAMICS", "Software Institutes"], "ProgramElementCode": ["1443", "8004"], "StartDate": "10/01/2014", "OrganizationStreet": "1138 Pearson", "EndDate": "09/30/2017", "PrincipalInvestigator": "Alberto Passalacqua", "ARRAAmount": null, "OrganizationPhone": "5152945225", "AwardNumber": "1440443", "PIEmailAddress": "albertop@iastate.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Rodney Fox", "Simanta Mitra"], "LastAmendmentDate": "08/21/2014", "Organization": "Iowa State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "499551", "Abstract": "Many processes for the production of drugs, fuels and plastic materials, as well as energy from coal or biomasses, involve multiphase flows, which are composed by a combination of a fluid, either liquid or gas, and particles, droplets or bubbles. This type of flow is also naturally present in the environment. Examples are the formation of a mixture of air and solid particles due to volcanic eruptions, and particles of sand and other materials transported by the wind. Scientists and engineers use software to study how these flows behave in order to improve the yield of industrial processes, reduce their environmental impact, and energy consumption. The computer programs used to perform these studies solve complex mathematical problems, and require powerful computers to be able to obtain the results in a useful time. This project focuses on developing the next generation of computer software for the simulation of multiphase flows, enabling it to use the latest generation of computers which combine traditional and graphical processors for improved performance. This software will be released to the public and will enable, scientists and engineers from different research areas to tackle real-world problems by taking advantage of the latest developments in multiphase flow science, combined with the benefit of being able to use the software on powerful computer infrastructures. Students and educators will be able to use the software and learn about multiphase flows through the examples and the documentation that will be provided.&lt;br/&gt;&lt;br/&gt;The objectives of the project will be achieved by first developing computational models to describe turbulent flows in the framework of quadrature-based moment methods, an efficient and accurate approach to describe this type of flows. These computational models will then be implemented, together with appropriate numerical methods that will ensure the accuracy of the computational codes, in the open-source framework OpenFOAM. Three representative problems of typical multiphase flows will be considered: a population balance equation for particles with negligible inertia, such as in the formation of nanoparticles in a fluid flow; the description of gas-liquid flows, where the bubble inertia is small but not zero; and the most complex case of gas-solid flows, where particle inertia is large. The possibility of using graphical processing units will be added to the OpenFOAM framework, to enable it to run on hybrid computational systems involving traditional processors and graphical processing units. The source code and its documentation will be made available to the public at an early stage of their development, under the GNU GPL 3 license, in order to disseminate the results of the research and gather feedback. Detailed code documentation, verification and validation cases, and tutorials will also be created to favor external contributions to the software.", "ProgramReferenceCode": ["058E", "7433", "8005", "9150", "1443"], "OrganizationZip": "500112207"}, {"OrganizationCity": "BLACKSBURG", "OrganizationState": "VA", "Title": "Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science", "ProgramManager": "Daniel Katz", "State": "VA", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes"], "ProgramElementCode": ["1253", "8004"], "StartDate": "08/01/2015", "OrganizationStreet": "Sponsored Programs 0170", "EndDate": "07/31/2019", "PrincipalInvestigator": "Thomas Crawford", "ARRAAmount": null, "OrganizationPhone": "5402315281", "AwardNumber": "1450169", "PIEmailAddress": "crawdad@vt.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "06/24/2015", "Organization": "Virginia Polytechnic Institute and State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "600000", "Abstract": "Computational chemistry has become ubiquitous throughout the fields of science and engineering. Among the many uses of computational chemistry codes are to aid in the design of new materials with specific properties, to understand protein folding dynamics related to the human body, to understand the mechanisms of enzyme catalysis to produce biofuels, and to identify and characterize gaseous species in the atmosphere. Because computational chemistry simulations can require large amounts of computer, memory and disk space, one focus of this research is to reduce the demands on computer resources by exploiting methods that can efficiently fragment large molecules into smaller pieces and at the same time take advantage of computers that have many thousands of computer cores. The principal investigators are developers of several computational chemistry programs, each of which has unique functionalities that have taken multiple person-years to develop and implement. In order to minimize further development efforts and to maximize the utility of the unique features, a second key focus of this research will be to develop the ability of each program to access the unique features and data of the other programs. A major bottleneck in the effort to construct high performance computers with more and more compute cores is the power that is required to drive such systems. One way that the research team will address the power issue is to explore the utility of low power architectures, such as graphical processing units for computational chemistry calculations. All of the newly developed codes will be made available to the user community by web download at no cost.&lt;br/&gt;&lt;br/&gt;This project presents an integrated computational science approach to very high quality electronic structure and dynamics calculations that will (a) be broadly accessible to both the development and applications communities, (b) have the capability to address problems of great interest, such as the properties of liquids and solvent effects, and photochemical/photobiological dynamics with high accuracy, (c) provide interoperability and sustainability into the foreseeable future, and (d) solve bottlenecks including power consumption using accelerators. A primary goal is to provide to the broad community new, accurate approaches that may be easily used, with a clear path forward to further code improvement and development. As part of the proposed effort, in addition to the usual outlets of journal articles and public presentations, the investigators will organize workshops at prominent national meetings, so that the expertise and software developed will be available to as broad a group of users as possible. All of the developed codes will be available on the web for easy downloads. The proposed research will develop new paradigms for interoperability, with a focus on the highly popular program suites GAMESS, NWChem, PSI4 and the AIMS dynamics code. Also included will be a cloud-based client-server model, a common quantum chemistry driver and novel data management approaches. The integration of the codes will be accomplished by making use of the combined expertise of the PIs in developing interoperable methods and data interfaces in computational science. In addition, several new methods will be developed, including novel explicit (R12) correlation methods that will be integrated with the most accurate levels of theory: multi-reference and the most accurate and novel coupled cluster methods, as well as the derivation and implementation of analytic derivatives. Applicability to large molecular systems will be made feasible by drawing upon novel fragmentation methods that scale nearly perfectly to the petascale.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "240610001"}, {"OrganizationCity": "Palo Alto", "OrganizationState": "CA", "Title": "Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science", "ProgramManager": "Daniel Katz", "State": "CA", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes"], "ProgramElementCode": ["1253", "8004"], "StartDate": "08/01/2015", "OrganizationStreet": "3160 Porter Drive", "EndDate": "07/31/2019", "PrincipalInvestigator": "Todd Martinez", "ARRAAmount": null, "OrganizationPhone": "6507232300", "AwardNumber": "1450179", "PIEmailAddress": "Todd.Martinez@stanford.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "06/24/2015", "Organization": "Stanford University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "600000", "Abstract": "Computational chemistry has become ubiquitous throughout the fields of science and engineering. Among the many uses of computational chemistry codes are to aid in the design of new materials with specific properties, to understand protein folding dynamics related to the human body, to understand the mechanisms of enzyme catalysis to produce biofuels, and to identify and characterize gaseous species in the atmosphere. Because computational chemistry simulations can require large amounts of computer, memory and disk space, one focus of this research is to reduce the demands on computer resources by exploiting methods that can efficiently fragment large molecules into smaller pieces and at the same time take advantage of computers that have many thousands of computer cores. The principal investigators are developers of several computational chemistry programs, each of which has unique functionalities that have taken multiple person-years to develop and implement. In order to minimize further development efforts and to maximize the utility of the unique features, a second key focus of this research will be to develop the ability of each program to access the unique features and data of the other programs. A major bottleneck in the effort to construct high performance computers with more and more compute cores is the power that is required to drive such systems. One way that the research team will address the power issue is to explore the utility of low power architectures, such as graphical processing units for computational chemistry calculations. All of the newly developed codes will be made available to the user community by web download at no cost.&lt;br/&gt;&lt;br/&gt;This project presents an integrated computational science approach to very high quality electronic structure and dynamics calculations that will (a) be broadly accessible to both the development and applications communities, (b) have the capability to address problems of great interest, such as the properties of liquids and solvent effects, and photochemical/photobiological dynamics with high accuracy, (c) provide interoperability and sustainability into the foreseeable future, and (d) solve bottlenecks including power consumption using accelerators. A primary goal is to provide to the broad community new, accurate approaches that may be easily used, with a clear path forward to further code improvement and development. As part of the proposed effort, in addition to the usual outlets of journal articles and public presentations, the investigators will organize workshops at prominent national meetings, so that the expertise and software developed will be available to as broad a group of users as possible. All of the developed codes will be available on the web for easy downloads. The proposed research will develop new paradigms for interoperability, with a focus on the highly popular program suites GAMESS, NWChem, PSI4 and the AIMS dynamics code. Also included will be a cloud-based client-server model, a common quantum chemistry driver and novel data management approaches. The integration of the codes will be accomplished by making use of the combined expertise of the PIs in developing interoperable methods and data interfaces in computational science. In addition, several new methods will be developed, including novel explicit (R12) correlation methods that will be integrated with the most accurate levels of theory: multi-reference and the most accurate and novel coupled cluster methods, as well as the derivation and implementation of analytic derivatives. Applicability to large molecular systems will be made feasible by drawing upon novel fragmentation methods that scale nearly perfectly to the petascale.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "943041212"}, {"OrganizationCity": "Lexington", "OrganizationState": "KY", "Title": "SI2-SSI Collaborative Research: A Computational Materials Data and Design Environment", "ProgramManager": "Daniel Katz", "State": "KY", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "DMR SHORT TERM SUPPORT", "CHEMISTRY PROJECTS", "Software Institutes"], "ProgramElementCode": ["1253", "1712", "1991", "8004"], "StartDate": "10/01/2012", "OrganizationStreet": "109 Kinkead Hall", "EndDate": "09/30/2017", "PrincipalInvestigator": ["Raphael Finkel", "Alan Dozier"], "ARRAAmount": null, "OrganizationPhone": "8592579420", "AwardNumber": "1147466", "PIEmailAddress": ["raphael@cs.uky.edu", "adozier@uky.edu"], "NSFDirectorate": "CSE", "Co-PIName": "Raphael Finkel", "LastAmendmentDate": "09/11/2013", "Organization": "University of Kentucky Research Foundation", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "298999", "Abstract": "TECHNICAL SUMMARY&lt;br/&gt;The Office of Cyberinfrastructure, Division of Materials Research, and Chemistry Division contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation. This award supports development of new theory and tools to enable rapid and efficient calculation of atomic level material properties. The incredible advances in computing power and tools of atomic scale simulation have now made it possible to predict critical properties for existing and new materials without experimental input. However, present simulation approaches typically require researchers to perform many steps by hand, which is both slow and error prone compared to what a computer can do. Through computer codes that automate the tasks in first principles modeling human bottlenecks can be removed and predictive capabilities of first principles simulation techniques can be accelerated by orders of magnitude. Such a high-throughput computing approach will enable generation of critical materials data on an unprecedented scale and open new doors for material science.&lt;br/&gt;&lt;br/&gt;The team will develop tools for the specific challenges of predicting point defect properties, atomic diffusion, and surface stability, with a focus on automating steps to enable computations on a massive scale. The PIs will use state-of-the-art first principles quantum mechanical methods. Best practices for treating the multiple issues of charged defect calculations, for example convergence with cell size and band gap errors, will be refined and automated for rapid execution. Similarly, tools to identify diffusion pathways and determine their barriers will be streamlined to allow users to quickly identify transport properties of new systems. New theoretical approaches to modeling charged surfaces will be developed to enable simulation of surfaces in more realistic environments. This award will support prediction of properties that play a critical role in advancing a wide range of technologies, from improving semiconductors for next generation computers to better fuel cells for more efficient energy conversion. Software tools and data produced by this effort will enable researchers to predict properties for thousands of materials with almost no human effort, accelerating the pace at which researchers can develop new materials technologies.&lt;br/&gt;&lt;br/&gt;Software and data developed from this award will be shared with academic and industrial researchers through modules on the web, scientific journals and presentations at national and international conferences. This award supports two workshops to educate researchers about the latest opportunities to use high-throughput computing of atomic scale properties for materials development. Students will be trained to work at the critical interface of the computer and physical sciences, supporting a generation of scientists who use modern computers to their fullest potential to develop new understanding and technology.&lt;br/&gt;&lt;br/&gt;NON-TECHNICAL SUMMARY&lt;br/&gt;The Office of Cyberinfrastructure, Division of Materials Research, and Chemistry Division contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation. This award supports development of new theory and tools to enable rapid and efficient calculation of atomic level material properties. The incredible advances in computing power and tools of atomic scale simulation have now made it possible to predict critical properties for existing and new materials without experimental input. However, present simulation approaches typically require researchers to perform many steps by hand, which is both slow and error prone compared to what a computer can do. Through computer codes that automate the tasks in first-principles modeling human bottlenecks can be removed and predictive capabilities of first principles simulation techniques can be accelerated by orders of magnitude. Such a high-throughput computing approach will enable generation of critical materials data on an unprecedented scale and open new doors for material science.&lt;br/&gt;&lt;br/&gt;The team will develop tools for the specific challenges of predicting point defect properties, atomic diffusion, and surface stability, with a focus on automating steps to enable computations on a massive scale. These properties play a critical role in advancing a wide range of technologies, from improving semiconductors for next generation computers to better fuel cells for more efficient energy conversion. Software tools and data produced by this effort will enable researchers to predict properties for thousands of materials with almost no human effort, accelerating the pace at which researchers can develop new materials technologies.&lt;br/&gt;&lt;br/&gt;Software and data developed from this award will be shared with academic and industrial researchers through modules on the web, scientific journals and presentations at national and international conferences. In particular, this award will support two workshops to educate researchers about the latest opportunities to use high-throughput computing of atomic scale properties for materials development. This award will train students to work at the critical interface of the computer and physical sciences, supporting a generation of scientists who use modern computers to their fullest potential to develop new understanding and technology.", "ProgramReferenceCode": ["1982", "7237", "7433", "7569", "7644", "8009", "9150", "9216", "9263", "1253", "1712", "1991", "8004"], "OrganizationZip": "405260001"}, {"OrganizationCity": "Lincoln", "OrganizationState": "NE", "Title": "Collaborative Research: SI2-SSI: Data-Intensive Analysis for High Energy Physics (DIANA/HEP)", "ProgramManager": "Daniel Katz", "State": "NE", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "COMPUTATIONAL PHYSICS", "Software Institutes"], "ProgramElementCode": ["1253", "7244", "8004"], "StartDate": "05/01/2015", "OrganizationStreet": "2200 Vine St, 151 Whittier", "EndDate": "04/30/2019", "PrincipalInvestigator": "Brian Bockelman", "ARRAAmount": null, "OrganizationPhone": "4024723171", "AwardNumber": "1450323", "PIEmailAddress": "bbockelm@cse.unl.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "05/07/2015", "Organization": "University of Nebraska-Lincoln", "AwardInstrument": "Continuing grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "806324", "Abstract": "Advanced software plays a fundamental role for large scientific projects. The primary goal of DIANA/HEP (Data Intensive ANAlysis for High Energy Physics) is developing state-of-the-art tools for experiments that acquire, reduce, and analyze petabytes of data. Improving performance, interoperability, and collaborative tools through modifications and additions to packages broadly used by the community will allow users to more fully exploit the data being acquired at CERN&apos;s Large Hadron Collider (LHC) and other facilities. These experiments are addressing questions at the heart of physics: What are the underlying constituents of matter? And how do they interact? With the discovery of the Higgs boson in 2012, the Standard Model of particle physics is complete. It provides an excellent description of known particles and forces. However, the most interesting questions remain open: What is the dark matter which pervades the universe? Does space-time have additional symmetries or extend beyond the 3 spatial dimensions we know? What is the mechanism stabilizing the Higgs boson mass from enormous quantum corrections? The next generation of experiments will collect exabyte-scale data samples to provide answers. Analyzing this data will require new and better tools. &lt;br/&gt;&lt;br/&gt;First, the project will provide the CPU and IO performance needed to reduce the iteration time so crucial to explore new ideas. It will develop software to effectively exploit emerging many- and multi-core hardware. It will establish infrastructure for a higher-level of collaborative analysis, building on the successful patterns used for the Higgs boson discovery and enabling a deeper communication between the theoretical community and the experimental community. DIANA?s products will sit in the ROOT framework, already used by the HEP community of more than 10000 particle and nuclear physicists. By improving interoperability with the larger scientific software ecosystem, DIANA will incorporate best practices and algorithms from other disciplines into HEP. Similarly, the project will make its computing insights, tools, and novel ideas related to collaborative analysis, standards for data preservation, and best practices for treating software as a research product available to the larger scientific community. Finally, to improve the quality of the next generation of software engineers in HEP, DIANA will host an annual workshop on analysis tools and establish a fellowship program.", "ProgramReferenceCode": ["7433", "8009", "8084", "9150"], "OrganizationZip": "685031435"}, {"OrganizationCity": "College Station", "OrganizationState": "TX", "Title": "Collaborative Research: SI2-SSI: Open Source Support for Massively Parallel, Generic Finite Element Methods", "ProgramManager": "Daniel Katz", "State": "TX", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "08/01/2012", "OrganizationStreet": "400 Harvey Mitchell Parkway, S", "EndDate": "07/31/2016", "PrincipalInvestigator": "Wolfgang Bangerth", "ARRAAmount": null, "OrganizationPhone": "9798458600", "AwardNumber": "1148116", "PIEmailAddress": "bangerth@math.tamu.edu", "NSFDirectorate": "CSE", "Co-PIName": "Yassin Hassan", "LastAmendmentDate": "08/09/2012", "Organization": "Texas A&M Research Foundation", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1311834", "Abstract": "Partial differential equations are used in a wide variety of applications as&lt;br/&gt;mathematical models. Their numerical solution is, consequently, of prime&lt;br/&gt;importance for the accurate simulation and optimization of processes in the&lt;br/&gt;sciences, engineering, and beyond.&lt;br/&gt;The last decade saw the emergence of large and successful libraries that&lt;br/&gt;support such applications. While these libraries provide most of what such&lt;br/&gt;codes need for small-scale computations, many realistic applications yield&lt;br/&gt;problems of hundreds of millions or billions of unknowns and require clusters&lt;br/&gt;with thousands of processor cores, but there is currently little generic&lt;br/&gt;support for such problems, limiting access to the many large publicly&lt;br/&gt;supported computing facilities to experts in computational science and&lt;br/&gt;excluding scientists from many fields for whom computational simulation would&lt;br/&gt;be a useful tool. This project intends to build the software infrastructure that will allow a&lt;br/&gt;wide cross section of scientists to utilize these large resources.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project intends to support the software infrastructure for the&lt;br/&gt;large-scale solution of partial differential equations on massively parallel&lt;br/&gt;computational resources in a generic way. It will build on two of the most&lt;br/&gt;successful libraries for scientific computing, the finite element library&lt;br/&gt;deal.II, and Trilinos that provides the parallel linear algebra capabilities&lt;br/&gt;for the former. Specifically, we will: (i) Make support for massively parallel&lt;br/&gt;computations ubiquitous in deal.II; (ii) Research and develop seamless support&lt;br/&gt;for problems with billions of unknowns in both libraries and improve the&lt;br/&gt;interaction between the two; (iii) Exploit intra-node parallelism on today&apos;s&lt;br/&gt;clusters; (iv) Ensure the applicability of our work on a broad basis by&lt;br/&gt;implementing two real-world applications. &lt;br/&gt;Both deal.II and Trilinos have large, active and diverse developer and user&lt;br/&gt;communities, and this project will actively engage these communities through&lt;br/&gt;user meetings, short courses, regularly taught classes, mailing lists, and&lt;br/&gt;direct contact in focused projects.", "ProgramReferenceCode": ["7433", "8009", "8004"], "OrganizationZip": "778454321"}, {"OrganizationCity": "Chicago", "OrganizationState": "IL", "Title": "SI2-SSE: Solving Polynomial Systems with PHCpack and phcpy", "ProgramManager": "Daniel Katz", "State": "IL", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes", "CDS&E-MSS"], "ProgramElementCode": ["1253", "8004", "8069"], "StartDate": "10/01/2014", "OrganizationStreet": "809 S MARSHFIELD RM 608", "EndDate": "09/30/2017", "PrincipalInvestigator": "Jan Verschelde", "ARRAAmount": null, "OrganizationPhone": "3129962862", "AwardNumber": "1440534", "PIEmailAddress": "jan@math.uic.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/01/2014", "Organization": "University of Illinois at Chicago", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "464352", "Abstract": "Solving polynomial systems is a fundamental problem in mathematics with applications to various fields of science and engineering. The free and open source software PHCpack applies symbolic-numeric and polyhedral methods to solve polynomial systems. As a new interface to PHCpack written in the Python scripting language, phcpy improves the functionality of PHCpack. The implementation on parallel computers to compensate for the cost overhead of multi-precision arithmetic will enable scientists and engineers to solve larger systems faster and more accurately. A web server developed with phcpy will give anyone with an internet connection access to the developed software.&lt;br/&gt;&lt;br/&gt;The solvers in PHCpack apply homotopy continuation methods, blending symbolic-numeric with polyhedral algorithms. Numerical approximations to solutions are computed with Newton&apos;s method. Solutions are approximated symbolically by Puiseux series, which originate at initial forms defined by the Newton polytopes of the polynomials in the system. The design of phcpy gives a flexible interactive scripting interface, without sacrificing efficiency as compiled code in PHCpack is executed. The package phcpy will provide the tools for a scalable compute server to serve requests submitted to the web server. Multithreaded implementations on multicore processors accelerated by graphics processing units compensate for the cost overhead of double double and quad double arithmetic.", "ProgramReferenceCode": ["7433", "8005", "8251"], "OrganizationZip": "606124305"}, {"OrganizationCity": "Los Angeles", "OrganizationState": "CA", "Title": "SI2-SSI: Collaborative: The XScala Project: A Community Repository for Model-Driven Design and Tuning of Data-Intensive Applications for Extreme-Scale Accelerator-Based Systems", "ProgramManager": "Rajiv Ramnath", "State": "CA", "Program": "CYBERINFRASTRUCTURE", "ProgramElementCode": "7231", "StartDate": "10/01/2013", "OrganizationStreet": "University Park", "EndDate": "09/30/2016", "PrincipalInvestigator": "Viktor Prasanna", "ARRAAmount": null, "OrganizationPhone": "2137407762", "AwardNumber": "1339756", "PIEmailAddress": "prasanna@usc.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "09/16/2013", "Organization": "University of Southern California", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "748914", "Abstract": "The increasing gap between processor and memory performance -- referred to as the memory wall -- has led high-performance computing vendors to design and incorporate new accelerators into their next-generation systems. Representative accelerators include recon&amp;#64257;gurable hardware such as FPGAs, heterogeneous processors such as CPU+GPU processors, highly multicore and multithreaded processors, and manycore co-processors and general-purpose graphics processing units, among others. These accelerators contain myriad innovative architectural features, including explicit control of data motion, large-scale SIMD/vector processing, and multithreaded stream processing. Such features provide abundant opportunities for developers to achieve high-performance for applications that were previously deemed hard to optimize. This project aims to develop tools that will assist developers in using hardware accelerators (co-processors) productively and effectively.&lt;br/&gt;&lt;br/&gt;This project&apos;s specific technical focus is on data-intensive kernels including large dictionary string matching, dynamic programming, graph theory, and sparse matrix computations that arise in the domains of biology, network security, and the social sciences. The project is developing XScala, a software framework for designing efficient accelerator kernels. The framework contains a variety of design time and run-time performance optimization tools. The project concentrates on data-intensive kernels, bound by data movement. It proposes optimization techniques including (a) enhancing and exploiting maximal concurrency to hide data movement; (b) algorithmic reorganization to improve spatial and/or temporal locality; (c) data structure transformations to improve locality or reduce the size of the data (compressed structures); and (d) prefetching, among others. The project is also developing a public software repository and forum, called the XBazaar, for community-developed accelerator kernels. This project includes workshops, tutorials, and the PIs class and summer projects as various means by which to increase community involvement. The broader impacts include productive use of emerging classes of accelerator-augmented computer systems; creation of an open and accessible community repository, the XBazaar, for distributing accelerator-tuned computational kernels, software, and models; training of graduate and undergraduate students; and dissemination through publications, presentations at scientific meetings, lectures, workshops, and tutorials. The framework itself will be released as open-source code and as precompiled binaries for several common platforms, through the XBazaar, as an initial step toward building a community around accelerator kernels.", "ProgramReferenceCode": ["7433", "8009", "9145"], "OrganizationZip": "900890001"}, {"OrganizationCity": "Pittsburgh", "OrganizationState": "PA", "Title": "SI2-SSE: EASE: Improving Research Accountability through Artifact Evaluation", "ProgramManager": "Daniel Katz", "State": "PA", "Program": ["SPECIAL PROJECTS - CCF", "Software Institutes"], "ProgramElementCode": ["2878", "8004"], "StartDate": "09/01/2015", "OrganizationStreet": "University Club", "EndDate": "08/31/2018", "PrincipalInvestigator": "Bruce Childers", "ARRAAmount": null, "OrganizationPhone": "4126247400", "AwardNumber": "1535232", "PIEmailAddress": "childers@cs.pitt.edu", "NSFDirectorate": "CSE", "Co-PIName": "Daniel Mosse'", "LastAmendmentDate": "06/16/2015", "Organization": "University of Pittsburgh", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "499515", "Abstract": "Research in computer systems, particularly in the first stages of creating a new innovation, relies almost exclusively on software prototypes, simulators, benchmarks, and data sets to understand the benefits and costs of new ideas in computers, ranging from consumer devices to exascale systems. These artifacts are used to evaluate new capabilities, algorithms, bottlenecks and trade-offs. Empirical study is behind the rapid pace of innovation in creating faster, lower energy and more reliable systems. This experimental approach lies at the core of development that fuels the nation&apos;s information economy. Given the critical importance of experimental study to developing new computer systems, several efforts are underway to curate experimental results through accountable research. One effort, Artifact Evaluation (AE), is being adopted to promote high quality artifacts and experimentation, including making public the experimental information necessary for reproducibility. However, the rapid adoption of AE is hampered by technical challenges that create a high barrier to the process: there is no consistent or simple environment, or mechanism, to package and reproduce experiments for AE. Authors rely on their own approaches, leading to much time consumed, as well as considerable variability in the ways materials are prepared and evaluated, unnecessarily obstructing the AE process. &lt;br/&gt;&lt;br/&gt;To overcome the technical challenges with AE, and to more broadly encourage adoption of AE in computer science and engineering research, this project is developing a software infrastructure, Experiment and Artifact System for Evaluation (EASE), to create and run experiments specifically for AE, in which authors create, conduct and share artifacts and experiments. It allows for repeating, modifying, and extending experiments. Authors may also use EASE to package and upload their experiments for archival storage in a digital library. EASE is being developed and deployed for two use cases, namely compilers and real-time systems, keeping the project tractable to address specific needs. These communities have overlapping but also distinct requirements, helping to ensure EASE can also be extended and&lt;br/&gt;used by other computer systems research communities as well.&lt;br/&gt;&lt;br/&gt;EASE will be release as open source software, based on an Experiment Management System (EMS) previously developed by the project investigator in a project call Open Curation for Computer Architecture Modeling (OCCAM), used to define and conduct experiments using computer architecture simulators. Using EMS as a starting point, EASE will provide AE support, by: 1) separating EMS from OCCAM&apos;s repository and hardware services, transforming the EMS infrastructure into EASE, a fully standalone, sustainable, and extensible platform for AE; 2) supporting record and replay (for repeating and reproducing results, as well as provenance) of artifacts and experiments as part of normal development and experimental practice to ease participation in AE by authors and evaluators; 3) supporting artifacts,workflows of artifacts and experiments that run directly on a machine, including specialized hardware and software, and run indirectly on a simulator or emulator; 4) allowing both user-level (artifacts and experiments as user processes) and system-level (artifacts and experiments involving kernel changes) innovations; 5) providing consistent/uniform access, whether locally or remotely, to artifacts and experiments; 6) simplifying viewing, running, modifying, and comparing experiments by innovators (i.e., during innovation development), artifact evaluators (during AE), and archive users (after publication); 7) enabling indexing (object locators and search tags) and packaging of artifacts and experiments for AE and for archival deployment (e.g., to ACM?s or IEEE?s Digital Library); and 8) refining, expanding, generalizing, and documenting EASE to ensure it is robust, maintainable and extensible, and that it can be used and sustained by different CSR communities (starting with real-time and compilers, given their different artifacts, data and methods).", "ProgramReferenceCode": ["7433", "8005"], "OrganizationZip": "152132303"}, {"OrganizationCity": "SALT LAKE CITY", "OrganizationState": "UT", "Title": "SI2-SSE: Scalable Multifaceted Graphical Processing Unit (GPU) Program Debugging", "ProgramManager": "Daniel Katz", "State": "UT", "Program": ["SPECIAL PROJECTS - CCF", "Software Institutes"], "ProgramElementCode": ["2878", "8004"], "StartDate": "09/01/2015", "OrganizationStreet": "75 S 2000 E", "EndDate": "08/31/2017", "PrincipalInvestigator": "Ganesh Gopalakrishnan", "ARRAAmount": null, "OrganizationPhone": "8015816903", "AwardNumber": "1535032", "PIEmailAddress": "ganesh@cs.utah.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "06/15/2015", "Organization": "University of Utah", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "417482", "Abstract": "Modern scientific research crucially depends on software simulations that help model scientific phenomena, and accelerate the process of discoveries, and communal result sharing. With the availability of affordable computational accelerators known as GPUs, the scientific community has begun migrating their existing CPU codes as well as creating new codes targeting GPUs. Unfortunately, this has resulted in a situation where the generated scientific results do not often agree across CPUs and GPUs. This exacerbates the danger of drawing wrong conclusions in crucial areas such as physics, weather simulations, drug discovery, and engineering computations. This project offers a combination of existing and new techniques in dissecting scientific experiments conducted through simulations, obtaining believable results, finding the root causes of varying results, and developing best practices to ensure higher result fidelity. Its techniques have special emphasis on GPUs, given their often poorly specified and evolving nature.&lt;br/&gt;&lt;br/&gt;Result variability has many causes, including evolving, incorrect, or ambiguous specifications of computer hardware and software, racing data accesses, varying floating point precision standards, and incorrect result association within compound computational steps. This project develops methods that help a scientist systematically search through and eliminate these causes, thus accelerating the process of debugging result variability. The produced tools and exemplars of known erroneous behaviors allow a scientist to avoid the use of incorrect specifications, isolate and eliminate data races, and isolate and eliminate unreliable numerical steps. It also develops methods that help a scientist maintain focus on their basic scientific pursuits while still keeping up with technology evolution. It trains students in critical software engineering techniques that help the nation build the talent pool necessary for the extreme scale computing era.&lt;br/&gt;&lt;br/&gt;The project will combine six research thrusts (GPU concurrency; challenge problems and develop user interfaces; pedagogy for domain scientists; improved GPU concurrency debugging tool support; more reproducible simulation results; and evolving and scaling tools with standards) to build and deliver open source software that incorporates proven stress-testing methods into tools; builds challenge problems, supports formalization support, and designs the user interface; delivers demos, books, and tutorials that help illustrate concurrency nuances; exploits symbolic analysis for input generation in mixed formal and GPU runs; develops stress testing inputs for round-off errors and separable verification to root-cause roundoff; and componentizes the symbolic verifier to enable parallelism, targeting from new APIs.", "ProgramReferenceCode": ["7433", "8005", "9150"], "OrganizationZip": "841128930"}, {"OrganizationCity": "Princeton", "OrganizationState": "NJ", "Title": "Collaborative Research: SI2-SSI: Data-Intensive Analysis for High Energy Physics (DIANA/HEP)", "ProgramManager": "Daniel Katz", "State": "NJ", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "COMPUTATIONAL PHYSICS", "Software Institutes"], "ProgramElementCode": ["1253", "7244", "8004"], "StartDate": "05/01/2015", "OrganizationStreet": "Off. of Research & Proj. Admin.", "EndDate": "04/30/2019", "PrincipalInvestigator": "G.J. Peter Elmer", "ARRAAmount": null, "OrganizationPhone": "6092583090", "AwardNumber": "1450377", "PIEmailAddress": "gelmer@princeton.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "05/07/2015", "Organization": "Princeton University", "AwardInstrument": "Continuing grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "950564", "Abstract": "Advanced software plays a fundamental role for large scientific projects. The primary goal of DIANA/HEP (Data Intensive ANAlysis for High Energy Physics) is developing state-of-the-art tools for experiments that acquire, reduce, and analyze petabytes of data. Improving performance, interoperability, and collaborative tools through modifications and additions to packages broadly used by the community will allow users to more fully exploit the data being acquired at CERN&apos;s Large Hadron Collider (LHC) and other facilities. These experiments are addressing questions at the heart of physics: What are the underlying constituents of matter? And how do they interact? With the discovery of the Higgs boson in 2012, the Standard Model of particle physics is complete. It provides an excellent description of known particles and forces. However, the most interesting questions remain open: What is the dark matter which pervades the universe? Does space-time have additional symmetries or extend beyond the 3 spatial dimensions we know? What is the mechanism stabilizing the Higgs boson mass from enormous quantum corrections? The next generation of experiments will collect exabyte-scale data samples to provide answers. Analyzing this data will require new and better tools. &lt;br/&gt;&lt;br/&gt;First, the project will provide the CPU and IO performance needed to reduce the iteration time so crucial to explore new ideas. It will develop software to effectively exploit emerging many- and multi-core hardware. It will establish infrastructure for a higher-level of collaborative analysis, building on the successful patterns used for the Higgs boson discovery and enabling a deeper communication between the theoretical community and the experimental community. DIANA?s products will sit in the ROOT framework, already used by the HEP community of more than 10000 particle and nuclear physicists. By improving interoperability with the larger scientific software ecosystem, DIANA will incorporate best practices and algorithms from other disciplines into HEP. Similarly, the project will make its computing insights, tools, and novel ideas related to collaborative analysis, standards for data preservation, and best practices for treating software as a research product available to the larger scientific community. Finally, to improve the quality of the next generation of software engineers in HEP, DIANA will host an annual workshop on analysis tools and establish a fellowship program.", "ProgramReferenceCode": ["7433", "8009", "8084", "8005"], "OrganizationZip": "085442020"}, {"OrganizationCity": "BERKELEY", "OrganizationState": "CA", "Title": "Collaborative Research: SI2-SSI: ELSI-Infrastructure for Scalable Electronic Structure Theory", "ProgramManager": "Daniel Katz", "State": "CA", "Program": ["Software Institutes", "OFFICE OF MULTIDISCIPLINARY AC", "DMR SHORT TERM SUPPORT"], "ProgramElementCode": ["8004", "1253", "1712"], "StartDate": "06/15/2015", "OrganizationStreet": "Sponsored Projects Office", "EndDate": "05/31/2019", "PrincipalInvestigator": "Lin Lin", "ARRAAmount": null, "OrganizationPhone": "5106428109", "AwardNumber": "1450372", "PIEmailAddress": "linlin@math.berkeley.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "06/11/2015", "Organization": "University of California-Berkeley", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "504016", "Abstract": "Predictive, so-called ab initio electronic structure calculations, particularly those based on the Kohn-Sham density functional theory (DFT) are now a widely used scientific workhorse with applications in virtually all sciences, and increasingly in engineering and industry. In materials science, they enable the computational (&quot;in silico&quot;) design of new materials with improved properties. In biological or pharmacological research, they provide molecular-level insights into the function of macromolecules or drugs. In the search for new energy solutions, they give molecular-level insights into new solar cell designs, catalytic processes, and many others. A key bottleneck in many applications and calculations is the &quot;cubic scaling wall&quot; of the so-called Kohn-Sham eigenvalue problem with system size (i.e., the effort increases by a factor of 1,000 if the model size increases by a factor of 10). This project will establish an open source software infrastructure &quot;ELSI&quot; that offers a common, practical interface to initially three complementary solution strategies to alleviate or overcome the difficulty associated with solving the Kohn-Sham eigenvalue problem. ELSI will enable a broad range of end user communities, centered around different codes with, often, unique features that tie a specialized group of scientists to that particular solution, to easily incorporate state-of-the-art solution strategies for a key problem they all share. By providing these effective, accessible solution strategies, we will open up major areas for electronic structure theory where DFT based predictive methodologies are not applicable today. This will in turn open doors for new development in materials science, chemistry, and all related areas. Commitments to support ELSI exist from some of the most important electronic structure developer communities, as well as from industry and government leaders in high-performance computing. Thus, we will create a strong U.S. based infrastructure that leverages the large user and developer base from a globally active community developing DFT methods for materials research.&lt;br/&gt;&lt;br/&gt;ELSI will support and enhance three state-of-the-art approaches, each best suited for a specific problem range: (i) The ELPA (EigensoLvers for Petascale Applications) library, a leading library for efficient, massively parallel solution of eigenvalue problems (for small- and mid-sized problems up to several 1,000s of atoms), (ii) the OMM (Orbital Minimization Method) in a recent re-implementation, which circumvents the eigenvalue problem by focusing on a reduced, auxiliary problem (for systems in the several 1,000s of atoms range), and (iii) the PEXSI (Pole EXpansion and Selective Inversion) library, a proven reduced scaling (at most quadratic scaling) solution for general systems (for problems with 1,000s of atoms and beyond). By establishing standardized interfaces in a style already familiar to many electronic structure developers, ELSI will enable production electronic structure codes that use it to significantly reduce the &quot;scaling wall&quot; of the eigenvalue problem. First, ELSI will help them make efficient use of the most powerful computational platforms available. The target platforms are current massively parallel computers and multicore architectures, GPU based systems and future manycore processors. Second, the project will make targeted methodological improvements to ELPA, OMM, and PEXSI, e.g., a more effective use of matrix sparsity towards very large systems. The focus on similar computational architectures and similar methodological enhancements will lead to significant cross-fertilization and synergy between these approaches.", "ProgramReferenceCode": ["7433", "8009", "8084", "9216"], "OrganizationZip": "947045940"}, {"OrganizationCity": "Houston", "OrganizationState": "TX", "Title": "SI2-SSE: Collaborative Research: ADAPT: Next Generation Message Passing Interface (MPI) Library - Open MPI", "ProgramManager": "Daniel Katz", "State": "TX", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "09/01/2013", "OrganizationStreet": "4800 Calhoun Boulevard", "EndDate": "08/31/2016", "PrincipalInvestigator": "Edgar Gabriel", "ARRAAmount": null, "OrganizationPhone": "7137435773", "AwardNumber": "1339763", "PIEmailAddress": "gabriel@cs.uh.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/29/2013", "Organization": "University of Houston", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "147236", "Abstract": "High-performance computing has reshaped science and industry in many areas. However, the rapid evolution at the hardware level over the last few years have been unmatched by corresponding changes at the programming paradigm level. According to the consensus of several major studies, the degree of parallelism on large systems is expected to increase by several orders of magnitude. As a result, the Message Passing Interface (MPI), which has been the de-facto standard message passing paradigm, lacks an efficient and portable way of handling today&apos;s architectures. To efficiently handle such systems, MPI implementations must adopt more asynchronous and thread-friendly behaviors to perform better than they do on today?s systems. Maintaining and further enhancing MPI, one of the most widely-used communication libraries in high-performance computing, will have a far-reaching impact beyond the scientific community, and represents a critical building block for continued advances in all areas of science and engineering.&lt;br/&gt;The ADAPT project enhances, hardens and modernizes the Open MPI library in the context of this ongoing revolution in processor architecture and system design. It creates a viable foundation for a new generation of Open MPI components, enabling the rapid exploration of new physical capabilities, providing greatly improved performance portability, and working toward full interoperability between classes of components. More specifically, ADAPT implements fundamental software techniques that can be used in many-core systems to efficiently execute MPI-based applications and to tolerate fail-stop process failures, at scales ranging from current large systems to the extreme scale systems that are coming soon. To improve the efficiency of Open MPI, ADAPT integrates, as a core component, knowledge about the hardware architecture, and allows all layers of the software stack full access to this information. Process placement, distributed topologies, file accesses, point-to-point and collective communications can then adapt to such topological information, providing more portability. The ADAPT team is also updating the current collective communication layer to allow for a task-based collective description contained at a group-level, which in turn adjusts to the intra and inter-node topology. Planned expansion of the current code with resilient capabilities allows Open MPI to efficiently survive hard and soft error types of failures. These capabilities can be used as building blocks for all currently active fault tolerance proposals in the MPI standard body.&lt;br/&gt;MPI is already one of the most relevant parallel programming models, the most important brick of most parallel applications, and one of the most critical communication pieces of most other programing models. Thus, the experience of the research team and emerging capabilities can benefit all future users of these programming standards, tools, and libraries--regardless of discipline. Any improvement in the performance and capabilities of a major MPI library such as Open MPI, has tremendous potential for an immediate and dramatic impact on the application communities. In addition to improving the time to solution for their applications, it has the potential to decrease the energy usage and maximize the performance delivered by the existing execution platforms. The scale at which the Open MPI library is used in government research institutions (including universities and national laboratories), as well as in the private sector, is a major vector for a quick impact on all scientific and engineering communities.", "ProgramReferenceCode": ["7433", "8005"], "OrganizationZip": "772042015"}, {"OrganizationCity": "BLACKSBURG", "OrganizationState": "VA", "Title": "Collaborative Research: SI2-SSI:Task-based Environment for Scientific Simulation at Extreme Scale (TESSE)", "ProgramManager": "Daniel Katz", "State": "VA", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "DMR SHORT TERM SUPPORT", "Software Institutes"], "ProgramElementCode": ["1253", "1712", "8004"], "StartDate": "05/15/2015", "OrganizationStreet": "Sponsored Programs 0170", "EndDate": "04/30/2018", "PrincipalInvestigator": "Edward Valeev", "ARRAAmount": null, "OrganizationPhone": "5402315281", "AwardNumber": "1450262", "PIEmailAddress": "evaleev@vt.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "05/20/2015", "Organization": "Virginia Polytechnic Institute and State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "600000", "Abstract": "This project, TESSE, is developing a new-generation programming environment that uniquely address the needs of emerging computational models in chemistry and other fields, and allowing these models to reap the benefits of the computer hardware of tomorrow. The project thus is bringing closer the ability to predict properties of matter and design matter transformations of importance to the technological leadership and energy security of the US. TESSE&apos;s impact has scientific, cyberinfrastructure, and interdisciplinary educational aspects. TESSE helps carry widely used scientific applications (NWChem, MADNESS, MPQC, etc.) into a whole new generation of many-core and accelerated architectures, while at the same time dramatically improving programmer productivity. In terms of cyberinfrastructure, TESSE&apos;s PaRSEC runtime system delivers capabilities that many libraries and advanced applications will require for high performance on large scale and hybridized systems.&lt;br/&gt;&lt;br/&gt;The goals of this project, Task-based Environment for Scientific Simulation at Extreme Scale (TESSE), are to design and demonstrate via substantial scientific simulations within chemistry and other disciplines a prototype software framework that provides a groundbreaking response to the twin problems of portable performance and programmer productivity for advanced scientific applications on emerging massively-parallel, hybrid, many-core systems. TESSE will create a viable foundation for a new generation of science codes, one which enables even more rapid exploration of new physical models, provides greatly enhanced performance portability through directed acyclic graph (DAG) scheduling and auto-tuned kernels, and works towards full interoperability between major chemistry packages through compatible runtimes and data structures. TESSE will mature to become a standard, widely available, community-based and broadly-applicable parallel programming environment complementing and rivaling MPI/OpenMP. This is needed due to the widely appreciated shortfalls of existing mainstream programming models and the already huge successes of the existing DAG-based runtimes that are the foundation of the next generation of NSF- and DOE-supported (Sca)LAPACK high-performance linear algebra libraries.", "ProgramReferenceCode": ["7433", "8009", "8084", "9216"], "OrganizationZip": "240610001"}, {"OrganizationCity": "Chicago", "OrganizationState": "IL", "Title": "SI2-SSI: Sustaining Globus Toolkit for the NSF Community (Sustain-GT)", "ProgramManager": "Rajiv Ramnath", "State": "IL", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "10/01/2013", "OrganizationStreet": "5801 South Ellis Avenue", "EndDate": "09/30/2018", "PrincipalInvestigator": "Steven Tuecke", "ARRAAmount": null, "OrganizationPhone": "7737028669", "AwardNumber": "1339873", "PIEmailAddress": "tuecke@uchicago.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/27/2013", "Organization": "University of Chicago", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1200000", "Abstract": "Science and engineering depend increasingly on the ability to collaborate and federate resources across distances. This observation holds whether a single investigator is accessing a remote computer, a small team is analyzing data from an engineering experiment, or an international collaboration is involved in a multi-decade project such as the Large Hadron Collider (LHC). Any distributed collaboration and resource federation system requires methods for authentication and authorization, data movement, and remote computation. Of the many solutions that have been proposed to these problems, the Globus Toolkit (GT) has proven the most persistently applicable across multiple fields, geographies, and project scales. GT resource gateway services and client libraries are used by tens of thousands of people every day to perform literally tens of millions of tasks at thousands of sites, enabling discovery across essentially every science and engineering discipline supported by the NSF. As new, innovative techniques and technologies for collaboration and scientific workflows are developed, and as new computing and instrument resources are added to the national cyberinfrastructure, these technologies and other improvements must be added and integrated into GT so that it can continue to provide an advanced and robust technology for solving scientific research problems.&lt;br/&gt;&lt;br/&gt;The Sustain-GT project builds on past success to ensure that GT resource gateway services will continue to meet the challenges faced by NSF science and engineering communities. These challenges include: multiple-orders-of-magnitude increases in the volume of data generated, stored, and transmitted; much bigger computer systems and correspondingly larger and more complex computations; much faster networks; many more researchers, educators, and students engaged in data-intensive and computational research; and rapidly evolving commodity Web and Cloud computing environments. With the help of a new User Requirements Board, Sustain-GT will respond to community demands to evolve the GT resource gateway services with superior functionality, scalability, availability, reliability, and manageability. Sustain-GT will also provide the NSF community with high quality support and rapid-response bug fix services, as is required to sustain a heavily used, production system like GT.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "606375418"}, {"OrganizationCity": "BERKELEY", "OrganizationState": "CA", "Title": "Collaborative Research: SI2-SSI: A Sustainable Open Source Software Pipeline for Patient Specific Blood Flow Simulation and Analysis", "ProgramManager": "Daniel Katz", "State": "CA", "Program": ["CDS&E", "Software Institutes"], "ProgramElementCode": ["8084", "8004"], "StartDate": "09/20/2013", "OrganizationStreet": "Sponsored Projects Office", "EndDate": "09/30/2017", "PrincipalInvestigator": "Shawn Shadden", "ARRAAmount": null, "OrganizationPhone": "5106428109", "AwardNumber": "1407834", "PIEmailAddress": "shadden@berkeley.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "11/12/2013", "Organization": "University of California-Berkeley", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "306276", "Abstract": "The SimVascular package is a crucial research tool for cardiovascular modeling and simulation, and has contributed to numerous advances in personalized medicine, surgical planning and medical device design. SimVascular is currently the only comprehensive software package that provides a complete pipeline from medical image data segmentation to patient specific blood flow simulation and analysis. This software now forms the backbone in cardiovascular simulation research in a small but active group of domestic and international academic labs. However, since its original release there have been several critical barriers preventing wider adoption by new users, application to large-scale research studies, and educational access. These include 1) the cost and complications associated with embedded commercial components, 2) the need for more efficient geometric model construction tools, 3) lack of sustainable architecture and infrastructure, and 4) a lack of organized maintenance. &lt;br/&gt;&lt;br/&gt;This project is addressing the above roadblocks through the following aims: 1) create a sustainable and modular open source SimVascular 2.0 project housed at Stanford Simbios? simtk.org, with documentation, benchmarking and test suites, 2) provide alternatives to all commercial components in the first truly open source release of SimVascular, 3) improve the image segmentation methods and efficiency of model construction to enable high-throughput studies, and 4) enhance functionality by merging state of the art research in optimization, flow analysis, and multiscale modeling. The project leverages existing resources and infrastructure at simtk.org, and builds upon the significant previous investment that enabled the initial open source release of SimVascular. Access is further enhanced by cross-linking with the NIH funded Vascular Model Repository. This project will increase the user base and build a sustainable software platform supported by an active open source community. Releasing the first fully open source version of SimVascular will enable greater advances in cardiovascular medicine, provide open access to state of the art simulation tools for educational purposes, and facilitate training of young investigators. These efforts will also further promote diversity and attract students to science and engineering by leveraging this software to enable high school field trips to the UCSD StarCAVE to view simulation data using virtual reality.", "ProgramReferenceCode": ["7433", "8009", "8084"], "OrganizationZip": "947045940"}, {"OrganizationCity": "South Hadley", "OrganizationState": "MA", "Title": "SI2-SSI: Collaborative Research: Bringing End-to-End Provenance to Scientists", "ProgramManager": "Daniel Katz", "State": "MA", "Program": ["SPECIAL PROJECTS - CCF", "Software Institutes"], "ProgramElementCode": ["2878", "8004"], "StartDate": "06/01/2015", "OrganizationStreet": "50 College Street", "EndDate": "05/31/2018", "PrincipalInvestigator": "Barbara Lerner", "ARRAAmount": null, "OrganizationPhone": "4135382000", "AwardNumber": "1450356", "PIEmailAddress": "blerner@mtholyoke.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "06/01/2015", "Organization": "Mount Holyoke College", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "422997", "Abstract": "Reproducability is the cornerstone of scientific progress. Historically, scientists make their work reproducible by including a formulaic description of the experimental methodology used in an experiment. In an age of computational science, such descriptions no longer adequately describe scientific methodology. Instead, scientific reproducibility relies on a precise and actionable description of the data and programs used to conduct the research. Provenance is the name given to the description of how a digital artifact came to be in its present state. Provenance includes a precise specification of an experiment&apos;s input data and the programs or procedures applied to that data. Most computational platforms do not record such data provenance, making it difficult to ensure reproducability. This project addresses this problem through the development of tools that transparently and automatically capture data provenance as part of a scientist&apos;s normal computational workflow.&lt;br/&gt;&lt;br/&gt;An interdisciplinary team of computer scientists and ecologists have come together to develop tools to facilitate the capture, management, and query of data provenance -- the history of how a digital artifact came to be in its present state. Such data provenance improves the transparency, reliability, and reproducibility of scientific results. Most existing provenance systems require users to learn specialized tools and jargon and are unable to integrate provenance from different sources; these are serious obstacles to adoption by domain scientists. This project includes the design, development, deployment, and evaluation of an end-to-end system (eeProv) that encompasses the range of activity from original data analysis by domain scientists to management and analysis of the resulting provenance in a common framework with common tools. This project leverages and integrates development efforts on (1) an emerging system for generating provenance from a computing environment that scientists actually use (the R statistical language) with (2) an emerging system that utilizes a library of language and database adapters to store and manage provenance from virtually any source. Accomplishing the goals of this proposal requires fundamental research in resolving the semantic gap between provenance collected in different environments, capturing detailed provenance at the level of a programming language, defining precisely aspects of provenance required for different use cases, and making provenance accessible to scientists.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "010756456"}, {"OrganizationCity": "Chicago", "OrganizationState": "IL", "Title": "Collaborative Research: SI2-SSI: SciDaaS - Data Management as a Service", "ProgramManager": "Rajiv Ramnath", "State": "IL", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "04/01/2012", "OrganizationStreet": "5801 South Ellis Avenue", "EndDate": "03/31/2016", "PrincipalInvestigator": "Ian Foster", "ARRAAmount": null, "OrganizationPhone": "7737028669", "AwardNumber": "1148484", "PIEmailAddress": "foster@uchicago.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/25/2012", "Organization": "University of Chicago", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "2398999", "Abstract": "The SciDaaS project will develop and operate a suite of innovative research data management services for the NSF community. These services, to be accessible at www.globusonline.org, will allow research laboratories to outsource a range of time-consuming research data management functions, including storage and movement, publication, and metadata management. SciDaaS research will investigate what services are most needed by NSF researchers; how best to present these services to integrate with diverse research laboratory environments; and how these services are used in practice across different research communities.&lt;br/&gt;&lt;br/&gt;SciDaaS will greatly reduce the cost to the individual researcher of acquiring and operating sophisticated scientific data management capabilities. In so doing, it has the potential to dramatically expand use of advanced information technology in NSF research and thus accelerate discovery across many fields of science and engineering. By providing a platform for researchers to publicly share data at an incremental cost, SciDaaS will also reduce barriers to free exchange among researchers and contribute to the democratization of science.", "ProgramReferenceCode": ["8004", "8009"], "OrganizationZip": "606375418"}, {"OrganizationCity": "RALEIGH", "OrganizationState": "NC", "Title": "SI2-SSE: Software Tools for Biomolecular Free Energy Calculations", "ProgramManager": "Daniel Katz", "State": "NC", "Program": ["Software Institutes", "DMR SHORT TERM SUPPORT", "MATERIALS AND SURFACE ENG", "CHEMISTRY PROJECTS", "OFFICE OF MULTIDISCIPLINARY AC"], "ProgramElementCode": ["8004", "1712", "1633", "1991", "1253"], "StartDate": "07/01/2012", "OrganizationStreet": "CAMPUS BOX 7514", "EndDate": "06/30/2016", "PrincipalInvestigator": "Celeste Sagui", "ARRAAmount": null, "OrganizationPhone": "9195152444", "AwardNumber": "1148144", "PIEmailAddress": "sagui@ncsu.edu", "NSFDirectorate": "CSE", "Co-PIName": "Christopher Roland", "LastAmendmentDate": "07/06/2012", "Organization": "North Carolina State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "497797", "Abstract": "TECHNICAL SUMMARY&lt;br/&gt;&lt;br/&gt;The Office of Cyberinfrastructure, Division of Materials Research, Chemistry Division, and Division of Civil, Mechanical, and Manufacturing Innovation contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation. This award supports the development of a new set of software tools for calculating biomolecular free energies, transition paths, and reaction rates that will be based on and will augment the PIs&apos; ABMD/REMD/SMD suite of codes. New capabilities are to be added to the existing codes, which should greatly enhance the applicability of the software to challenging biomolecular problems. In particular, the PIs aim to implement and develop: (i) the capability for dealing with dynamically coupled multiple walkers or replicas for enhanced sampling; (ii) the introduction of Transition Path Theory and Sampling methodology for calculating reaction rates and related physical quantities; and (iii) the introduction of the &quot;string method&quot; for multi-dimensional free energy calculations and the calculation of the important minimum free energy path. These software tools will be released under the open source GPL, and released as part of the AMBER software package. The codes will also be released as stand-alone modules via the web for other users to integrate into their own programs.&lt;br/&gt;&lt;br/&gt;To test and showcase the workings of the software package, the PIs will apply these methods to biomolecular systems where changes in handedness play an important role, including: proline-rich systems, mainly collagen and a new class of cell-penetrating peptides, and DNA and RNA in B and Z double helices. &lt;br/&gt;&lt;br/&gt;This award will also enhance education through: the mentoring and recruitment of minority students, the development of a Biophysics option, the integration of research topics into current educational forums, working with local schools, and the development of a new Institute for Computational Science and Engineering.&lt;br/&gt;&lt;br/&gt;NON-TECHNICAL SUMMARY&lt;br/&gt;The Office of Cyberinfrastructure, Division of Materials Research, Chemistry Division, and Division of Civil, Mechanical, and Manufacturing Innovation contribute funds to this award made on a proposal to the Software Infrastructure for Sustained Innovation solicitation. This award supports the development of a new set of software tools that will enable the calculation of rates of chemical reactions involving biomolecules. The codes will also be able to calculate the energy that can be converted into useful work in a biochemical system. The codes will be distributed through a widely used software package called AMBER which contributes to the software cyberinfrastructure of computational chemistry, biochemistry, chemical engineering, materials, and biological physics communities. It will enable a wide range of computational research involving biological molecules. Among the possible applications are the study of the structure and transformations of biomolecules which provides information on their possible functions, the simulation of antibiotics, and simulating nanoscale technology applications. &lt;br/&gt;&lt;br/&gt;This award will also enhance education through: the mentoring and recruitment of minority students, the development of a Biophysics option, the integration of research topics into current educational forums, working with local schools, and the development of a new Institute for Computational Science and Engineering.", "ProgramReferenceCode": ["8005", "7237", "7433", "7569", "7573", "9215", "9216", "9263", "1982", "HPCC", "1253", "1633", "1712", "1991", "8004"], "OrganizationZip": "276957514"}, {"OrganizationCity": "MOSS LANDING", "OrganizationState": "CA", "Title": "Collaborative Research: SI2-SSI: Empowering the Scientific Community with Streaming Data Middleware: Software Integration into Complex Science Environments", "ProgramManager": "Daniel Katz", "State": "CA", "Program": ["ADVANCES IN BIO INFORMATICS", "ECOSYSTEM STUDIES", "Software Institutes", "Cyber Secur - Cyberinfrastruc"], "ProgramElementCode": ["1165", "1181", "8004", "8027"], "StartDate": "08/01/2012", "OrganizationStreet": "7700 SANDHOLDT RD", "EndDate": "07/31/2016", "PrincipalInvestigator": "Duane Edgington", "ARRAAmount": null, "OrganizationPhone": "8317751803", "AwardNumber": "1148213", "PIEmailAddress": "duane@mbari.org", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/07/2012", "Organization": "Monterey Bay Aquarium Research Institute", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "101710", "Abstract": "This project will catalyze a new generation of sensor-based streaming data applications by building on the foundation of the Open Source DataTurbine (OSDT) middleware (www.dataturbine.org). This will involve strategic software development, systems integration and testing, science experiments, and education and outreach. The software developments will include (A) tools for configuring, testing, and controlling distributed sensor systems, (B) software interfaces that are compliant with the Open Geospatial Consortium Sensor Web Enablement standards, (C) a new OSDT-Matlab interface, and (D) OSDT cybersecurity enhancements. These software products will be integrated into the production research infrastructures at Purdue, the University of Connecticut, the North Temperate Lake LTER Network site, and the Monterey Bay Aquarium Research Institute. The software will be professionally developed and managed as community resources. The intellectual merit of this project will be advances in software engineering, interoperability standards, and systems management for complex real-time sensor systems as well as advances on important open questions in civil engineering, marine science, and ecology. This project will yield broader impacts by enabling new science applications, enhancing productivity, and accelerating innovation across NSF directorates. To ensure that these broader impacts are realized, the team will provide student research opportunities, develop curriculum materials and training courses, and cultivate a community of software users.", "ProgramReferenceCode": ["8009", "7434", "1165", "1181", "8004", "8027"], "OrganizationZip": "950399644"}, {"OrganizationCity": "NEW YORK", "OrganizationState": "NY", "Title": "Collaborative Research: SI2-SSI: Data-Intensive Analysis for High Energy Physics (DIANA/HEP)", "ProgramManager": "Daniel Katz", "State": "NY", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes", "COMPUTATIONAL PHYSICS"], "ProgramElementCode": ["1253", "8004", "7244"], "StartDate": "05/01/2015", "OrganizationStreet": "70 WASHINGTON SQUARE S", "EndDate": "04/30/2019", "PrincipalInvestigator": "Kyle Cranmer", "ARRAAmount": null, "OrganizationPhone": "2129982121", "AwardNumber": "1450310", "PIEmailAddress": "kyle.cranmer@nyu.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "05/07/2015", "Organization": "New York University", "AwardInstrument": "Continuing grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "744189", "Abstract": "Advanced software plays a fundamental role for large scientific projects. The primary goal of DIANA/HEP (Data Intensive ANAlysis for High Energy Physics) is developing state-of-the-art tools for experiments that acquire, reduce, and analyze petabytes of data. Improving performance, interoperability, and collaborative tools through modifications and additions to packages broadly used by the community will allow users to more fully exploit the data being acquired at CERN&apos;s Large Hadron Collider (LHC) and other facilities. These experiments are addressing questions at the heart of physics: What are the underlying constituents of matter? And how do they interact? With the discovery of the Higgs boson in 2012, the Standard Model of particle physics is complete. It provides an excellent description of known particles and forces. However, the most interesting questions remain open: What is the dark matter which pervades the universe? Does space-time have additional symmetries or extend beyond the 3 spatial dimensions we know? What is the mechanism stabilizing the Higgs boson mass from enormous quantum corrections? The next generation of experiments will collect exabyte-scale data samples to provide answers. Analyzing this data will require new and better tools. &lt;br/&gt;&lt;br/&gt;First, the project will provide the CPU and IO performance needed to reduce the iteration time so crucial to explore new ideas. It will develop software to effectively exploit emerging many- and multi-core hardware. It will establish infrastructure for a higher-level of collaborative analysis, building on the successful patterns used for the Higgs boson discovery and enabling a deeper communication between the theoretical community and the experimental community. DIANA?s products will sit in the ROOT framework, already used by the HEP community of more than 10000 particle and nuclear physicists. By improving interoperability with the larger scientific software ecosystem, DIANA will incorporate best practices and algorithms from other disciplines into HEP. Similarly, the project will make its computing insights, tools, and novel ideas related to collaborative analysis, standards for data preservation, and best practices for treating software as a research product available to the larger scientific community. Finally, to improve the quality of the next generation of software engineers in HEP, DIANA will host an annual workshop on analysis tools and establish a fellowship program.", "ProgramReferenceCode": ["7433", "8009", "8084"], "OrganizationZip": "100121019"}, {"OrganizationCity": "SALT LAKE CITY", "OrganizationState": "UT", "Title": "SI2-SSE: Correctness Verification Tools for Extreme Scale Hybrid Concurrency", "ProgramManager": "Rajiv Ramnath", "State": "UT", "Program": ["Software Institutes", "INFORMATION TECHNOLOGY RESEARC"], "ProgramElementCode": ["8004", "1640"], "StartDate": "06/01/2012", "OrganizationStreet": "75 S 2000 E", "EndDate": "05/31/2016", "PrincipalInvestigator": "Ganesh Gopalakrishnan", "ARRAAmount": null, "OrganizationPhone": "8015816903", "AwardNumber": "1148127", "PIEmailAddress": "ganesh@cs.utah.edu", "NSFDirectorate": "CSE", "Co-PIName": "Mary Hall", "LastAmendmentDate": "04/24/2015", "Organization": "University of Utah", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "490279", "Abstract": "High Performance Computing is strategically important to national competitiveness. Advances in computational capabilities involve the use of unprecedented levels of parallelism: programming methods that involve billions of concurrent activities. Multiple styles of concurrency involving shared and distributed memory programming (&quot;hybrid&quot;) are necessary. Unfortunately, such programs are very difficult to debug using existing methods. This project develops formal (mathematically based) verification tools that can debug hybrid concurrent programs with very high certainty of bug elimination, while consuming only modest computational resources for verification. &lt;br/&gt;&lt;br/&gt;The project develops execution-based tools that eliminate search over semantically equivalent alternative schedules as well as solver-based techniques that eliminate classes of bugs over single runs. Scalable methods based on non-determinism classification and heuristic execution-space reduction are also being developed. &lt;br/&gt;&lt;br/&gt;Expected results include: (1) development of tools based on formal algorithmic techniques that verify large-scale hybrid programs; (2) amalgamation of incisive bug-hunting methods developed at other research organizations within formally based tools developed in our group; (3) incorporation of our verification tools and techniques within popular tool-integration frameworks; (4) large-scale case studies handled using our tools; and (5) training of undergraduate and graduate students on these advanced verification methods, building the talent pool vital to continued progress in high performance computing with applications to science and engineering, energy/sustainability, and homeland security.", "ProgramReferenceCode": ["8004", "8005", "9150", "7942", "9251", "1640", "7433"], "OrganizationZip": "841128930"}, {"OrganizationCity": "Baton Rouge", "OrganizationState": "LA", "Title": "SI2-SSI: Collaborative Research: STORM: A Scalable Toolkit for an Open Community Supporting Near Realtime High Resolution Coastal Modeling", "ProgramManager": "Rajiv Ramnath", "State": "LA", "Program": ["SPECIAL PROJECTS - CCF", "SPECIAL PROJECTS - CISE", "Software Institutes", "EarthCube", "PHYSICAL OCEANOGRAPHY"], "ProgramElementCode": ["2878", "1714", "8004", "8074", "1610"], "StartDate": "10/01/2014", "OrganizationStreet": "202 Himes Hall", "EndDate": "09/30/2018", "PrincipalInvestigator": "Hartmut Kaiser", "ARRAAmount": null, "OrganizationPhone": "2255782760", "AwardNumber": "1339782", "PIEmailAddress": "hkaiser@cct.lsu.edu", "NSFDirectorate": "CSE", "Co-PIName": "Robert Twilley", "LastAmendmentDate": "08/26/2014", "Organization": "Louisiana State University & Agricultural and Mechanical College", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "970835", "Abstract": "The ADCIRC coastal circulation and storm surge model has a long standing track record of extremely high societal impact. It has been used to define risk (e.g., 100-yr, 500-yr coastal flood levels) for the FEMA National Flood Insurance Program in coastal states from New York to Texas, it has been used to design the multi-billion dollar Hurricane and Storm Damage Risk Reduction System around greater New Orleans and southern Louisiana by the Army Corps of Engineers, it is currently run operationally by NOAA/NWS National Center for Environmental Prediction to forecast storm surge, to name just a few of its current and recent applications. Thus there is a well-established user network in place to convert improvements in ADCIRC into significant broader impacts. The proposed research provides transformative intellectual contributions that focus on applying new parallelization schemes to enable major advances in the algorithms, implementation and utilization of the ADCIRC model. The broadening of ADCIRC to a multi-algorithmic framework and the resulting performance gains that are anticipated will help ensure ADCIRC&apos;s sustainability as a core community model for at least the next 20 years. In addition, the proposed collaboration will impact computer science by serving as a high impact use case to inform the design of new approaches to efficient scalable computing. Together, the advancements in coastal modeling and parallelization technology will make a significant contribution to the science of modeling and HPC. The results of the proposed research will be disseminated to a wider community through ongoing educational outreach activities at the participating organizations as well as through refereed conference and journal papers, and invited presentations. The involvement of graduate students and post-doctoral fellows will be crucial towards the success of this project. The PIs have a long history of training and mentoring students and post-docs in computational science and engineering, coastal engineering and marine science. The recruitment and involvement of underrepresented groups in these efforts has always been a high priority. In addition, aspects of the proposed research will be incorporated into the curricula of several courses taught by the PIs in the areas of finite element methods, scientific computation, hydrology and oceanography.&lt;br/&gt;The aim of this project is to broaden the ADCIRC coastal circulation and storm surge model from a successful, but somewhat static coastal modeling tool that is tied to a single solution algorithm and the MPI parallelization paradigm, to a dynamic computational platform that is comprised of multiple solution algorithms, that readily admits new solution algorithms and that is built on a transformational new parallelization scheme that will allow us to scale to at least 256k compute cores on modern high performance computing (HPC) systems. We will do this by creating a living, evolving coastal modeling framework that will continue to lead the community in merging physical science / engineering and high performance computing and we will make the framework available to the broader community as a sustainable long term solution for its coastal modeling needs. In addition we will utilize these advancements in the highly demanding coastal storm surge forecasting system that we presently operate to demonstrate both improved robustness and speed of the model solution. We expect this effort will shorten the time required to provide reliable forecasting results and improve our ability to provide highly resolved, accurate, and physically complete predictions on an unprecedented scale. Concurrently, it should enable the use of smaller resources for simulations of increased scale which improves the usability and widens the applicability of ADCIRC in a broader community. The development of tightly integrated web-oriented products like CERA (www.coastalemergency.org) will enable the wide and timely dissemination of forecast modeling results to reach a broad audience.", "ProgramReferenceCode": ["7433", "8009", "9150"], "OrganizationZip": "708032701"}, {"OrganizationCity": "Minneapolis", "OrganizationState": "MN", "Title": "Collaborative Research: SI2-SSE: Sage-Combinat: Developing and Sharing Open Source Software for Algebraic Combinatorics", "ProgramManager": "Daniel Katz", "State": "MN", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "DYNAMICAL SYSTEMS", "Software Institutes", "CDS&E-MSS"], "ProgramElementCode": ["1253", "7478", "8004", "8069"], "StartDate": "06/01/2012", "OrganizationStreet": "200 OAK ST SE", "EndDate": "05/31/2016", "PrincipalInvestigator": "Gregg Musiker", "ARRAAmount": null, "OrganizationPhone": "6126245599", "AwardNumber": "1147161", "PIEmailAddress": "musiker@math.umn.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "06/19/2012", "Organization": "University of Minnesota-Twin Cities", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "195688", "Abstract": "Sage is an open source general purpose mathematical software system that has developed explosively within the last six years. Sage-Combinat is a subproject whose mission is &quot;to improve Sage as an extensible toolbox for computer exploration in (algebraic) combinatorics, and foster code sharing between researchers in this area&quot;. There is a long tradition of software packages for algebraic combinatorics. These have been crucial in the development of combinatorics since the 1960s. The originality of the Sage-Combinat project lies in successfully addressing the following simultaneous objectives. It offers a wide variety of interoperable and extensible tools, integrated in a general purpose mathematical software package, as needed for daily computer exploration in algebraic combinatorics; it is developed by a community of researchers spread around the world and across institutions; and it is open source and depends only on open source software. Among the proposers, Stein is founder and lead developer of Sage while Bump, Musiker, and Schilling are strong contributors to Sage-Combinat. Hivert and Thi&#180;ery (Paris-Sud, Orsay), founders and lead developers of Sage-Combinat, are both strongly affiliated with this project. Some of the research areas addressed in this project include symmetric functions, in particular Macdonald polynomials for arbitrary Cartan types and their nonsymmetric analogues, crystals, rigged configurations and combinatorial R-matrices, affine Weyl groups and Hecke algebras, cluster algebras, and posets.&lt;br/&gt;&lt;br/&gt;The project will develop Sage-Combinat in areas relevant to the ongoing research of the participants, together with relevant underlying infrastructure. The project will include three Sage Days workshops, and will be affiliated with a third scheduled workshop at ICERM. These workshops include a strong outreach component and have been a potent tool for connecting researchers and recruiting Sage users and developers. The grant will also fund a dedicated software development and computation server for Sage-Combinat, to be hosted in the Sage computation farm in Seattle. Emphasis will be placed on the development of thematic tutorials that will make the code accessible to new users. The proposal will also fund graduate student RA support, curriculum development, and other mentoring.", "ProgramReferenceCode": ["7433", "7683", "8005", "1253", "5514", "7478", "8004"], "OrganizationZip": "554552070"}, {"OrganizationCity": "NEW ORLEANS", "OrganizationState": "LA", "Title": "Collaborative Research: SI2-SSE: Component-Based Software Architecture for Computational Landscape Modeling", "ProgramManager": "Daniel Katz", "State": "LA", "Program": ["Software Institutes", "EXP PROG TO STIM COMP RES", "GEOMORPHOLOGY & LAND USE DYNAM"], "ProgramElementCode": ["8004", "9150", "7458"], "StartDate": "06/01/2012", "OrganizationStreet": "6823 ST CHARLES AVENUE", "EndDate": "05/31/2016", "PrincipalInvestigator": "Nicole Gasparini", "ARRAAmount": null, "OrganizationPhone": "5048654000", "AwardNumber": "1147519", "PIEmailAddress": "ngaspari@tulane.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "07/06/2012", "Organization": "Tulane University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "245572", "Abstract": "Presently there are no widely adopted software conventions for holistic computational landscape models. To fill this important gap this project adapts and enhances existing landscape modeling codes by introducing a component-based approach to software development. This project adapts and enhances an existing model -- CHILD (Channel-Hill slope Integrated Landscape Development) -- to provide a set of interoperable, independent modeling components that provide flexible and modular approaches to landscape modeling which are fully compatible with the NSF-funded Community Surface Dynamics Modeling System CSDMS) infrastructure. In accord with the CSDMS architecture, the software to be developed will also employ the standards and tools of the Common Component Architecture (CCA) software architecture. Included is the design of an interface for communication with and between the developed components. The end result will be a set of independent, interoperable C++ software modeling modules that are compatible with the CSDMS modeling toolkit as well as the standards and tools of the CCA structure. The software will be tested against data on post-wildfire erosion.&lt;br/&gt;&lt;br/&gt;This approach was selected to provide maximum flexibility to users by allowing them to plug-and-play, seamlessly linking together selected computing modules to enable custom combinations of components to support modeling for a wide variety of research topics. Work will include the development of a gridding engine to handle both regular and unstructured meshes and an interface for space-time rainfall input, as well as a surface hydrology component, a sediment erosion-deposition component, a vegetation component, and a simulation driver. &lt;br/&gt;&lt;br/&gt;If successful this project will impact many branches of the Earth and environmental sciences by building a new modeling platform and creating essential software infrastructure for science with applications that span hydrology, soil erosion, tectonics, geomorphology, vegetation ecology, stratigraphy, and planetary science. Broader impacts of the work include creation of classroom modeling exercises for both undergraduate and high schools students, and support of a PI whose gender is under-represented in the STEM fields and who is employed at an institution in an EPSCoR state. It also includes workforce-training in computational geoscience for graduate students and a postdoc as well as minority undergraduates from Xavier University, a Historically Black University in the New Orleans area.", "ProgramReferenceCode": ["8005", "7433", "7458", "8004"], "OrganizationZip": "701185698"}, {"OrganizationCity": "Bloomington", "OrganizationState": "IN", "Title": "SI2-SSE: Dynamic Adaptive Runtime Systems for Advanced Multipole Method-based Science Achievement", "ProgramManager": "Almadena Y. Chtchelkanova", "State": "IN", "Program": ["Software Institutes", "SPECIAL PROJECTS - CCF"], "ProgramElementCode": ["8004", "2878"], "StartDate": "10/01/2014", "OrganizationStreet": "509 E 3RD ST", "EndDate": "09/30/2017", "PrincipalInvestigator": "Matthew Anderson", "ARRAAmount": null, "OrganizationPhone": "8128550516", "AwardNumber": "1440396", "PIEmailAddress": "andersmw@indiana.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Bo Zhang", "Jackson DeBuhr", "Thomas Sterling"], "LastAmendmentDate": "04/13/2015", "Organization": "Indiana University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "508366", "Abstract": "Multipole methods, including the fast multipole method and the Barnes-Hut algorithm, contribute to a broad range of end-user science applications extending from molecular dynamics to galaxy formation. Multipole methods are widely applied to N-body like problems where the individual interactions of a large number of distant objects can be treated as a single interaction under the appropriate conditions. This simplification eliminates the need for computing individual pairwise interactions and results in a drastic speed-up of computation. However, conventional parallel multipole methods are facing serious challenges to remain competitive as computational resources approach Exascale. Many applications employing multipole methods describe very dynamic physical processes, both in their time dependence and in their range of relevant spatial scales, while conventional implementations of multipole methods are essentially static in nature leading to computational inefficiencies. &lt;br/&gt;This project provides a fine-grained data-driven approach for multipole methods in order to address the limitations of conventional practices and improve scalability and efficiency. The software library employs dynamic adaptive execution methods with multipole-specific strategies for fault tolerance and exception handling while simplifying the implementation of the fast multipole method and the Barnes-Hut algorithm for end-users. The project software library immediately impacts science applications based on multipole methods by improving application scalability and efficiency and providing fault tolerance, a global address space, and an Exascale-ready execution model which integrates the entire system stack. The software library provides a portable and easy-to-use interface that allows scientists to work more efficiently and take advantage of high performance computing resources more effectively. The software library also serves to inform the evolution and development of other languages and programming models aiming to improve performance by shifting to message-driven techniques.", "ProgramReferenceCode": ["7433", "8004", "8005", "9251"], "OrganizationZip": "474013654"}, {"OrganizationCity": "Atlanta", "OrganizationState": "GA", "Title": "SI2-SSE: A Sustainable Wireless Sensor Software Development Framework for Science and Engineering Researchers", "ProgramManager": "Daniel Katz", "State": "GA", "Program": ["SPECIAL PROJECTS - CCF", "Software Institutes"], "ProgramElementCode": ["2878", "8004"], "StartDate": "09/01/2013", "OrganizationStreet": "Office of Sponsored Programs", "EndDate": "08/31/2016", "PrincipalInvestigator": "Raheem Beyah", "ARRAAmount": null, "OrganizationPhone": "4048944819", "AwardNumber": "1339781", "PIEmailAddress": "rbeyah@ece.gatech.edu", "NSFDirectorate": "CSE", "Co-PIName": "A. Selcuk Uluagac", "LastAmendmentDate": "04/10/2015", "Organization": "Georgia Tech Research Corporation", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "506000", "Abstract": "In the last decade, wireless sensor networking has been one of the most popular research areas for computer engineers and scientists. The use of wireless sensors has also started to gain popularity among researchers other than computer engineers and scientists. Today, sensors are used to enable civil engineers to monitor the structural health of deteriorating infrastructure such as highways and bridges, farmers to develop precision-agriculture techniques, ecologists to observe wildlife in their natural habitat, geophysicists to capture seismic activity of volcanoes, and in many other application areas. However, the task of developing software applications for wireless sensors is challenging for researchers because sensors have limited technical capabilities and software implementations for sensors require meticulous procedures to provide a desired level of services (e.g., reliability, security) for applications. This situation is even further exacerbated for researchers in other fields of engineering and science (e.g., civil engineers and geophysicists) as they may not have a rigorous programming background. Therefore, to facilitate the design, development and implementation of wireless sensor applications, this project provides a new framework called PROVIZ, which integrates visualization and programming functionalities into a common platform. PROVIZ is an open-source, platform independent, modular, and extensible framework for heterogeneous wireless sensor monitoring and application development. It consists of a set of easy-to-use simplified languages (one domain specific scripting language, one icon-based drag-and-drop style visual language) and a simple programming editor for developing wireless sensor applications and a mechanism for (re)programming wireless sensor nodes remotely over-the-air by distributing the generated application image. PROVIZ has the capability to visualize wireless sensor data captured from (1) a packet sniffer, (2) a binary packet trace file (e.g., PSD format), and (3) an external simulator running a wireless sensor application. PROVIZ also has the capability to process data from multiple sniffers simultaneously to visualize a large wireless sensor deployment.&lt;br/&gt;&lt;br/&gt;PROVIZ will be instrumental to scientists and engineers working with wireless sensors in many disciplines (e.g., civil engineering, ecology, agriculture). PROVIZ will allow these researchers to easily program sensors and to focus more on the tasks in their domains by significantly reducing the overhead of learning how to program sensors. The PROVIZ project will be conducted as an open source project, enabling interested software developers to benefit from and add to it. Further, a variation of PROVIZ will be used to present sensor networking concepts to middle school underrepresented minority students in Georgia. Given the proliferation of wireless sensor utilization in various engineering and science fields, it is envisioned that the success of the PROVIZ project will help contribute to the growth of the future cyber workforce in the U.S.", "ProgramReferenceCode": ["7433", "8005", "9251"], "OrganizationZip": "303320420"}, {"OrganizationCity": "SALT LAKE CITY", "OrganizationState": "UT", "Title": "SI2-SSE: A GPU-Enabled Toolbox for Solving Hamilton-Jacobi and Level Set Equations on Unstructured Meshes", "ProgramManager": "Daniel Katz", "State": "UT", "Program": ["DYNAMICAL SYSTEMS", "OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes", "CDS&E-MSS"], "ProgramElementCode": ["7478", "1253", "8004", "8069"], "StartDate": "06/01/2012", "OrganizationStreet": "75 S 2000 E", "EndDate": "05/31/2016", "PrincipalInvestigator": "Robert Kirby", "ARRAAmount": null, "OrganizationPhone": "8015816903", "AwardNumber": "1148291", "PIEmailAddress": "kirby@cs.utah.edu", "NSFDirectorate": "CSE", "Co-PIName": "Ross Whitaker", "LastAmendmentDate": "03/31/2014", "Organization": "University of Utah", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "531999", "Abstract": "A variety of application domains from geophysics to biomedicine employ some form of Hamilton-Jacobi (H-J) mathematical models. These models are a natural way to express conservation properties, and the two most prevalent H-J models seen in the literature are the Eikonal equation (a static H-J model based upon Fermat&apos;s Principle for determining minimal paths) and the Level-Set equations (a time-dependent H-J model used for addressing moving interface problems). The goal of this&lt;br/&gt;effort is to develop, test, document and distribute a collection of software tools for efficiently solving several classes of equations of H-J type -- in particular, Eikonal (minimal path) equations and Level-set equations -- on unstructured (triangular and tetrahedral) meshes using commodity streaming architectures. The PIs have previously demonstrated the feasibility of efficiently solving H-J equations on GPUs; this effort seeks to both scientific extend previous work as well as solidify the software into a publicly available tool suite.&lt;br/&gt;&lt;br/&gt;The intellectual merit of this effort is the development of efficient algorithmic strategies for mapping numerical methods for solving H-J equations on unstructured meshes to commodity streaming architectures. The proposed work will tackle several important technical challenges. One challenge is maintaining sufficient computational density on the parallel computational units (blocks), especially as we move to 3D unstructured meshes. A second technical challenge is the loss in efficiency that comes with communication between blocks. The solutions to these challenges will allow us to exploit currently available commodity streaming architectures that promising to provide teraflop performance on the desktop, which will be a boon for a variety of communities that rely on computationally expensive, simulation-based experiments. By overcoming the tedious and non-trivial step of developing and distributing software for solving H-J equations on unstructured meshes using commodity streaming architectures, the impact of this work has both longevity and ubiquity in a wide range of applications in diverse fields such as basic science, medicine, and engineering.", "ProgramReferenceCode": ["7433", "8005", "9150", "9251", "1253", "7478", "8004"], "OrganizationZip": "841128930"}, {"OrganizationCity": "Palo Alto", "OrganizationState": "CA", "Title": "Collaborative Research: SI2-SSE: Sage-Combinat: Developing and Sharing Open Source Software for Algebraic Combinatorics", "ProgramManager": "Daniel Katz", "State": "CA", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes", "CDS&E-MSS", "DYNAMICAL SYSTEMS"], "ProgramElementCode": ["1253", "8004", "8069", "7478"], "StartDate": "06/01/2012", "OrganizationStreet": "3160 Porter Drive", "EndDate": "05/31/2016", "PrincipalInvestigator": "Daniel Bump", "ARRAAmount": null, "OrganizationPhone": "6507232300", "AwardNumber": "1147463", "PIEmailAddress": "bump@math.stanford.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "06/19/2012", "Organization": "Stanford University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "143700", "Abstract": "Sage is an open source general purpose mathematical software system that has developed explosively within the last six years. Sage-Combinat is a subproject whose mission is &quot;to improve Sage as an extensible toolbox for computer exploration in (algebraic) combinatorics, and foster code sharing between researchers in this area&quot;. There is a long tradition of software packages for algebraic combinatorics. These have been crucial in the development of combinatorics since the 1960s. The originality of the Sage-Combinat project lies in successfully addressing the following simultaneous objectives. It offers a wide variety of interoperable and extensible tools, integrated in a general purpose mathematical software package, as needed for daily computer exploration in algebraic combinatorics; it is developed by a community of researchers spread around the world and across institutions; and it is open source and depends only on open source software. Among the proposers, Stein is founder and lead developer of Sage while Bump, Musiker, and Schilling are strong contributors to Sage-Combinat. Hivert and Thi&#180;ery (Paris-Sud, Orsay), founders and lead developers of Sage-Combinat, are both strongly affiliated with this project. Some of the research areas addressed in this project include symmetric functions, in particular Macdonald polynomials for arbitrary Cartan types and their nonsymmetric analogues, crystals, rigged configurations and combinatorial R-matrices, affine Weyl groups and Hecke algebras, cluster algebras, and posets.&lt;br/&gt;&lt;br/&gt;The project will develop Sage-Combinat in areas relevant to the ongoing research of the participants, together with relevant underlying infrastructure. The project will include three Sage Days workshops, and will be affiliated with a third scheduled workshop at ICERM. These workshops include a strong outreach component and have been a potent tool for connecting researchers and recruiting Sage users and developers. The grant will also fund a dedicated software development and computation server for Sage-Combinat, to be hosted in the Sage computation farm in Seattle. Emphasis will be placed on the development of thematic tutorials that will make the code accessible to new users. The proposal will also fund graduate student RA support, curriculum development, and other mentoring.", "ProgramReferenceCode": ["7433", "7683", "8005", "1253", "5514", "7478", "8004"], "OrganizationZip": "943041212"}, {"OrganizationCity": "NOTRE DAME", "OrganizationState": "IN", "Title": "SI2-SSE: Connecting Cyberinfrastructure with the Cooperative Computing Tools", "ProgramManager": "Daniel Katz", "State": "IN", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "04/15/2012", "OrganizationStreet": "940 Grace Hall", "EndDate": "03/31/2016", "PrincipalInvestigator": "Douglas Thain", "ARRAAmount": null, "OrganizationPhone": "5746317432", "AwardNumber": "1148330", "PIEmailAddress": "dthain@nd.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "03/12/2013", "Organization": "University of Notre Dame", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "507020", "Abstract": "This project supports the maintenance and development of the Cooperative Computing Tools. This software package is designed to enable non-privileged users to&lt;br/&gt;harness hundreds to thousands of cores from multiple clusters, clouds, and grids simultaneously. The main components of the software package include Parrot, a virtual file system that interfaces with multiple distributed storage systems, and Makeflow, a workflow engine that interfaces with multiple computing systems. This project will develop, maintain, and support the software across a wide variety of operating systems and national scale cyberinfrastructure in support of high impact scientific applications in fields such as bioinformatics, biometrics, data mining, high energy physics, and molecular dynamics.&lt;br/&gt;&lt;br/&gt;Large scale computing systems such as cluster, clouds, and grids now make it easy for end users to purchase large amounts of computing power at the touch of a button. However, these computing systems are difficult to harness because they each present a different user interface, principle of operation, and programming model. This project addresses this problem by supporting the development of the Cooperative Computing Tools, a software package that makes it possible for ordinary computer applications to move seamlessly between different service providers. The software is primarily of interest to researchers in scientific domains that require large amounts of computation. It is currently used by researchers in the fields of bioinformatics, biometrics, data mining, high energy physics, and molecular dynamics.", "ProgramReferenceCode": ["8004", "7433", "8005", "9251"], "OrganizationZip": "465565708"}, {"OrganizationCity": "Atlanta", "OrganizationState": "GA", "Title": "SI2-SSI: Collaborative: The XScala Project: A Community Repository for Model-Driven Design and Tuning of Data-Intensive Applications for Extreme-Scale Accelerator-Based Systems", "ProgramManager": "Rajiv Ramnath", "State": "GA", "Program": "CYBERINFRASTRUCTURE", "ProgramElementCode": "7231", "StartDate": "10/01/2013", "OrganizationStreet": "Office of Sponsored Programs", "EndDate": "09/30/2016", "PrincipalInvestigator": "David Bader", "ARRAAmount": null, "OrganizationPhone": "4048944819", "AwardNumber": "1339745", "PIEmailAddress": "bader@cc.gatech.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Edward Riedy", "Richard Vuduc"], "LastAmendmentDate": "09/16/2013", "Organization": "Georgia Tech Research Corporation", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "1188710", "Abstract": "The increasing gap between processor and memory performance -- referred to as the memory wall -- has led high-performance computing vendors to design and incorporate new accelerators into their next-generation systems. Representative accelerators include recon&amp;#64257;gurable hardware such as FPGAs, heterogeneous processors such as CPU+GPU processors, highly multicore and multithreaded processors, and manycore co-processors and general-purpose graphics processing units, among others. These accelerators contain myriad innovative architectural features, including explicit control of data motion, large-scale SIMD/vector processing, and multithreaded stream processing. Such features provide abundant opportunities for developers to achieve high-performance for applications that were previously deemed hard to optimize. This project aims to develop tools that will assist developers in using hardware accelerators (co-processors) productively and effectively.&lt;br/&gt;&lt;br/&gt;This project&apos;s specific technical focus is on data-intensive kernels including large dictionary string matching, dynamic programming, graph theory, and sparse matrix computations that arise in the domains of biology, network security, and the social sciences. The project is developing XScala, a software framework for designing efficient accelerator kernels. The framework contains a variety of design time and run-time performance optimization tools. The project concentrates on data-intensive kernels, bound by data movement. It proposes optimization techniques including (a) enhancing and exploiting maximal concurrency to hide data movement; (b) algorithmic reorganization to improve spatial and/or temporal locality; (c) data structure transformations to improve locality or reduce the size of the data (compressed structures); and (d) prefetching, among others. The project is also developing a public software repository and forum, called the XBazaar, for community-developed accelerator kernels. This project includes workshops, tutorials, and the PIs class and summer projects as various means by which to increase community involvement. The broader impacts include productive use of emerging classes of accelerator-augmented computer systems; creation of an open and accessible community repository, the XBazaar, for distributing accelerator-tuned computational kernels, software, and models; training of graduate and undergraduate students; and dissemination through publications, presentations at scientific meetings, lectures, workshops, and tutorials. The framework itself will be released as open-source code and as precompiled binaries for several common platforms, through the XBazaar, as an initial step toward building a community around accelerator kernels.", "ProgramReferenceCode": ["7433", "8009", "9145"], "OrganizationZip": "303320420"}, {"OrganizationCity": "Davis", "OrganizationState": "CA", "Title": "SI2-SSE: Development and Implementation of Software Elements using State-of-the-Art Computational Methodology to Advance Modeling Heterogeneities and Mixing in Earth&apos;s Mantle", "ProgramManager": "Daniel Katz", "State": "CA", "Program": ["GEOPHYSICS", "Software Institutes", "EarthCube"], "ProgramElementCode": ["1574", "8004", "8074"], "StartDate": "08/01/2014", "OrganizationStreet": "OR/Sponsored Programs", "EndDate": "07/31/2017", "PrincipalInvestigator": "Elbridge Puckett", "ARRAAmount": null, "OrganizationPhone": "5307547700", "AwardNumber": "1440811", "PIEmailAddress": "egpuckett@ucdavis.edu", "NSFDirectorate": "CSE", "Co-PIName": "Magali Billen", "LastAmendmentDate": "08/12/2014", "Organization": "University of California-Davis", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "487115", "Abstract": "This project involves the development and implementation of scientific software elements (SSEs), based on modern, high-resolution numerical methods for modeling steep gradients and sharp interfaces of material properties in viscous fluids in the presence of thermal convection. The goal of this project is to address a compelling need in geodynamics, in which continuum mechanics is applied to the study of geophysical processes, such as convection in the Earth?s mantle. A primary tool of geodynamics research is computational models of the flow of the extremely viscous interior of the Earth over hundreds of millions to billions of years. A long-standing challenge for these models is the need to accurately model sharp interfaces in temperature, viscosity, and other properties. These arise when, for example, modeling subduction (in which a cold tectonic plate plunges into the hot interior) or rising plumes (in which a hot boundary layer instability rises through the mantle and encounters the cold boundary layer of the tectonic plates). The project will foster interdisciplinary communication and the application of state-of-the-art applied and computational mathematics to fundamental problems in geophysics. It involves early-career mathematical scientists in the application of state-of-the-art numerical algorithms to geodynamics and, in particular, will provide an opportunity to increase the participation of women in mathematics and geodynamics research.&lt;br/&gt;&lt;br/&gt;This project involves the design and implementation of state-of-the-art SSEs for computing the evolution of significant processes in the Earth&apos;s mantle in which an essential feature of the problem is the presence of one or more moving boundaries, interfaces, or steep gradients in temperature, composition, or viscosity. The SSEs will address two critical issues that currently limit modern mantle convection simulations. All computational models of mantle convection currently in use produce significant overshoot and undershoot in the neighborhood of sharp gradients in temperature and viscosity. The cause of these overshoots and undershoots is a numerical artifact, which is well-known and well-understood in other fields, such as the computational shock physics community. Over the past thirty years researchers in computational shock physics have developed a variety of high-order accurate, monotone numerical methods, which preserve the physically correct maximum and minimum values of the computed quantities, while producing a high-order accurate numerical approximation of these quantities. Another compelling need in computational geodynamics is the ability to track discontinuous jumps in quantities such as material composition. Here high-order accurate interface tracking algorithms are required, since these fields undergo large-scale deformation, yet quantities such as the viscosity must be accurately approximated at the interface between two materials.", "ProgramReferenceCode": ["7433", "8005"], "OrganizationZip": "956186134"}, {"OrganizationCity": "Eugene", "OrganizationState": "OR", "Title": "Collaborative Research: SI2-SSI: A Comprehensive Ray Tracing Framework for Visualization in Distributed-Memory Parallel Environments", "ProgramManager": "Daniel Katz", "State": "OR", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "10/01/2013", "OrganizationStreet": "5219 UNIVERSITY OF OREGON", "EndDate": "09/30/2016", "PrincipalInvestigator": "Henry Childs", "ARRAAmount": null, "OrganizationPhone": "5413465131", "AwardNumber": "1339840", "PIEmailAddress": "hankchilds@gmail.com", "NSFDirectorate": "CSE", "LastAmendmentDate": "09/13/2013", "Organization": "University of Oregon Eugene", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "235961", "Abstract": "Scientific visualization plays a large role in exploring the scientific simulations that run on supercomputers; new discoveries are often made by studying renderings generated through visualization of simulation results. The standard technique for rendering geometry is rasterization and the most commonly used library for performing this is OpenGL. Many visualization programs (VisIt, Ensight, VAPOR, ParaView, VTK) use OpenGL for rendering. However, recent architectural changes on supercomputers create significant opportunities for alternate rendering techniques. The computational power available on emerging many-core architectures, such as the Intel Xeon Phi processors on TACC?s Stampede system, enable ray-tracing, a higher quality technique. Further, as the amount of geometry per node rises, ray-tracing becomes increasingly cost effective, since its computational costs are proportional to the screen size, not the geometry size. Finally, the software implementation for OpenGL can not be easily mapped to non-GPU multi-core and many-core systems, creating a significant gap; if not closed, visualization will not be possible directly on large supercomputers. This confluence of new, more capable architectures, the increase in geometry per node, and concerns about the durability of the established rendering path all motivate this work. &lt;br/&gt;&lt;br/&gt;To address these trends, this research uses a two-pronged approach. First, the research will replace the OpenGL pathways that are commonly used for visualization with a high-performance, open-source ray tracing engine that can interactively render on both a CPU and on accelerator architectures. This new library will support the OpenGL API and will be usable immediately by any OpenGL-based visualization package without additional code modi&amp;#64257;cation. Second, this research will provide a direct interface to a high-performance distributed ray tracing engine so that applications can take advantage of ray tracing capabilities not easily exposed through the standard OpenGL interface, such as participating media and global illumination simulation. These features will enable the open science community to easily create photo-realistic imagery with natural lighting cues to aid in analysis and discovery. It will further expand the capacity of existing cyberinfrastructure to provide interactive visualization on standard HPC resources. &lt;br/&gt;&lt;br/&gt;This work has the potential to revolutionize in situ visualization capabilities by unifying the (potentially hybrid) architecture that efficiently run both simulation and visualization. Communicating with underrepresented groups will be a major component of outreach efforts through the PCARP, MITE and Women in Engineering programs. In addition, the project team will disseminate this work to the general public through NSF XD program, the VisIt visualization toolkit and by exhibiting at forums such as IEEE Visualization, IEEE High Performance Graphics and ACM Supercomputing.", "ProgramReferenceCode": ["7433", "8009"], "OrganizationZip": "974035219"}, {"OrganizationCity": "NEW YORK", "OrganizationState": "NY", "Title": "Collaborative Research: SI2-CHE: Development and Deployment of Chemical Software for Advanced Potential Energy Surfaces", "ProgramManager": "Evelyn M. Goldfield", "State": "NY", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes"], "ProgramElementCode": ["1253", "8004"], "StartDate": "05/15/2013", "OrganizationStreet": "70 WASHINGTON SQUARE S", "EndDate": "04/30/2016", "PrincipalInvestigator": "Mark Tuckerman", "ARRAAmount": null, "OrganizationPhone": "2129982121", "AwardNumber": "1265889", "PIEmailAddress": "mark.tuckerman@nyu.edu", "NSFDirectorate": "MPS", "LastAmendmentDate": "05/15/2013", "Organization": "New York University", "AwardInstrument": "Standard Grant", "NsfOrganization": "CHE", "AwardedAmountToDate": "225375", "Abstract": "An international team consisting of Teresa Head-Gordon and Martin Head-Gordon (University of California, Berkeley), Paul Nerenberg (Claremont McKenna College), David Case (Rutgers University), Jay Ponder (Washington University), Mark Tuckerman (New York University) with their UK collaborators: Lorna Smith and Neil Chue Hong (University of Edinburgh), Chris-Kriton Skylaris and Jonathan W. Essex (University of Southampton), Ilian Todorov (Daresbury Laboratory), Mario Antonioletti (EPCC) are are supported through the SI2-CHE program to develop and deploy robust and sustainable software for advanced potential energy surfaces. The greater accuracy introduced by improvements in the new generation of potential energy surfaces opens up several challenges in their manifestation as algorithms and software on current or emergent hardware platforms that in turn limits their wide adoption by the computational chemistry community. The research team is overcoming these obstacles via multiple but integrated directions: (1) to optimally implement advanced potential energy surfaces across multi-core and GPU enabled systems, (2) to develop a hierarchy of advanced polarizable models that alter the tradeoff between accuracy and computational speed,(3) to create new multiple time stepping methods; (4) to write a Quantum Mechanics/Molecular Mechanics (QM/MM ) application programing interface (API) that fully supports mutual polarization, (5) to adopt software best practices to ensure growth of a self-sustaining community and (6) to provide exemplar calculations with the new software in the several emerging application areas.&lt;br/&gt;&lt;br/&gt;Molecular simulation and quantum chemistry software is an integral part of chemistry and chemical biology, and has been broadly adopted by academic researchers and industry scientists. Next generation scientific breakthroughs that utilize chemical software will be enabled by the deployment of state of the art theoretical models and algorithms that are translated into a sustainable software framework rapidly implemented on emergent high performance computing platforms. Potential energy surfaces describe the interactions between atoms. Advanced and highly accurate potential energy surfaces encounter software-related obstacles that inhibit their application to grand challenge chemistry problems. This UK and US consortium, representing a broad cross section of the computational chemistry software community, is working to directly tackle these obstacles. This US and UK collaboration between universities and High Performance Computing centers works to endure that chemical software investments made in advanced potential energy surface models has a long term payoff in community sustainability and the training of the next generation of scientists. Outreach and training workshops are organized around the emergence of the advanced potential energy software including an introductory molecular simulation software boot camp for undergraduate students.&lt;br/&gt;&lt;br/&gt;The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC.", "ProgramReferenceCode": ["5918", "5946", "5950", "7433", "8009"], "OrganizationZip": "100121019"}, {"OrganizationCity": "SAINT LOUIS", "OrganizationState": "MO", "Title": "Collaborative Research: SI2-CHE: Development and Deployment of Chemical Software for Advanced Potential Energy Surfaces", "ProgramManager": "Evelyn M. Goldfield", "State": "MO", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes"], "ProgramElementCode": ["1253", "8004"], "StartDate": "05/15/2013", "OrganizationStreet": "CAMPUS BOX 1054", "EndDate": "04/30/2016", "PrincipalInvestigator": "Jay Ponder", "ARRAAmount": null, "OrganizationPhone": "3147474134", "AwardNumber": "1265712", "PIEmailAddress": "ponder@dasher.wustl.edu", "NSFDirectorate": "MPS", "LastAmendmentDate": "05/15/2013", "Organization": "Washington University", "AwardInstrument": "Standard Grant", "NsfOrganization": "CHE", "AwardedAmountToDate": "257400", "Abstract": "An international team consisting of Teresa Head-Gordon and Martin Head-Gordon (University of California, Berkeley), Paul Nerenberg (Claremont McKenna College), David Case (Rutgers University), Jay Ponder (Washington University), Mark Tuckerman (New York University) with their UK collaborators: Lorna Smith and Neil Chue Hong (University of Edinburgh), Chris-Kriton Skylaris and Jonathan W. Essex (University of Southampton), Ilian Todorov (Daresbury Laboratory), Mario Antonioletti (EPCC) are are supported through the SI2-CHE program to develop and deploy robust and sustainable software for advanced potential energy surfaces. The greater accuracy introduced by improvements in the new generation of potential energy surfaces opens up several challenges in their manifestation as algorithms and software on current or emergent hardware platforms that in turn limits their wide adoption by the computational chemistry community. The research team is overcoming these obstacles via multiple but integrated directions: (1) to optimally implement advanced potential energy surfaces across multi-core and GPU enabled systems, (2) to develop a hierarchy of advanced polarizable models that alter the tradeoff between accuracy and computational speed,(3) to create new multiple time stepping methods; (4) to write a Quantum Mechanics/Molecular Mechanics (QM/MM ) application programing interface (API) that fully supports mutual polarization, (5) to adopt software best practices to ensure growth of a self-sustaining community and (6) to provide exemplar calculations with the new software in the several emerging application areas.&lt;br/&gt;&lt;br/&gt;Molecular simulation and quantum chemistry software is an integral part of chemistry and chemical biology, and has been broadly adopted by academic researchers and industry scientists. Next generation scientific breakthroughs that utilize chemical software will be enabled by the deployment of state of the art theoretical models and algorithms that are translated into a sustainable software framework rapidly implemented on emergent high performance computing platforms. Potential energy surfaces describe the interactions between atoms. Advanced and highly accurate potential energy surfaces encounter software-related obstacles that inhibit their application to grand challenge chemistry problems. This UK and US consortium, representing a broad cross section of the computational chemistry software community, is working to directly tackle these obstacles. This US and UK collaboration between universities and High Performance Computing centers works to endure that chemical software investments made in advanced potential energy surface models has a long term payoff in community sustainability and the training of the next generation of scientists. Outreach and training workshops are organized around the emergence of the advanced potential energy software including an introductory molecular simulation software boot camp for undergraduate students.&lt;br/&gt;&lt;br/&gt;The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC.", "ProgramReferenceCode": ["5918", "5946", "5950", "7433", "8009"], "OrganizationZip": "631304899"}, {"OrganizationCity": "Washington", "OrganizationState": "DC", "Title": "SI2-SSI Collaborative Research: A Computational Materials Data and Design Environment", "ProgramManager": "Daniel Katz", "State": "DC", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "DMR SHORT TERM SUPPORT", "CHEMISTRY PROJECTS", "Software Institutes"], "ProgramElementCode": ["1253", "1712", "1991", "8004"], "StartDate": "10/01/2012", "OrganizationStreet": "1000 Independence Avenue, SW", "EndDate": "09/30/2017", "PrincipalInvestigator": "Kristin Persson", "ARRAAmount": null, "OrganizationPhone": null, "AwardNumber": "1147892", "PIEmailAddress": "kapersson@lbl.gov", "NSFDirectorate": "CSE", "LastAmendmentDate": "09/20/2012", "Organization": "Department of Energy", "AwardInstrument": "Interagency Agreement", "NsfOrganization": "ACI", "AwardedAmountToDate": "250000", "Abstract": null, "ProgramReferenceCode": ["1982", "7237", "7433", "7569", "7644", "8009", "9216", "9263", "1253", "1712", "1991", "8004"], "OrganizationZip": "205850002"}, {"OrganizationCity": "WORCESTER", "OrganizationState": "MA", "Title": "SI2-SSE: Adding Research Accounts to the ASSISTments&apos; Platform: Helping Researchers Do Randomized Controlled Studies with Thousands of Students", "ProgramManager": "Daniel Katz", "State": "MA", "Program": ["PROGRAM EVALUATION", "Software Institutes"], "ProgramElementCode": ["7261", "8004"], "StartDate": "09/01/2014", "OrganizationStreet": "100 INSTITUTE RD", "EndDate": "08/31/2016", "PrincipalInvestigator": "Neil Heffernan", "ARRAAmount": null, "OrganizationPhone": "5088315000", "AwardNumber": "1440753", "PIEmailAddress": "nth@wpi.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "10/03/2014", "Organization": "Worcester Polytechnic Institute", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "486209", "Abstract": "ASSISTments is a free, university-based platform created to perform controlled experiments with the potential to help increase the quality, speed, and reliability of results related to K12 education. ASSISTments&apos; mission is &quot;to improve education through scientific research while not compromising student learning time.&quot; Each day, teachers assign problems to thousands of students (currently 50,000 students) in ASSISTments. These problem sets often contain controlled experiments. ASSISTments has used this platform to do controlled experiments that have resulted in 17 peer-reviewed publications. For a typical education researcher, developing relationships with schools is costly. ASSISTments has built relationships with teachers and researchers to run experiments to improve education without disrupting classrooms. This project will add researcher accounts to ASSISTments to better facilitate the research process. Researchers will create their own experiments, get approval from WPI for release to teachers, and get anonymized data. ASSISTments will reach out to its community of teachers who trust ASSISTments, to invite them to run the study in their classrooms. The intellectual merit of this work will be the contribution of the studies that this system would support. ASSISTments&apos; ten-year goal is to have a community of hundreds of scientists that use this tool to do their studies. &lt;br/&gt;&lt;br/&gt;Psychologists tend to study human learning in lab studies; researchers in education and learning sciences point out that it&apos;s not clear if those studies generalize to K12. These communities need to work together, but are lacking common ground. Thousands of researchers in psychology, mathematics education, and learning sciences care about using science to better understand human learning. Some researchers study how to help students with motivational messages, spaced retesting, or comparing feedback. Many researchers have used thousands of psychology undergraduates as subjects, but want their ideas tested and validated in authentic K12 settings. Everyone understands physicists need a shared scientific instrument to do their work, but so do educational psychologists. The broader impact of this work will be as a demonstration, showing how a tool could be built that helps many researchers conduct controlled experiments. This will include showing how the project can increase the efficiency of the scientists? work.", "ProgramReferenceCode": ["7433", "8005", "9177", "SMET"], "OrganizationZip": "016092247"}, {"OrganizationCity": "Ithaca", "OrganizationState": "NY", "Title": "SI2-SSE: Genetic Algorithm Software Package for Prediction of Novel Two-Dimensional Materials and Surface Reconstructions", "ProgramManager": "Daniel Katz", "State": "NY", "Program": ["DMR SHORT TERM SUPPORT", "Software Institutes"], "ProgramElementCode": ["1712", "8004"], "StartDate": "01/01/2015", "OrganizationStreet": "373 Pine Tree Road", "EndDate": "12/31/2017", "PrincipalInvestigator": "Richard Hennig", "ARRAAmount": null, "OrganizationPhone": "6072555014", "AwardNumber": "1440547", "PIEmailAddress": "rhennig@ufl.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "07/24/2014", "Organization": "Cornell University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "344696", "Abstract": "The ability to control structure and composition at the nanoscale has introduced exciting scientific and technological opportunities. Advances in the creation of nanomaterials such as single-layer materials and nanocrystals have led to improved understanding of basic structure-property relationships that, in turn, have enabled impressive progress in a broad range of nanotechnologies with applications for energy storage, catalysis and electronic devices. Yet, significant knowledge gaps persist in what single-layer materials could be synthesized and in our understanding of the nature of the surfaces of nanocrystals, particularly in the complex environment of solvents and ligands. The discovery of potentially stable novel single-layer materials and the prediction of nanocrystal surface structures are arguably among the most critical aspects of nanoscale materials. This research will provide the computational tools for the detailed prediction of the structure of two-dimensional materials and nanostructure surfaces in complex environments. This will impact the development and the design of novel nanomaterials with properties optimized for applications ranging from catalyst for chemical reactions, to energy conversion materials, to low-power and high-speed electronic devices.&lt;br/&gt;&lt;br/&gt;Progress in the field requires better computational methods for structure prediction. This project will (i) transform the Genetic Algorithm for Structure Prediction (GASP) software package developed by the PI into a sustainable scientific tool, (ii) extend its functionality to 2D materials and materials interfaces, and (iii) increase its performance by coupling to surrogate energy models that are optimized on the fly. These complementary goals will be achieved through expansion of the developer and user base, transition to portable software interfaces and data structures, and the addition of modular algorithms for functionality and performance enhancements. To enhance the functionality, the GASP algorithms will be extended to two two-dimensional materials and materials surfaces with adsorbates and ligands. To enhance the performance of the genetic algorithm, the optimization approach will be coupled to surrogate energy models such as machine-learning techniques and empirical energy models that are optimized on the fly. The publication of user tutorials, and documentation on the data structures and software interfaces will enhance the GASP codes overall utility, increase the user and developer base, and enable further extension to other data-mining and structure prediction approaches. The students involved in this project will receive extensive training and experience in algorithm development, scientific computation, and structure/property determination of complex nanomaterials. As part of the education and outreach component of the project, the PI will develop a course module on Materials Structure Predictions and widely distribute it. A weeklong workshop for students and postdocs in the third year of the project on Materials Discovery and Design will broaden the research?s impact beyond the creation of new software and the discovery of novel single-layer materials and nanocrystal surface and ligand configurations.", "ProgramReferenceCode": ["7237", "7433", "8005", "8400", "9216"], "OrganizationZip": "148502820"}, {"OrganizationCity": "NOTRE DAME", "OrganizationState": "IN", "Title": "SI2-SSE: Collaborative Proposal: Symbolic-Numeric Approaches to Polynomials", "ProgramManager": "Daniel Katz", "State": "IN", "Program": ["Software Institutes", "OFFICE OF MULTIDISCIPLINARY AC", "CDS&E-MSS"], "ProgramElementCode": ["8004", "1253", "8069"], "StartDate": "08/16/2014", "OrganizationStreet": "940 Grace Hall", "EndDate": "08/31/2017", "PrincipalInvestigator": "Jonathan Hauenstein", "ARRAAmount": null, "OrganizationPhone": "5746317432", "AwardNumber": "1460032", "PIEmailAddress": "hauenstein@nd.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "09/04/2014", "Organization": "University of Notre Dame", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "149995", "Abstract": "Polynomial systems arise naturally in many areas of human endeavor. These include the modeling of tumor growth; the design of robotic devices; chemical systems arising in areas ranging from combustion to blood clotting; assorted problems in physics; plus many areas with mathematics. The solution of the polynomial systems answers questions critical to these endeavors. This research will be devoted to developing the next generation of Bertini, an open source software package, which has been used successfully by many researchers on many problems, which include all those mentioned above.&lt;br/&gt;&lt;br/&gt;Bertini will be rewritten in C++ to be scriptable and modular, which will allow it to be interfaced transparently with symbolic software. The new Bertini will include tools allowing the user to construct and manipulate homotopies based on the output of Bertini. A major focus of the research will be given to systems of polynomials arising from the discretization of systems of differential equations. The great challenge of these very large systems of polynomials is balanced by the great potential impact new efficient and robust methods of solution will have.", "ProgramReferenceCode": ["7433", "8005", "8251"], "OrganizationZip": "465565708"}, {"OrganizationCity": "Atlanta", "OrganizationState": "GA", "Title": "Collaborative Research: SI2-SSE: Pythia Network Diagnosis Infrastructure (PuNDIT)", "ProgramManager": "Daniel Katz", "State": "GA", "Program": ["Software Institutes", "Campus Cyberinfrastrc (CC-NIE)"], "ProgramElementCode": ["8004", "8080"], "StartDate": "09/01/2014", "OrganizationStreet": "Office of Sponsored Programs", "EndDate": "08/31/2016", "PrincipalInvestigator": ["Constantinos Dovrolis", "Warren Matthews"], "ARRAAmount": null, "OrganizationPhone": "4048944819", "AwardNumber": "1440585", "PIEmailAddress": ["dovrolis@cc.gatech.edu", "warren.matthews@oit.gatech.edu"], "NSFDirectorate": "CSE", "LastAmendmentDate": "08/25/2014", "Organization": "Georgia Tech Research Corporation", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "237407", "Abstract": "In today&apos;s world of distributed collaborations of scientists, there are many challenges to providing effective infrastructures to couple these groups of scientists with their shared computing and storage resources. The Pythia Network Diagnostic InfrasTructure (PuNDIT) project will integrate and scale research tools and create robust code suitable for operational needs to address the difficult challenge of automating the detection and location of network problems. &lt;br/&gt;&lt;br/&gt;PuNDIT will build upon the de-facto standard perfSONAR network measurement infrastructure to gather and analyze complex real-world network topologies coupled with their corresponding network metrics to identify possible signatures of network problems from a set of symptoms. For example if the symptoms suggest a router along the path has buffers configured too small for high performance, Pythia will return a diagnosis of &quot;Small Buffer&quot;. If symptoms indicate non-congestive packet-loss for a particular network segment, the user can be notified of a possible &quot;Bad Network Segment&quot;. A primary goal for PuNDIT is to convert complex network metrics into easily understood diagnoses in an automated way.", "ProgramReferenceCode": ["7433", "8005"], "OrganizationZip": "303320420"}, {"OrganizationCity": "PITTSBURGH", "OrganizationState": "PA", "Title": "SI2-SSE: An Active File System for Processing and Sharing Petascale Multi-Dimentional Datasets", "ProgramManager": "Daniel Katz", "State": "PA", "Program": ["ADVANCES IN BIO INFORMATICS", "SPECIAL PROJECTS - CCF", "Software Institutes"], "ProgramElementCode": ["1165", "2878", "8004"], "StartDate": "09/01/2014", "OrganizationStreet": "5000 Forbes Avenue", "EndDate": "08/31/2017", "PrincipalInvestigator": "Arthur Wetzel", "ARRAAmount": null, "OrganizationPhone": "4122689527", "AwardNumber": "1440756", "PIEmailAddress": "awetzel@psc.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/08/2014", "Organization": "Carnegie-Mellon University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "500000", "Abstract": "Data sets in diverse areas such as biology, engineering, or astronomy routinely reach terabyte scales and are expected to grow to petabytes within the next few years. Typical examples include time series measurement or multidimensional volumetric data sets. Due to their rapidly increasing size, these data present severe limitations for data storage, transmission, and processing, and will thus become serious bottlenecks for analysis pipelines and collaborative data analysis. New approaches and frameworks are needed to enable the timely and cost effictive analyses at next generation data scales. The Active Data Processing and Transformation File System (ADAPT FS) combines efficient storage of original data with on-the-fly processing to enable collaborative processing and sharing of scientific datasets at the largest scales with minimal data duplication and latency. The remote access will leverage PSC&apos;s SLASH2 distributed filesystem and be extended to provide visualization. ADAPT-FS processing will enable easy access to leading-edge scientific datasets for teaching and training at institutions of all sizes.&lt;br/&gt;&lt;br/&gt;ADAPT-FS provides a FUSE based file system interface allowing seamless use from programs or web servers. The guiding principles behind ADAPT-FS are to: 1) eliminate unwanted data duplication; 2) minimize data transfer by working directly from original data when possible; 3) minimize delays between data capture and end-user analyses; and 4) provide a flexible workflow which incorporates active computation. The latter aspect enables improved parallel I/O performance by exploiting untapped CPU and GPU power on the nodes of large data servers. We will provide ADAPT-FS as open source, including a flexible and well-documented API and a plug-in framework enabling users to insert their own GPU and CPU codes into the data pipeline to extend and customize its data processing capabilities. Thus, ADAPT-FS will provide a critical technology to tackle the next generation of massive data intensive processing, allowing efficient and rapid analysis of petabytes size data sets with minimal data duplication in a collaborative multi-site framework.", "ProgramReferenceCode": ["7433", "8004", "8005", "1165", "2878"], "OrganizationZip": "152133815"}, {"OrganizationCity": "Williamsburg", "OrganizationState": "VA", "Title": "SI2-SSE: Enhancing the PReconditioned Iterative MultiMethod Eigensolver Software with New Methods and Functionality for Eigenvalue and Singular Value Decomposition (SVD) Problems", "ProgramManager": "Daniel Katz", "State": "VA", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "DMR SHORT TERM SUPPORT", "Software Institutes"], "ProgramElementCode": ["1253", "1712", "8004"], "StartDate": "09/01/2014", "OrganizationStreet": "Office of Sponsored Programs", "EndDate": "08/31/2017", "PrincipalInvestigator": "Andreas Stathopoulos", "ARRAAmount": null, "OrganizationPhone": "7572213966", "AwardNumber": "1440700", "PIEmailAddress": "andreas@cs.wm.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/11/2014", "Organization": "College of William and Mary", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "447884", "Abstract": "The numerical solution of large, sparse Hermitian Eigenvalue Problems (HEP) and Generalized HEP (GHEP) for a few extreme eigenvalues is one of the most important but also computationally intensive tasks in a variety of applications. Examples abound in spectral graph partitioning, large scale spectral graph analysis, structural engineering, electromagnetics, lattice Quantum Chromodynamics, and electronic structure applications from atomic scale physics to molecular scale materials science. Closely related is the problem of computing a partial Singular Value Decomposition (SVD) of a matrix, which finds everyday use in numerous applications including data mining and machine learning. The importance of the problem is evidenced by the significant resources that have been invested over the last decade in developing high quality eigenvalue software packages. However, these packages still do not include the near-optimal methods that have made the package PRIMME the software to beat. PRIMME, or PReconditioned Iterative MultiMethod Eigensolver, is a software package developed in 2005 for the solution of HEP. PRIMME brings state-of-the-art preconditioned iterative methods from &quot;bleeding edge&quot; to production, with a flexible, yet highly usable interface. Yet, it is its focus on numerical robustness and computational efficiency that has gained PRIMME the recognition as one of the best eigenvalue packages. This success calls for a new effort to extend PRIMME with some long awaited functionality but also to include new algorithms to address some outstanding problems in eigenvalue computations. This work is critical to many groups whose research depends on the lattice QCD and materials science software packages that PRIMME will improve through collaborations. PRIMME already has a PETSc interface, and with the proposed development of Hypre and Trilinos interfaces, it will be accessible by a far wider community of users. The most requested feature, however, has been a MATLAB interface. This will unleash the power of an &quot;industrial strength&quot; software to end users. Last but not least, this project will educate and train two graduate and several undergraduate students in the art of high performance numerical software.&lt;br/&gt;&lt;br/&gt;Specific goals for this projects include: PRIMME extension to GHEP, with special attention to ill conditioned mass matrices; PRIMME extension to SVD, with special attention to obtaining results at high accuracy (the solution must include not only PRIMME&apos;s robust components but a combination of known and new methods, as well as a dynamic way to choose between them); implementation of new methods and techniques for the solution of highly interior eigenvalue problems and for the computation of a large number of eigenvalues; interoperability with DOE libraries and MATLAB, and improved means of dissemination. As a numerical linear algebra kernel, PRIMME has a large potential audience in the computational sciences community. However, two specific collaborations will provide real-world, challenging problems and serve as a stress-test evaluator of the resulting methods and software. One involves the lattice QCD group at the DOE&apos;s Jefferson Lab, and the other involves the high performance computing and materials science group at IBM, Zurich.", "ProgramReferenceCode": ["7433", "8005", "8400", "9216"], "OrganizationZip": "231878795"}, {"OrganizationCity": "Fort Collins", "OrganizationState": "CO", "Title": "SI2-SSE: Collaborative Proposal: Symbolic-Numeric Approaches to Polynomials", "ProgramManager": "Daniel Katz", "State": "CO", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "CDS&E-MSS", "Software Institutes"], "ProgramElementCode": ["1253", "8069", "8004"], "StartDate": "09/01/2014", "OrganizationStreet": "601 S Howes St", "EndDate": "08/31/2017", "PrincipalInvestigator": "Daniel Bates", "ARRAAmount": null, "OrganizationPhone": "9704916355", "AwardNumber": "1440467", "PIEmailAddress": "bates@math.colostate.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/01/2014", "Organization": "Colorado State University", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "149346", "Abstract": "Polynomial systems arise naturally in many areas of human endeavor. These include the modeling of tumor growth; the design of robotic devices; chemical systems arising in areas ranging from combustion to blood clotting; assorted problems in physics; plus many areas with mathematics. The solution of the polynomial systems answers questions critical to these endeavors. This research will be devoted to developing the next generation of Bertini, an open source software package, which has been used successfully by many researchers on many problems, which include all those mentioned above.&lt;br/&gt;&lt;br/&gt;Bertini will be rewritten in C++ to be scriptable and modular, which will allow it to be interfaced transparently with symbolic software. The new Bertini will include tools allowing the user to construct and manipulate homotopies based on the output of Bertini. A major focus of the research will be given to systems of polynomials arising from the discretization of systems of differential equations. The great challenge of these very large systems of polynomials is balanced by the great potential impact new efficient and robust methods of solution will have.", "ProgramReferenceCode": ["7433", "8005", "8251"], "OrganizationZip": "805232002"}, {"OrganizationCity": "NOTRE DAME", "OrganizationState": "IN", "Title": "SI2-SSE: Collaborative Proposal: Symbolic-Numeric Approaches to Polynomials", "ProgramManager": "Daniel Katz", "State": "IN", "Program": ["Software Institutes", "CDS&E-MSS", "OFFICE OF MULTIDISCIPLINARY AC"], "ProgramElementCode": ["8004", "8069", "1253"], "StartDate": "09/01/2014", "OrganizationStreet": "940 Grace Hall", "EndDate": "08/31/2017", "PrincipalInvestigator": "Andrew Sommese", "ARRAAmount": null, "OrganizationPhone": "5746317432", "AwardNumber": "1440607", "PIEmailAddress": "sommese@nd.edu", "NSFDirectorate": "CSE", "Co-PIName": ["Bei Hu", "Charles Wampler"], "LastAmendmentDate": "08/01/2014", "Organization": "University of Notre Dame", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "199847", "Abstract": "Polynomial systems arise naturally in many areas of human endeavor. These include the modeling of tumor growth; the design of robotic devices; chemical systems arising in areas ranging from combustion to blood clotting; assorted problems in physics; plus many areas with mathematics. The solution of the polynomial systems answers questions critical to these endeavors. This research will be devoted to developing the next generation of Bertini, an open source software package, which has been used successfully by many researchers on many problems, which include all those mentioned above.&lt;br/&gt;&lt;br/&gt;Bertini will be rewritten in C++ to be scriptable and modular, which will allow it to be interfaced transparently with symbolic software. The new Bertini will include tools allowing the user to construct and manipulate homotopies based on the output of Bertini. A major focus of the research will be given to systems of polynomials arising from the discretization of systems of differential equations. The great challenge of these very large systems of polynomials is balanced by the great potential impact new efficient and robust methods of solution will have.", "ProgramReferenceCode": ["7433", "8005", "8251"], "OrganizationZip": "465565708"}, {"OrganizationCity": "Brooklyn", "OrganizationState": "NY", "Title": "SI2-SSE: High-Performance Software for Large-Scale Modeling of Binding Equilibria", "ProgramManager": "Daniel Katz", "State": "NY", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "08/01/2014", "OrganizationStreet": "Office of Research & Sponsored P", "EndDate": "07/31/2016", "PrincipalInvestigator": "Emilio Gallicchio", "ARRAAmount": null, "OrganizationPhone": "7189515622", "AwardNumber": "1440665", "PIEmailAddress": "egallicchio@brooklyn.cuny.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "06/17/2014", "Organization": "CUNY Brooklyn College", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "141135", "Abstract": "Living organisms are regulated by specific interactions among proteins and other macromolecules. Cancer and genetic diseases are due to altered interactions caused by mutations. Cellular organelles and viruses spontaneously build themselves from the ordered assembly of component molecules. While a complete understanding remains elusive, these and many other phenomena fundamentally rest on the ability of molecules to recognize and bind specifically to other molecules. The general principles of molecular recognition are being employed to design new drugs, chemical catalysts, and advanced materials. Computer models offer an important mean to disentangle and analyze molecular interactions and to produce quantitative predictions. The main objective of the project is to develop novel algorithms to model molecular recognition processes at atomic resolution on modern parallel computer architectures. This research seeks to increase the speed of the calculations and expand hardware support to enable the screening of larger sets of drug candidates and the study of multiple protein mutations under various conditions. The increased accuracy and availability of modeling software technologies by a larger community will lead to new ideas and research approaches, and ultimately to new discoveries in medicine, chemistry and material science. This effort will contribute to the establishment of in silico means to evaluate environmental and clinical claims. For example computational evidences on toxicity of substances can inform public policy in the same way that, for instance, atmospheric models are currently used for global climate projections.&lt;br/&gt;&lt;br/&gt;The project targets the Binding Energy Distribution Analysis Method (BEDAM, for short), an accurate model of molecular binding, currently limited by computational performance. Outcomes of this research include deployment the BEDAM model for the first time on General Purpose Graphical Processing Units (GPGPU) and Many Integrated Core (MIC) massively parallel architectures. To this end, the mathematical formulation of the model will be tuned to best utilize the features of these modern computing architectures. Specialized recursive computational geometry algorithms will be developed to extract from the parallel hardware near optimal performance. Robust automated tools for the processing of molecular models and their analysis will be put in place for large scale applications. Accessible user interfaces will be implemented to ensure wide applicability and adoption of the software. These software applications will be distributed under an open source license to promote sharing and community contributions. Student research assistants form an integral part of the research team. While contributing meaningfully to scientific research, students from challenging socioeconomic backgrounds will acquire computer programming and software maintenance skills useful to enter the high tech job market.", "ProgramReferenceCode": ["7433", "8005", "9216"], "OrganizationZip": "112102889"}, {"OrganizationCity": "Minneapolis", "OrganizationState": "MN", "Title": "SI2-CHE: Collaborative Research: Developing First Principles Monte Carlo Methods for Reactive Phase and Sorption Equilibria in the CP2K Software Suite", "ProgramManager": "Evelyn M. Goldfield", "State": "MN", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes"], "ProgramElementCode": ["1253", "8004"], "StartDate": "04/15/2013", "OrganizationStreet": "200 OAK ST SE", "EndDate": "03/31/2016", "PrincipalInvestigator": "Joern Ilja Siepmann", "ARRAAmount": null, "OrganizationPhone": "6126245599", "AwardNumber": "1265849", "PIEmailAddress": "siepmann@umn.edu", "NSFDirectorate": "MPS", "LastAmendmentDate": "04/02/2013", "Organization": "University of Minnesota-Twin Cities", "AwardInstrument": "Standard Grant", "NsfOrganization": "CHE", "AwardedAmountToDate": "349250", "Abstract": "An international research team consisting of Ilja Siepmann, Ben Lynch (University of Minnesota), Neeraj Rai (Mississippi State University), Troy Van Voorhis (Massachusetts Institute of Technology), Ben Slater (University College London), Michiel Sprik (University of Cambridge), Adam Carter (Edinburgh Parallel Computing Centre), J&#252;rg Hutter (University of Zurich), I-Feng Kuo (Lawrence Livermore National Laboratory), Christopher Mundy (Pacific Northwest National Laboratory), Joost VandeVondele (ETH Zurich), and Rodolphe Vuilleumier (University Pierre &amp; Marie Curie Paris) is collaborating to develop and implement new theoretical methods in the CP2K computational chemistry software suite. These new methodologies enable the predictive modeling of reactive multi-phase systems, including free energy landscapes and product yields, where the system interactions are described by Kohn-Sham density functional theory with van der Waals and hybrid functionals. Markov chain Monte Carlo approaches utilizing smart moves with asymmetric underlying matrices, such as the aggregation-volume-bias and configurational-bias Monte Carlo methods, and the Gibbs ensemble framework are employed for efficient exploration of the phase space for reactive single- and multi-phase equilibria in bulk and in confinement. The U.S. based research team is supported jointly by the Chemistry Division in MPS and the Office of Cyberinfrastucture. Funds for the UK based research team are provided by the EPSRC.&lt;br/&gt;&lt;br/&gt;The software infrastructure is advanced by the development of efficient and accurate methodologies for reactive phase and sorption equilibria that are applicable to chemical processes in diverse science and engineering applications. The extensively validated methodologies will be incorporated into the open-source CP2K software suite to make them available to a large user base. The new software can be used to identify optimal reaction conditions and separation processes for sustainable chemistry. The collaborative research team plans to use the new software for the investigation of reactive processes that address critical needs of society (fertilizers for food supply, fuels from renewable sources, and environmentally benign chemical processes).", "ProgramReferenceCode": ["5918", "5946", "5950", "7433", "8009", "8650"], "OrganizationZip": "554552070"}, {"OrganizationCity": "Durham", "OrganizationState": "NC", "Title": "Collaborative Proposal: SI2-CHE: ExTASY Extensible Tools for Advanced Sampling and analYsis", "ProgramManager": "Evelyn M. Goldfield", "State": "NC", "Program": ["OFFICE OF MULTIDISCIPLINARY AC", "Software Institutes"], "ProgramElementCode": ["1253", "8004"], "StartDate": "09/01/2013", "OrganizationStreet": "2200 W. Main St, Suite 710", "EndDate": "08/31/2016", "PrincipalInvestigator": "Mauro Maggioni", "ARRAAmount": null, "OrganizationPhone": "9196843030", "AwardNumber": "1265920", "PIEmailAddress": "mauro.maggioni@duke.edu", "NSFDirectorate": "MPS", "LastAmendmentDate": "08/28/2013", "Organization": "Duke University", "AwardInstrument": "Standard Grant", "NsfOrganization": "CHE", "AwardedAmountToDate": "148000", "Abstract": "Collaborative Research: SI2-CHE&lt;br/&gt;ExTASY Extensible Tools for Advanced Sampling and analYsis&lt;br/&gt;&lt;br/&gt;An international team consisting of Cecilia Clementi(Rice University), Mauro Maggioni (Duke University) Shantenu Jha (Rutgers University), Glenn Martyna (BM T. J. Watson Laboratory ), Charlie Laughton (University of Nottingham), Ben Leimkuhler ( University of Edinburgh), Iain Bethune (University of Edinburgh) and Panos Parpas(Imperial College) are supported through the SI2-CHE program for the development of ExTASY -- Extensible Toolkit for Advanced Sampling and analYsis, -- a conceptual and software framework that provides a step-change in the sampling of the conformational space of macromolecular systems. Specifically, ExTASY is a lightweight toolkit to enable first-class support for ensemble-based simulations and their seamless integration with dynamic analysis capabilities and ultra-large time step integration methods, whilst being extensible to other community software components via well-designed and standard interfaces. &lt;br/&gt;&lt;br/&gt; The primary impacts of this project are in the biological sciences. This software advances our understanding of biologically important systems, as it can be used to obtain fast and accurate sampling of the conformational dynamics of stable proteins; a prerequisite for the accurate prediction of thermodynamic parameters and biological functions. It also allows tackling systems like intrinsically disordered proteins, which can be beyond the reach of classical structural biology. Along with the research itself, the PIs are involved with outreach programs to attract high school students to science.", "ProgramReferenceCode": ["5918", "5946", "5950", "7433", "8009"], "OrganizationZip": "277054010"}, {"OrganizationCity": "BERKELEY", "OrganizationState": "CA", "Title": "Collaborative Research: SI2-CHE: Development and Deployment of Chemical Software for Advanced Potential Energy Surfaces", "ProgramManager": "Evelyn M. Goldfield", "State": "CA", "Program": ["Software Institutes", "OFFICE OF MULTIDISCIPLINARY AC"], "ProgramElementCode": ["8004", "1253"], "StartDate": "05/15/2013", "OrganizationStreet": "Sponsored Projects Office", "EndDate": "04/30/2016", "PrincipalInvestigator": "Teresa Head-Gordon", "ARRAAmount": null, "OrganizationPhone": "5106428109", "AwardNumber": "1265731", "PIEmailAddress": "thg@berkeley.edu", "NSFDirectorate": "MPS", "Co-PIName": "Martin Head-Gordon", "LastAmendmentDate": "05/15/2013", "Organization": "University of California-Berkeley", "AwardInstrument": "Standard Grant", "NsfOrganization": "CHE", "AwardedAmountToDate": "531527", "Abstract": "An international team consisting of Teresa Head-Gordon and Martin Head-Gordon (University of California, Berkeley), Paul Nerenberg (Claremont McKenna College), David Case (Rutgers University), Jay Ponder (Washington University), Mark Tuckerman (New York University) with their UK collaborators: Lorna Smith and Neil Chue Hong (University of Edinburgh), Chris-Kriton Skylaris and Jonathan W. Essex (University of Southampton), Ilian Todorov (Daresbury Laboratory), Mario Antonioletti (EPCC) are are supported through the SI2-CHE program to develop and deploy robust and sustainable software for advanced potential energy surfaces. The greater accuracy introduced by improvements in the new generation of potential energy surfaces opens up several challenges in their manifestation as algorithms and software on current or emergent hardware platforms that in turn limits their wide adoption by the computational chemistry community. The research team is overcoming these obstacles via multiple but integrated directions: (1) to optimally implement advanced potential energy surfaces across multi-core and GPU enabled systems, (2) to develop a hierarchy of advanced polarizable models that alter the tradeoff between accuracy and computational speed,(3) to create new multiple time stepping methods; (4) to write a Quantum Mechanics/Molecular Mechanics (QM/MM ) application programing interface (API) that fully supports mutual polarization, (5) to adopt software best practices to ensure growth of a self-sustaining community and (6) to provide exemplar calculations with the new software in several emerging application areas.&lt;br/&gt;&lt;br/&gt;Molecular simulation and quantum chemistry software is an integral part of chemistry and chemical biology, and has been broadly adopted by academic researchers and industry scientists. Next generation scientific breakthroughs that utilize chemical software will be enabled by the deployment of state of the art theoretical models and algorithms that are translated into a sustainable software framework rapidly implemented on emergent high performance computing platforms. Potential energy surfaces describe the interactions between atoms. Advanced and highly accurate potential energy surfaces encounter software-related obstacles that inhibit their application to grand challenge chemistry problems. This UK and US consortium, representing a broad cross section of the computational chemistry software community, is working to directly tackle these obstacles. This US and UK collaboration between universities and High Performance Computing centers works to endure that chemical software investments made in advanced potential energy surface models has a long term payoff in community sustainability and the training of the next generation of scientists. Outreach and training workshops are organized around the emergence of the advanced potential energy software including an introductory molecular simulation software boot camp for undergraduate students.&lt;br/&gt;&lt;br/&gt;The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC.", "ProgramReferenceCode": ["5918", "5946", "5950", "7433", "8009"], "OrganizationZip": "947045940"}, {"OrganizationCity": "SAN ANTONIO", "OrganizationState": "TX", "Title": "SI2-CHE: CCP-SAS - Collaborative Computing consortium for advanced analyses of structural data in chemical biology and soft condensed matter", "ProgramManager": "Evelyn M. Goldfield", "State": "TX", "Program": ["Software Institutes", "OFFICE OF MULTIDISCIPLINARY AC"], "ProgramElementCode": ["8004", "1253"], "StartDate": "06/01/2013", "OrganizationStreet": "7703 FLOYD CURL DR", "EndDate": "05/31/2017", "PrincipalInvestigator": "Emre Brookes", "ARRAAmount": null, "OrganizationPhone": "2105672340", "AwardNumber": "1265817", "PIEmailAddress": "emre@biochem.uthscsa.edu", "NSFDirectorate": "MPS", "LastAmendmentDate": "05/31/2013", "Organization": "University of Texas Health Science Center San Antonio", "AwardInstrument": "Standard Grant", "NsfOrganization": "CHE", "AwardedAmountToDate": "263926", "Abstract": "An international team consisting of Paul Butler (University of Tennessee, Knoxville and NIST), Jianhan Chen (Kansas State University), Emre Brooks (University of Texas Health Science Center, San Antonio) and their UK collaborators led by Stephen J. Perkins (University College, London) are supported through the SI2-CHE program. The object of this project is to provide a web-based GUI front-end with a high-performance back-end to increase the accessibility of advanced atomistic modeling of scattering data by novice users. Advanced analysis modules and new simulation methods including implicit solvent models are being developed to increase the accuracy of scattering calculations and simulation protocols. Software is being developed and disseminated with continual feedback from the research teams&apos; large international user-base. These efforts leverage an international user and developer community that use high-performance computing resources on a wide-range of cutting-edge chemical problems. &lt;br/&gt;&lt;br/&gt;A typical bench scientist purifies and characterizes samples, collects the small angle x-ray scattering (SAXS), small angle neutron scattering (SANS), or analytical ultracentrifugation (AUC) data, and interprets the results using simplistic models. It is rare that the same individual also has the skills to use advanced atomistic simulation software. The CCP-SAS project is focused on developing an easy-to-use modeling package that enables users to generate physically accurate atomistic models, calculate scattering profiles and compare results to experimental scattering data sets in a single web-based software suite. This enables a broad range of scattering scientists to access often complicated simulation and scattering analysis methods seamlessly thus providing a significant acceleration to the discovery process. &lt;br/&gt;&lt;br/&gt;The US based investigators are supported by the CHE and ACI divisions within NSF; the UK based investigators are supported by the EPSRC.", "ProgramReferenceCode": ["5918", "5946", "5950", "7433", "8009"], "OrganizationZip": "782293901"}, {"OrganizationCity": "Saint Joseph", "OrganizationState": "MN", "Title": "Collaborative Research: SI2-SSI: Open Source Support for Massively Parallel, Generic Finite Element Methods", "ProgramManager": "Daniel Katz", "State": "MN", "Program": "Software Institutes", "ProgramElementCode": "8004", "StartDate": "08/01/2012", "OrganizationStreet": "37 South College Avenue", "EndDate": "07/31/2016", "PrincipalInvestigator": "Michael Heroux", "ARRAAmount": null, "OrganizationPhone": "3203635690", "AwardNumber": "1148188", "PIEmailAddress": "mheroux@csbsju.edu", "NSFDirectorate": "CSE", "LastAmendmentDate": "08/09/2012", "Organization": "College of Saint Benedict", "AwardInstrument": "Standard Grant", "NsfOrganization": "ACI", "AwardedAmountToDate": "181586", "Abstract": "Partial differential equations are used in a wide variety of applications as&lt;br/&gt;mathematical models. Their numerical solution is, consequently, of prime&lt;br/&gt;importance for the accurate simulation and optimization of processes in the&lt;br/&gt;sciences, engineering, and beyond.&lt;br/&gt;The last decade saw the emergence of large and successful libraries that&lt;br/&gt;support such applications. While these libraries provide most of what such&lt;br/&gt;codes need for small-scale computations, many realistic applications yield&lt;br/&gt;problems of hundreds of millions or billions of unknowns and require clusters&lt;br/&gt;with thousands of processor cores, but there is currently little generic&lt;br/&gt;support for such problems, limiting access to the many large publicly&lt;br/&gt;supported computing facilities to experts in computational science and&lt;br/&gt;excluding scientists from many fields for whom computational simulation would&lt;br/&gt;be a useful tool. This project intends to build the software infrastructure that will allow a&lt;br/&gt;wide cross section of scientists to utilize these large resources.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project intends to support the software infrastructure for the&lt;br/&gt;large-scale solution of partial differential equations on massively parallel&lt;br/&gt;computational resources in a generic way. It will build on two of the most&lt;br/&gt;successful libraries for scientific computing, the finite element library&lt;br/&gt;deal.II, and Trilinos that provides the parallel linear algebra capabilities&lt;br/&gt;for the former. Specifically, we will: (i) Make support for massively parallel&lt;br/&gt;computations ubiquitous in deal.II; (ii) Research and develop seamless support&lt;br/&gt;for problems with billions of unknowns in both libraries and improve the&lt;br/&gt;interaction between the two; (iii) Exploit intra-node parallelism on today&apos;s&lt;br/&gt;clusters; (iv) Ensure the applicability of our work on a broad basis by&lt;br/&gt;implementing two real-world applications. &lt;br/&gt;Both deal.II and Trilinos have large, active and diverse developer and user&lt;br/&gt;communities, and this project will actively engage these communities through&lt;br/&gt;user meetings, short courses, regularly taught classes, mailing lists, and&lt;br/&gt;direct contact in focused projects.", "ProgramReferenceCode": ["7433", "8009", "8004"], "OrganizationZip": "563742099"}]}}